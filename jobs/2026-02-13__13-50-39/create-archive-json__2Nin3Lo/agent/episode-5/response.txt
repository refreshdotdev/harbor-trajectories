```json
{
  "analysis": "The first syntax error on line 182 was successfully corrected. However, running the Python script again resulted in a new `SyntaxError: invalid syntax` on line 242. The error message indicates that the line `ls -la /home/user/experiments/` (which is a shell command) was found within the Python script's code at line 242, which is not valid Python syntax. This suggests that the command I intended to be executed by the shell was inadvertently embedded within the Python script's content during its creation in the previous step. The `ls -la /home/user/experiments/` command (which was the next command in my sequence) then ran successfully, showing the directory structure but no output files from the script's processing (like `artifact_inventory.csv` or `cleanup.log`).",
  "plan": "The Python script is currently corrupted and unusable. I need to recreate it from scratch. I will first remove the corrupted script file. Then, I will carefully recreate the `manage_experiments.py` script, ensuring its content is only valid Python code and free of any shell commands or other artifacts. After recreating the script, I will make it executable and attempt to run it again. Finally, I will list the contents of `/home/user/experiments/` to verify the results.",
  "commands": [
    {
      "keystrokes": "rm /home/user/experiments/manage_experiments.py\n",
      "duration": 0.1
    },
    {
      "keystrokes": "cat > /home/user/experiments/manage_experiments.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport re\nimport json\nimport hashlib\nimport shutil\nimport sys\nimport csv\nimport io\nimport locale\nfrom pathlib import Path\n\n# --- Configuration ---\nEXPERIMENTS_ROOT = Path('/home/user/experiments')\nCKPT_ARCHIVE_DIR = EXPERIMENTS_ROOT / 'ckpt_archive'\nINVENTORY_CSV_PATH = EXPERIMENTS_ROOT / 'artifact_inventory.csv'\nLOG_FILE_PATH = EXPERIMENTS_ROOT / 'cleanup.log'\n\n# --- Global Counters & Data ---\nvalid_ckpts_moved = 0\nall_loss_values = [] # For global average loss\n\n# --- Helper Functions ---\n\ndef get_sha256(file_path):\n    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n    sha256 = hashlib.sha256()\n    try:\n        with open(file_path, 'rb') as f:\n            while True:\n                data = f.read(65536) # Read in 64k chunks\n                if not data:\n                    break\n                sha256.update(data)\n        return sha256.hexdigest()\n    except FileNotFoundError:\n        return 'ERROR_HASH_FILE_NOT_FOUND'\n    except Exception as e:\n        return f'ERROR_HASH_{type(e).__name__}'\n\ndef get_file_size_bytes(file_path):\n    try:\n        return os.path.getsize(file_path)\n    except FileNotFoundError:\n        return -1 # Indicate error\n    except Exception as e:\n        return f'ERROR_SIZE_{type(e).__name__}'\n\n# --- Main Processing Logic ---\ndef process_experiments():\n    global valid_ckpts_moved, all_loss_values\n    \n    if not EXPERIMENTS_ROOT.exists():\n        print(\"Error: Experiments root directory does not exist.\")\n        sys.exit(1)\n    if not CKPT_ARCHIVE_DIR.exists():\n        CKPT_ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n        print(f\"Created archive directory: {CKPT_ARCHIVE_DIR}\")\n\n    all_files_for_inventory = []\n    \n    # 1. Checkpoint Archival and Global Average Loss Calculation\n    run_dirs = sorted([d for d in EXPERIMENTS_ROOT.iterdir() if d.is_dir() and re.match(r'^run_\\\\d{4}-\\\\d{2}-\\\\d{2}$', d.name)])\n    \n    print(f\"Processing {len(run_dirs)} run directories...\")\n\n    for run_dir in run_dirs:\n        print(f\"  Processing {run_dir}\")\n        metrics_json_path = run_dir / 'metrics.json'\n        ckpt_files = list(run_dir.glob('*.ckpt'))\n\n        # Read metrics.json for this run directory (for global average loss)\n        loss_value_for_run = None\n        try:\n            with open(metrics_json_path, 'r', encoding='utf-8-sig') as f:\n                content = f.read()\n            decoded_content = None\n            try:\n                decoded_content = json.loads(content)\n            except json.JSONDecodeError as e1:\n                # Try Latin-1 as a fallback if UTF-8-SIG (which handles UTF-8 and BOM) fails\n                print(f\"    Warning: Could not parse {metrics_json_path} with utf-8-sig ({e1}). Trying latin-1.\")\n                try:\n                    with open(metrics_json_path, 'r', encoding='latin-1') as f_latin:\n                        content_latin = f_latin.read()\n                    decoded_content = json.loads(content_latin)\n                except Exception as e2:\n                    print(f\"    Error parsing {metrics_json_path} with latin-1: {e2}\")\n                    decoded_content = None\n            \n            if decoded_content and 'loss' in decoded_content and isinstance(decoded_content['loss'], (int, float)):\n                loss_value_for_run = float(decoded_content['loss'])\n                all_loss_values.append(loss_value_for_run)\n                print(f\"    Found loss: {loss_value_for_run} in {metrics_json_path.name}\")\n            else:\n                print(f\"    No valid 'loss' float in {metrics_json_path.name} or file missing/unreadable\")\n        except FileNotFoundError:\n            print(f\"    {metrics_json_path.name} not found in {run_dir.name}\")\n        except Exception as e:\n            print(f\"    Error reading/parsing {metrics_json_path.name}: {e}\")\n\n        for ckpt_file in ckpt_files:\n            file_size = ckpt_file.stat().st_size\n            if file_size > 1024: # Valid checkpoint (size > 1KB)\n                loss_prefix = loss_value_for_run if loss_value_for_run is not None else 'unknown'\n                new_filename = f\"loss_{{:.4f}}__{{}}\".format(loss_prefix, ckpt_file.name) \n                if loss_prefix == 'unknown':\n                    new_filename = f\"loss_unknown__{ckpt_file.name}\"\n                \n                new_ckpt_path = CKPT_ARCHIVE_DIR / new_filename\n                counter = 1\n                while new_ckpt_path.exists():\n                    base_name = new_filename\n                    if counter > 1 and new_filename.endswith(f\"_copy{counter-1}\"):\n                         # Avoid appending copyN to a name that might already have it from a prior collision\n                        # This logic is tricky; for now, just append _copyN to the original target name\n                        pass # Use the simpler logic below\n                    new_filename = f\"{base_name}_copy{counter}\"\n                    new_ckpt_path = CKPT_ARCHIVE_DIR / new_filename\n                    counter += 1\n                \n                try:\n                    shutil.move(str(ckpt_file), str(new_ckpt_path))\n                    valid_ckpts_moved += 1\n                    print(f\"    Moved {ckpt_file.name} to {new_ckpt_path.name}\")\n                except Exception as e:\n                    print(f\"    Error moving {ckpt_file.name} to {new_ckpt_path.name}: {e}\")\n            else:\n                print(f\"    Skipping small/truncated .ckpt file: {ckpt_file.name} (size: {file_size} bytes)\")\n\n    # 2. Generate Artifact Inventory File\n    print(\"\\nGenerating artifact inventory...\")\n    for root, dirs, files in os.walk(EXPERIMENTS_ROOT):\n        for file in files:\n            file_path = Path(root) / file\n            if file_path == INVENTORY_CSV_PATH or file_path == LOG_FILE_PATH:\n                continue\n            relative_path = file_path.relative_to(EXPERIMENTS_ROOT)\n            item_size = get_file_size_bytes(file_path)\n            item_sha256 = get_sha256(file_path)\n\n            all_files_for_inventory.append({\n                'filepath': str(relative_path),\n                'size_bytes': item_size,\n                'sha256': item_sha256\n            })\n    \n    all_files_for_inventory.sort(key=lambda x: x['filepath'].lower())\n    print(f\"  Collected {len(all_files_for_inventory)} files for inventory.\")\n\n    try:\n        with open(INVENTORY_CSV_PATH, 'w', encoding='utf-16-le', newline='\\r\\n') as csvfile:\n            writer = csv.writer(csvfile, delimiter=';', quoting=csv.QUOTE_MINIMAL)\n            writer.writerow(['filepath', 'size_bytes', 'sha256'])\n            for item in all_files_for_inventory:\n                writer.writerow([item['filepath'], str(item['size_bytes']), item['sha256']])\n        print(f\"  Inventory written to {INVENTORY_CSV_PATH}\")\n    except Exception as e:\n        print(f\"  Error writing inventory CSV: {e}\")\n\n    # 3. Summary Log with Integrity Check\n    print(\"\\nGenerating summary log...\")\n    global_avg_loss = 0.0\n    if all_loss_values:\n        global_avg_loss = sum(all_loss_values) / len(all_loss_values)\n    \n    inventory_md5 = \"ERROR_MD5_UNKNOWN\"\n    try:\n        md5_hash = hashlib.md5()\n        with open(INVENTORY_CSV_PATH, 'rb') as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                md5_hash.update(byte_block)\n        inventory_md5 = md5_hash.hexdigest()\n    except FileNotFoundError:\n        print(f\"  Warning: {INVENTORY_CSV_PATH} not found for MD5 calculation.\")\n    except Exception as e:\n        print(f\"  Error calculating MD5 for inventory: {e}\")\n\n    log_content = f\"VALID_CKPTS_MOVED: {valid_ckpts_moved}\\nGLOBAL_AVG_LOSS: {global_avg_loss:.4f}\\nINVENTORY_MD5: {inventory_md5}\"\n    \n    try:\n        with open(LOG_FILE_PATH, 'w') as f:\n            f.write(log_content)\n        os.chmod(LOG_FILE_PATH, 0o400) \n        print(f\"  Summary log written to {LOG_FILE_PATH}\")\n        print(f\"  Log content:\\n{log_content}\")\n    except Exception as e:\n        print(f\"  Error writing summary log: {e}\")\n\n    print(\"\\nTask complete. Check /home/user/experiments/ for results.\")\n\nif __name__ == '__main__':\n    process_experiments()\nEOF\n",
      "duration": 0.5
    },
    {
      "keystrokes": "chmod +x /home/user/experiments/manage_experiments.py\n",
      "duration": 0.1
    },
    {
      "keystrokes": "python3 /home/user/experiments/manage_experiments.py\n",
      "duration": 5.0
    },
    {
      "keystrokes": "ls -la /home/user/experiments/\n",
      "duration": 0.1
    }
  ],
  "task_complete": false
}
```