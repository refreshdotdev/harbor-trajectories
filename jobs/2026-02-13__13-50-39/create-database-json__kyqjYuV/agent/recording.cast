{"version": 2, "width": 160, "height": 40, "timestamp": 1771008742, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.006692, "o", "\u001b[?2004hroot@e18e87cd-d242-4dd1-8ab5-bba6b7ab15c2:/home/user# "]
[1.053638, "i", "clear\r"]
[1.053742, "o", "clear\r\n\u001b[?2004l\r"]
[1.054823, "o", "\u001b[H\u001b[J"]
[1.054977, "o", "\u001b[?2004hroot@e18e87cd-d242-4dd1-8ab5-bba6b7ab15c2:/home/user# "]
[6.90286, "i", "cd /home/user\n"]
[6.902977, "o", "cd /home/user\r\n\u001b[?2004l\r"]
[6.903058, "o", "\u001b[?2004hroot@e18e87cd-d242-4dd1-8ab5-bba6b7ab15c2:/home/user# "]
[6.976187, "i", "mkdir -p query_optimization\n"]
[6.976322, "o", "mkdir -p query_optimization\r\n\u001b[?2004l\r"]
[6.978137, "o", "\u001b[?2004hroot@e18e87cd-d242-4dd1-8ab5-bba6b7ab15c2:/home/user# "]
[7.093192, "i", "cd query_optimization\n"]
[7.093318, "o", "cd query_optimization\r\n\u001b[?2004l\r"]
[7.093376, "o", "\u001b[?2004h"]
[7.093404, "o", "root@e18e87cd-d242-4dd1-8ab5-bba6b7ab15c2:/home/user/query_optimization# "]
[7.175815, "i", "python3 -m venv env\n"]
[7.176145, "o", "python3 -m venv env\r\n\u001b[?2004l\r"]
[7.679334, "i", "env/bin/pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\n"]
[7.679373, "o", "env/bin/pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\r\n"]
[9.282843, "o", "\u001b[?2004hroot@e18e87cd-d242-4dd1-8ab5-bba6b7ab15c2:/home/user/query_optimization# "]
[9.282981, "o", "env/bin/pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\r\n\u001b[?2004l\r"]
[9.675464, "o", "Collecting sqlparse==0.4.4\r\n"]
[9.711566, "o", "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\r\n"]
[9.71185, "o", "\u001b[?25l"]
[9.713023, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[9.725988, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[9.843546, "o", "Collecting psycopg2-binary==2.9.7\r\n"]
[9.85006, "o", "  Downloading psycopg2_binary-2.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n"]
[9.850279, "o", "\u001b[?25l"]
[9.851214, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[9.886851, "o", "\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/3.0 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"]
[9.908314, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[9.932745, "o", "Installing collected packages: sqlparse, psycopg2-binary\r\n"]
[10.021887, "o", "Successfully installed psycopg2-binary-2.9.7 sqlparse-0.4.4\r\n"]
[10.062569, "o", "\u001b[?2004hroot@e18e87cd-d242-4dd1-8ab5-bba6b7ab15c2:/home/user/query_optimization# "]
[11.986737, "i", " "]
[11.986848, "o", " "]
[99.011923, "i", "cat > analyze_and_optimize.py << 'EOF'\n#!/home/user/query_optimization/env/bin/python3\n\"\"\"\nSQL Query Log Analysis & Optimization Script\n\"\"\"\nimport argparse\nimport configparser\nimport csv\nimport hashlib\nimport os\nimport re\nimport sqlparse\nimport sys\nfrom datetime import datetime\nimport json\n\n# Read configuration file\ndef load_config():\n    config = configparser.ConfigParser()\n    config.read('/home/user/db_tools/db_config.ini')\n    return config['DEFAULT']\n\n# Parse log line function\ndef parse_log_line(line, date_str, min_duration_ms):\n    parts = [p.strip() for p in line.split('|')]\n    if len(parts) != 4:\n        return None\n    \n    timestamp_str, duration_str, level, message = parts\n    \n    # Validate timestamp format\n    try:\n        timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\n        if timestamp.date().isoformat() != date_str:\n            return None\n    except ValueError:\n        return None\n    \n    # Validate level\n    if level != '[QUERY]':\n        return None\n    \n    #"]
[99.012032, "i", " Validate and parse duration\n    try:\n        duration_ms = float(duration_str.rstrip('ms'))\n        if duration_ms < min_duration_ms:\n            return None\n    except ValueError:\n        return None\n    \n    return timestamp, duration_ms, message.strip()\n\n# Normalize SQL function\ndef normalize_sql(query_text):\n    try:\n        parsed = sqlparse.parse(query_text)[0]\n        formatted = sqlparse.format(\n            parsed, \n            keyword_case='upper', \n            strip_comments=True, \n            reindent=False\n        )\n        # Remove trailing semicolon and whitespace\n        return re.sub(r'[\\\\s;]+\\\\Z', '', formatted).strip()\n    except Exception:\n        return query_text\n\n# Extract table and columns from WHERE clause\ndef extract_index_info(normalized_sql):\n    try:\n        parsed = sqlparse.parse(normalized_sql)[0]\n        # Find WHERE token\n        where_token = None\n        for token in parsed.tokens:\n            if token.value.upper() == 'WHERE':\n                where_token = token\n          "]
[99.012063, "i", "      break\n        \n        if not where_token:\n            return None, None\n        \n        # Get tokens after WHERE\n        where_clause = ''\n        next_token = where_token.next\n        while next_token and next_token.value.upper() not in ['LIMIT', 'OFFSET', 'GROUP BY', 'ORDER BY', 'HAVING']:\n            where_clause += str(next_token)\n            next_token = next_token.next\n        \n        # Extract table name and columns\n        table_match = re.search(r'FROM\\\\s+(\\\\w+)', normalized_sql, re.IGNORECASE)\n        if not table_match:\n            return None, None\n        \n        table_name = table_match.group(1)\n        \n        # Extract column=value patterns\n        conditions = re.findall(r'(\\\\w+)\\\\s*=\\\\s*[^\\\\s\\''\\\"]+', where_clause, re.IGNORECASE)\n        if not conditions:\n            return None, None\n        \n        columns = [cond[0] for cond in conditions]\n        \n        return table_name, columns\n    except Exception:\n        return None, None\n\n# Main analysis function\ndef analyze_queries("]
[99.012084, "o", "cat > analyze_and_optimize.py << 'EOF'"]
[99.012106, "i", "log_path, date_str, min_duration_ms):\n    raw_queries = []\n    \n    # Read and parse log file\n    try:\n        with open(log_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                if not line.strip():\n                    continue\n                parsed = parse_log_line(line, date_str, min_duration_ms)\n                if parsed:\n                    raw_queries.append(parsed)\n    except FileNotFoundError:\n        print(f\"Error: Log file {log_path} not found\", file=sys.stderr)\n        sys.exit(1)\n    \n    if not raw_queries:\n        print(\"No qualifying queries found\")\n        return None, []\n    \n    # Prepare staging CSV data\n    staging_data = []\n    for timestamp, duration_ms, query_text in raw_queries:\n        staging_data.append([\n            timestamp.isoformat(),\n            duration_ms,\n            query_text\n        ])\n    \n    # Create staging directory\n    os.makedirs('staging', exist_ok=True)\n    \n    # Write staging CSV with specific encoding and line endings\n    csv_path = o"]
[99.012132, "o", "\r\n\u001b[?2004l\r\u001b[?2004h> "]
[99.012156, "i", "s.path.join('staging', 'raw_extract.csv')\n    try:\n        with open(csv_path, 'w', encoding='ISO-8859-1', newline='') as f:\n            writer = csv.writer(f, lineterminator='\\\\r\\\\n')\n            writer.writerow(['timestamp', 'duration_ms', 'query_text'])\n            writer.writerows(staging_data)\n    except Exception as e:\n        print(f\"Error writing staging CSV: {e}\", file=sys.stderr)\n        sys.exit(1)\n    \n    # Compute SHA-256 checksum of the CSV file\n    sha256 = hashlib.sha256()\n    try:\n        with open(csv_path, 'rb') as f:\n            sha256.update(f.read())\n    except Exception as e:\n        print(f\"Error computing checksum: {e}\", file=sys.stderr)\n        sys.exit(1)\n    \n    checksum = sha256.hexdigest()\n    \n    # Process queries: normalize, deduplicate, and generate suggestions\n    query_data = {}\n    for timestamp, duration_ms, query_text in raw_queries:\n        normalized = normalize_sql(query_text)\n        \n        if normalized not in query_data:\n            table_name, columns = extrac"]
[99.012179, "i", "t_index_info(normalized)\n            if table_name and columns:\n                # Format columns for index name and definition\n                col_str_name = '_'.join(columns)\n                col_str_def = ', '.join(columns)\n                index_suggestion = f\"CREATE INDEX idx_{table_name}_{col_str_name} ON {table_name}({col_str_def});\"\n            else:\n                index_suggestion = None\n            \n            query_data[normalized] = {\n                'first_timestamp': timestamp,\n                'count': 0,\n                'index_suggestion': index_suggestion\n            }\n        \n        query_data[normalized]['count'] += 1\n    \n    # Prepare suggestions list sorted by first occurrence\n    suggestions = []\n    for normalized, data in query_data.items():\n        if data['index_suggestion']:\n            suggestions.append({\n                'id': data['first_timestamp'].isoformat(),\n                'normalized_sql': normalized,\n                'index_suggestion': data['index_suggestion']\n           "]
[99.0122, "i", " })\n    \n    suggestions.sort(key=lambda x: x['id'])\n    \n    return checksum, suggestions\n\ndef main():\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description='SQL Query Log Analyzer')\n    parser.add_argument('--date', required=True, help='Date to analyze in YYYY-MM-DD format')\n    args = parser.parse_args()\n    \n    # Validate date format\n    try:\n        datetime.fromisoformat(args.date.replace('-', ''))\n    except ValueError:\n        print(\"Invalid date format. Use YYYY-MM-DD\", file=sys.stderr)\n        sys.exit(1)\n    \n    # Load configuration\n    config = load_config()\n    log_path = config.get('log_file_path')\n    min_duration_ms = config.getfloat('min_duration_ms', fallback=0.0)\n    \n    if not log_path:\n        print(\"log_file_path not found in config\", file=sys.stderr)\n        sys.exit(1)\n    \n    # Perform analysis\n    checksum, suggestions = analyze_queries(log_path, args.date, min_duration_ms)\n    if checksum is None:\n        return\n    \n    # Prepare report data\n    to"]
[99.012218, "i", "tal_queries = sum(data['count'] for data in query_data.values())\n    unique_patterns = len(query_data)\n    \n    report_data = {\n        'meta': {\n            'date_analyzed': args.date,\n            'staging_file_sha256': checksum\n        },\n        'stats': {\n            'total_queries_extracted': total_queries,\n            'unique_patterns': unique_patterns\n        },\n        'suggestions': suggestions\n    }\n    \n    # Write report JSON\n    report_path = 'report.json'\n    try:\n        with open(report_path, 'w', encoding='utf-8', newline='') as f:\n            json.dump(report_data, f, indent=2)\n    except Exception as e:\n        print(f\"Error writing report: {e}\", file=sys.stderr)\n        sys.exit(1)\n    \n    # Print report to stdout as required\n    print(json.dumps(report_data, indent=2))\n\nif __name__ == '__main__':\n    main()\nEOF"]
[99.012261, "o", "#!/home/user/query_optimization/env/bin/python3\r\n\u001b[?2004l\r"]
[99.012277, "o", "\u001b[?2004h"]
[99.012293, "o", "> \"\"\""]
[99.012309, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[99.012328, "o", "> "]
[99.012483, "o", "SQL Query Log Analysis & Optimization Script\r\n\u001b[?2004l\r"]
[99.012499, "o", "\u001b[?2004h"]
[99.012515, "o", "> \"\"\""]
[99.012531, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[99.012547, "o", "> "]
[99.012605, "o", "import argparse\r\n\u001b[?2004l\r"]
[99.012631, "o", "\u001b[?2004h"]
[99.012648, "o", "> "]
[99.012707, "o", "import configparser\r\n\u001b[?2004l\r"]
[99.012725, "o", "\u001b[?2004h> "]
[99.01277, "o", "import csv\r\n\u001b[?2004l\r"]
[99.012787, "o", "\u001b[?2004h"]
[99.012804, "o", "> "]
[99.012851, "o", "import hashlib\r\n"]
[99.014631, "o", "\u001b[?2004l\r\u001b[?2004h> import os\r\n\u001b[?2004l\r\u001b[?2004h> import re\r\n\u001b[?2004l\r\u001b[?2004h> import sqlparse\r\n\u001b[?2004l\r\u001b[?2004h> import sys\r\n\u001b[?2004l\r\u001b[?2004h> from datetime import datetime\r\n\u001b[?2004l\r\u001b[?2004h> import json\r\n\u001b[?2004l\r\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h> # Read configuration file\r\n\u001b[?2004l\r\u001b[?2004h> def load_config():\r\n\u001b[?2004l\r\u001b[?2004h>     config = configparser.ConfigParser()\r\n\u001b[?2004l\r\u001b[?2004h>     config.read('/home/user/db_tools/db_config.ini')\r\n\u001b[?2004l\r\u001b[?2004h>     return config['DEFAULT']\r\n\u001b[?2004l\r\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h> # Parse log line function\r\n\u001b[?2004l\r\u001b[?2004h> def parse_log_line(line, date_str, min_duration_ms):\r\n\u001b[?2004l\r\u001b[?2004h>     parts = [p.strip() for p in line.split('|')]\r\n\u001b[?2004l\r\u001b[?2004h>     if len(parts) != 4:\r\n\u001b[?2004l\r\u001b[?2004h>         return None\r\n\u001b[?2004l\r\u001b[?2004h>     \r\n\u001b[?2004l\r\u001b[?2004h>     timestamp_str, duration_str, level, message = parts\r\n\u001b[?2004l\r\u001b[?2004h>     \r\n\u001b[?2004l\r\u001b[?2004h>     # Validate timestamp format\r\n\u001b[?2004l\r\u001b[?2004h>     try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[99.014843, "o", "        timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))\r\n\u001b[?2004l\r"]
[99.014868, "o", "\u001b[?2004h> "]
[99.01506, "o", "        if timestamp.date().isoformat() != date_str:\r\n\u001b[?2004l\r"]
[99.015079, "o", "\u001b[?2004h> "]
[99.01518, "o", "            return None\r\n\u001b[?2004l\r"]
[99.015206, "o", "\u001b[?2004h> "]
[99.015286, "o", "    except ValueError:\r\n\u001b[?2004l\r"]
[99.015304, "o", "\u001b[?2004h> "]
[99.015379, "o", "        return None\r\n\u001b[?2004l\r"]
[99.015397, "o", "\u001b[?2004h> "]
[99.015431, "o", "    \r\n\u001b[?2004l\r"]
[99.01563, "o", "\u001b[?2004h>     # Validate level\r\n\u001b[?2004l\r\u001b[?2004h>     if level != '[QUERY]':\r\n\u001b[?2004l\r\u001b[?2004h> "]
[99.015669, "o", "        return None\r\n\u001b[?2004l\r"]
[99.015687, "o", "\u001b[?2004h> "]
[99.015707, "o", "    \r\n\u001b[?2004l\r"]
[99.015725, "o", "\u001b[?2004h> "]
[99.015858, "o", "    # Validate and parse duration\r\n\u001b[?2004l\r"]
[99.015877, "o", "\u001b[?2004h> "]
[99.015914, "o", "    try:\r\n\u001b[?2004l\r"]
[99.015929, "o", "\u001b[?2004h"]
[99.015945, "o", "> "]
[99.016115, "o", "        duration_ms = float(duration_str.rstrip('ms'))\r\n\u001b[?2004l\r"]
[99.016138, "o", "\u001b[?2004h> "]
[99.016283, "o", "        if duration_ms < min_duration_ms:\r\n\u001b[?2004l\r"]
[99.016305, "o", "\u001b[?2004h> "]
[99.016396, "o", "            return None\r\n\u001b[?2004l\r"]
[99.016412, "o", "\u001b[?2004h"]
[99.016434, "o", "> "]
[99.016505, "o", "    except ValueError:\r\n\u001b[?2004l\r"]
[99.016526, "o", "\u001b[?2004h> "]
[99.016603, "o", "        return None\r\n\u001b[?2004l\r"]
[99.016634, "o", "\u001b[?2004h> "]
[99.016646, "o", "    \r\n\u001b[?2004l\r"]
[99.016669, "o", "\u001b[?2004h> "]
[99.016825, "o", "    return timestamp, duration_ms, message.strip()\r\n\u001b[?2004l\r"]
[99.016843, "o", "\u001b[?2004h> "]
[99.016861, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[99.016879, "o", "> "]
[99.01696, "o", "# Normalize SQL function\r\n\u001b[?2004l\r"]
[99.016976, "o", "\u001b[?2004h"]
[99.016992, "o", "> "]
[99.017097, "o", "def normalize_sql(query_text):\r\n\u001b[?2004l\r"]
[99.017114, "o", "\u001b[?2004h"]
[99.017132, "o", "> "]
[99.017161, "o", "    try:\r\n\u001b[?2004l\r"]
[99.017178, "o", "\u001b[?2004h> "]
[99.017353, "o", "        parsed = sqlparse.parse(query_text)[0]\r\n\u001b[?2004l\r"]
[99.01737, "o", "\u001b[?2004h"]
[99.01739, "o", "> "]
[99.017507, "o", "        formatted = sqlparse.format(\r\n\u001b[?2004l\r"]
[99.017523, "o", "\u001b[?2004h"]
[99.017544, "o", "> "]
[99.017599, "o", "            parsed, \r\n\u001b[?2004l\r"]
[99.01762, "o", "\u001b[?2004h> "]
[99.017751, "o", "            keyword_case='upper', \r\n\u001b[?2004l\r"]
[99.017775, "o", "\u001b[?2004h> "]
[99.0179, "o", "            strip_comments=True, \r\n\u001b[?2004l\r"]
[99.017917, "o", "\u001b[?2004h> "]
[99.018018, "o", "            reindent=False\r\n\u001b[?2004l\r"]
[99.018035, "o", "\u001b[?2004h"]
[99.018052, "o", "> "]
[99.018077, "o", "        )\r\n\u001b[?2004l\r"]
[99.018104, "o", "\u001b[?2004h> "]
[99.018282, "o", "        # Remove trailing semicolon and whitespace\r\n\u001b[?2004l\r"]
[99.018299, "o", "\u001b[?2004h"]
[99.018324, "o", "> "]
[99.018487, "o", "        return re.sub(r'[\\\\s;]+\\\\Z', '', formatted).strip()\r\n\u001b[?2004l\r"]
[99.018505, "o", "\u001b[?2004h"]
[99.018523, "o", "> "]
[99.018591, "o", "    except Exception:\r\n\u001b[?2004l\r"]
[99.018613, "o", "\u001b[?2004h"]
[99.018663, "o", "> "]
[99.018742, "o", "        return query_text\r\n\u001b[?2004l\r"]
[99.018761, "o", "\u001b[?2004h> "]
[99.018781, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[99.018801, "o", "> "]
[99.018953, "o", "# Extract table and columns from WHERE clause\r\n\u001b[?2004l\r"]
[99.01897, "o", "\u001b[?2004h"]
[99.018993, "o", "> "]
[99.01912, "o", "def extract_index_info(normalized_sql):\r\n\u001b[?2004l\r"]
[99.019138, "o", "\u001b[?2004h"]
[99.019154, "o", "> "]
[99.019179, "o", "    try:\r\n\u001b[?2004l\r"]
[99.019201, "o", "\u001b[?2004h> "]
[99.019384, "o", "        parsed = sqlparse.parse(normalized_sql)[0]\r\n\u001b[?2004l\r"]
[99.019401, "o", "\u001b[?2004h"]
[99.019417, "o", "> "]
[99.019506, "o", "        # Find WHERE token\r\n\u001b[?2004l\r"]
[99.019522, "o", "\u001b[?2004h"]
[99.019538, "o", "> "]
[99.019628, "o", "        where_token = None\r\n\u001b[?2004l\r"]
[99.019644, "o", "\u001b[?2004h> "]
[99.01976, "o", "        for token in parsed.tokens:\r\n\u001b[?2004l\r"]
[99.019776, "o", "\u001b[?2004h"]
[99.019797, "o", "> "]
[99.019926, "o", "            if token.value.upper() == 'WHERE':\r\n\u001b[?2004l\r"]
[99.019942, "o", "\u001b[?2004h"]
[99.019958, "o", "> "]
[99.020077, "o", "                where_token = token\r\n\u001b[?2004l\r"]
[99.020093, "o", "\u001b[?2004h"]
[99.020111, "o", "> "]
[99.020171, "o", "                break\r\n\u001b[?2004l\r"]
[99.020186, "o", "\u001b[?2004h"]
[99.020202, "o", "> "]
[99.020225, "o", "        \r\n\u001b[?2004l\r"]
[99.020241, "o", "\u001b[?2004h"]
[99.020257, "o", "> "]
[99.020352, "o", "        if not where_token:\r\n\u001b[?2004l\r"]
[99.020369, "o", "\u001b[?2004h> "]
[99.020484, "o", "            return None, None\r\n\u001b[?2004l\r"]
[99.020501, "o", "\u001b[?2004h"]
[99.020517, "o", "> "]
[99.020538, "o", "        \r\n\u001b[?2004l\r"]
[99.020555, "o", "\u001b[?2004h"]
[99.02057, "o", "> "]
[99.020684, "o", "        # Get tokens after WHERE\r\n\u001b[?2004l\r"]
[99.0207, "o", "\u001b[?2004h"]
[99.020716, "o", "> "]
[99.020803, "o", "        where_clause = ''\r\n\u001b[?2004l\r"]
[99.020819, "o", "\u001b[?2004h"]
[99.020835, "o", "> "]
[99.020966, "o", "        next_token = where_token.next\r\n\u001b[?2004l\r"]
[99.020983, "o", "\u001b[?2004h"]
[99.020999, "o", "> "]
[99.021403, "o", "        while next_token and next_token.value.upper() not in ['LIMIT', 'OFFSET', 'GROUP BY', 'ORDER BY', 'HAVING']:\r\n\u001b[?2004l\r"]
[99.021425, "o", "\u001b[?2004h> "]
[99.021578, "o", "            where_clause += str(next_token)\r\n\u001b[?2004l\r"]
[99.0216, "o", "\u001b[?2004h"]
[99.021621, "o", "> "]
[99.021737, "o", "            next_token = next_token.next\r\n\u001b[?2004l\r"]
[99.021755, "o", "\u001b[?2004h"]
[99.021772, "o", "> "]
[99.021795, "o", "        \r\n\u001b[?2004l\r"]
[99.021813, "o", "\u001b[?2004h"]
[99.021834, "o", "> "]
[99.021972, "o", "        # Extract table name and columns\r\n\u001b[?2004l\r"]
[99.021987, "o", "\u001b[?2004h> "]
[99.022287, "o", "        table_match = re.search(r'FROM\\\\s+(\\\\w+)', normalized_sql, re.IGNORECASE)\r\n\u001b[?2004l\r"]
[99.022303, "o", "\u001b[?2004h"]
[99.022319, "o", "> "]
[99.02241, "o", "        if not table_match:\r\n\u001b[?2004l\r"]
[99.022426, "o", "\u001b[?2004h"]
[99.022442, "o", "> "]
[99.02254, "o", "            return None, None\r\n\u001b[?2004l\r"]
[99.022557, "o", "\u001b[?2004h"]
[99.022573, "o", "> "]
[99.022597, "o", "        \r\n\u001b[?2004l\r"]
[99.022666, "o", "\u001b[?2004h> "]
[99.022757, "o", "        table_name = table_match.group(1)\r\n\u001b[?2004l\r"]
[99.022774, "o", "\u001b[?2004h"]
[99.022791, "o", "> "]
[99.022817, "o", "        \r\n\u001b[?2004l\r"]
[99.022833, "o", "\u001b[?2004h"]
[99.022849, "o", "> "]
[99.022984, "o", "        # Extract column=value patterns\r\n\u001b[?2004l\r"]
[99.023001, "o", "\u001b[?2004h"]
[99.023017, "o", "> "]
[99.023267, "o", "        conditions = re.findall(r'(\\\\w+)\\\\s*=\\\\s*[^\\\\s\\''\\\"]+', where_clause, re.IGNORECASE)\r\n\u001b[?2004l\r"]
[99.023294, "o", "\u001b[?2004h> "]
[99.023383, "o", "        if not conditions:\r\n\u001b[?2004l\r"]
[99.023402, "o", "\u001b[?2004h> "]
[99.023515, "o", "            return None, None\r\n\u001b[?2004l\r"]
[99.02353, "o", "\u001b[?2004h"]
[99.023546, "o", "> "]
[99.023569, "o", "        \r\n\u001b[?2004l\r"]
[99.023586, "o", "\u001b[?2004h"]
[99.023602, "o", "> "]
[99.023785, "o", "        columns = [cond[0] for cond in conditions]\r\n\u001b[?2004l\r"]
[99.023802, "o", "\u001b[?2004h> "]
[99.02384, "o", "        \r\n\u001b[?2004l\r"]
[99.023857, "o", "\u001b[?2004h"]
[99.023872, "o", "> "]
[99.023987, "o", "        return table_name, columns\r\n\u001b[?2004l\r"]
[99.024004, "o", "\u001b[?2004h"]
[99.02403, "o", "> "]
[99.024095, "o", "    except Exception:\r\n\u001b[?2004l\r"]
[99.024114, "o", "\u001b[?2004h> "]
[99.024212, "o", "        return None, None\r\n\u001b[?2004l\r"]
[99.024227, "o", "\u001b[?2004h"]
[99.024243, "o", "> \r\n\u001b[?2004l\r"]
[99.02426, "o", "\u001b[?2004h> "]
[99.02435, "o", "# Main analysis function\r\n\u001b[?2004l\r"]
[99.024366, "o", "\u001b[?2004h"]
[99.024382, "o", "> "]
[99.024588, "o", "def analyze_queries(log_path, date_str, min_duration_ms):\r\n\u001b[?2004l\r"]
[99.024613, "o", "\u001b[?2004h> "]
[99.024691, "o", "    raw_queries = []\r\n\u001b[?2004l\r"]
[99.024707, "o", "\u001b[?2004h"]
[99.024723, "o", "> "]
[99.02474, "o", "    \r\n\u001b[?2004l\r"]
[99.024756, "o", "\u001b[?2004h> "]
[99.024857, "o", "    # Read and parse log file\r\n\u001b[?2004l\r"]
[99.024872, "o", "\u001b[?2004h"]
[99.024888, "o", "> "]
[99.024913, "o", "    try:\r\n\u001b[?2004l\r"]
[99.024928, "o", "\u001b[?2004h"]
[99.024944, "o", "> "]
[99.025141, "o", "        with open(log_path, 'r', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[99.025157, "o", "\u001b[?2004h"]
[99.025173, "o", "> "]
[99.02526, "o", "            for line in f:\r\n\u001b[?2004l\r"]
[99.025275, "o", "\u001b[?2004h"]
[99.025291, "o", "> "]
[99.025417, "o", "                if not line.strip():\r\n\u001b[?2004l\r"]
[99.025433, "o", "\u001b[?2004h"]
[99.025451, "o", "> "]
[99.025548, "o", "                    continue\r\n\u001b[?2004l\r"]
[99.025564, "o", "\u001b[?2004h"]
[99.025582, "o", "> "]
[99.025842, "o", "                parsed = parse_log_line(line, date_str, min_duration_ms)\r\n\u001b[?2004l\r"]
[99.025858, "o", "\u001b[?2004h"]
[99.025875, "o", "> "]
[99.02596, "o", "                if parsed:\r\n\u001b[?2004l\r"]
[99.025976, "o", "\u001b[?2004h"]
[99.025992, "o", "> "]
[99.026122, "o", "                    raw_queries.append(parsed)\r\n\u001b[?2004l\r"]
[99.026139, "o", "\u001b[?2004h"]
[99.026155, "o", "> "]
[99.026258, "o", "    except FileNotFoundError:\r\n\u001b[?2004l\r"]
[99.026275, "o", "\u001b[?2004h"]
[99.026291, "o", "> "]
[99.02654, "o", "        print(f\"Error: Log file {log_path} not found\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[99.026557, "o", "\u001b[?2004h"]
[99.026573, "o", "> "]
[99.026634, "o", "        sys.exit(1)\r\n"]
[99.02665, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.026666, "o", "> "]
[99.026684, "o", "    \r\n\u001b[?2004l\r"]
[99.0267, "o", "\u001b[?2004h> "]
[99.026788, "o", "    if not raw_queries:\r\n\u001b[?2004l\r"]
[99.026805, "o", "\u001b[?2004h"]
[99.026821, "o", "> "]
[99.026977, "o", "        print(\"No qualifying queries found\")\r\n\u001b[?2004l\r"]
[99.026995, "o", "\u001b[?2004h> "]
[99.027087, "o", "        return None, []\r\n\u001b[?2004l\r"]
[99.027104, "o", "\u001b[?2004h"]
[99.027119, "o", "> "]
[99.027137, "o", "    \r\n\u001b[?2004l\r"]
[99.027153, "o", "\u001b[?2004h> "]
[99.027264, "o", "    # Prepare staging CSV data\r\n\u001b[?2004l\r"]
[99.027282, "o", "\u001b[?2004h"]
[99.027305, "o", "> "]
[99.02737, "o", "    staging_data = []\r\n\u001b[?2004l\r"]
[99.027387, "o", "\u001b[?2004h"]
[99.027405, "o", "> "]
[99.027614, "o", "    for timestamp, duration_ms, query_text in raw_queries:\r\n\u001b[?2004l\r"]
[99.027631, "o", "\u001b[?2004h> "]
[99.027735, "o", "        staging_data.append([\r\n\u001b[?2004l\r"]
[99.02775, "o", "\u001b[?2004h"]
[99.027767, "o", "> "]
[99.027869, "o", "            timestamp.isoformat(),\r\n\u001b[?2004l\r"]
[99.027885, "o", "\u001b[?2004h"]
[99.027902, "o", "> "]
[99.027982, "o", "            duration_ms,\r\n\u001b[?2004l\r"]
[99.027999, "o", "\u001b[?2004h"]
[99.028015, "o", "> "]
[99.028086, "o", "            query_text\r\n\u001b[?2004l\r"]
[99.028105, "o", "\u001b[?2004h> "]
[99.028135, "o", "        ])\r\n\u001b[?2004l\r"]
[99.028151, "o", "\u001b[?2004h"]
[99.028167, "o", ">     \r\n"]
[99.028183, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.028198, "o", "> "]
[99.028273, "o", "    # Create staging directory\r\n\u001b[?2004l\r"]
[99.028291, "o", "\u001b[?2004h> "]
[99.028393, "o", "    os.makedirs('staging', exist_ok=True)\r\n\u001b[?2004l\r"]
[99.028416, "o", "\u001b[?2004h"]
[99.028428, "o", ">     \r\n\u001b[?2004l\r"]
[99.028471, "o", "\u001b[?2004h> "]
[99.028604, "o", "    # Write staging CSV with specific encoding and line endings\r\n\u001b[?2004l\r"]
[99.028626, "o", "\u001b[?2004h> "]
[99.028771, "o", "    csv_path = os.path.join('staging', 'raw_extract.csv')\r\n\u001b[?2004l\r"]
[99.028799, "o", "\u001b[?2004h> "]
[99.028819, "o", "    try:\r\n\u001b[?2004l\r"]
[99.028837, "o", "\u001b[?2004h> "]
[99.029019, "o", "        with open(csv_path, 'w', encoding='ISO-8859-1', newline='') as f:\r\n\u001b[?2004l\r"]
[99.029041, "o", "\u001b[?2004h> "]
[99.029182, "o", "            writer = csv.writer(f, lineterminator='\\\\r\\\\n')\r\n\u001b[?2004l\r"]
[99.029199, "o", "\u001b[?2004h"]
[99.029217, "o", "> "]
[99.029379, "o", "            writer.writerow(['timestamp', 'duration_ms', 'query_text'])\r\n\u001b[?2004l\r"]
[99.0294, "o", "\u001b[?2004h> "]
[99.029501, "o", "            writer.writerows(staging_data)\r\n\u001b[?2004l\r"]
[99.029516, "o", "\u001b[?2004h"]
[99.029534, "o", "> "]
[99.029588, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[99.029603, "o", "\u001b[?2004h"]
[99.029616, "o", "> "]
[99.029772, "o", "        print(f\"Error writing staging CSV: {e}\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[99.029794, "o", "\u001b[?2004h> "]
[99.029843, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[99.029858, "o", "\u001b[?2004h"]
[99.029877, "o", ">     \r\n"]
[99.029896, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.02992, "o", "> "]
[99.030017, "o", "    # Compute SHA-256 checksum of the CSV file\r\n\u001b[?2004l\r"]
[99.030034, "o", "\u001b[?2004h"]
[99.030052, "o", "> "]
[99.030116, "o", "    sha256 = hashlib.sha256()\r\n\u001b[?2004l\r"]
[99.030132, "o", "\u001b[?2004h"]
[99.03015, "o", "> "]
[99.030174, "o", "    try:\r\n\u001b[?2004l\r"]
[99.030191, "o", "\u001b[?2004h> "]
[99.030283, "o", "        with open(csv_path, 'rb') as f:\r\n\u001b[?2004l\r"]
[99.030298, "o", "\u001b[?2004h"]
[99.030316, "o", "> "]
[99.030394, "o", "            sha256.update(f.read())\r\n\u001b[?2004l\r"]
[99.03041, "o", "\u001b[?2004h"]
[99.030429, "o", "> "]
[99.030485, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[99.030501, "o", "\u001b[?2004h"]
[99.03052, "o", "> "]
[99.030669, "o", "        print(f\"Error computing checksum: {e}\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[99.030684, "o", "\u001b[?2004h"]
[99.030702, "o", "> "]
[99.030743, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[99.030762, "o", "\u001b[?2004h"]
[99.03078, "o", ">     \r\n"]
[99.030798, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.030816, "o", "> "]
[99.030891, "o", "    checksum = sha256.hexdigest()\r\n\u001b[?2004l\r"]
[99.030908, "o", "\u001b[?2004h"]
[99.030926, "o", ">     \r\n"]
[99.030946, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.030964, "o", "> "]
[99.031128, "o", "    # Process queries: normalize, deduplicate, and generate suggestions\r\n\u001b[?2004l\r"]
[99.031154, "o", "\u001b[?2004h> "]
[99.031199, "o", "    query_data = {}\r\n\u001b[?2004l\r"]
[99.031216, "o", "\u001b[?2004h> "]
[99.031359, "o", "    for timestamp, duration_ms, query_text in raw_queries:\r\n\u001b[?2004l\r"]
[99.031375, "o", "\u001b[?2004h"]
[99.031391, "o", "> "]
[99.031497, "o", "        normalized = normalize_sql(query_text)\r\n\u001b[?2004l\r"]
[99.031512, "o", "\u001b[?2004h"]
[99.031529, "o", "> "]
[99.031545, "o", "        \r\n\u001b[?2004l\r"]
[99.031562, "o", "\u001b[?2004h> "]
[99.031669, "o", "        if normalized not in query_data:\r\n\u001b[?2004l\r"]
[99.031691, "o", "\u001b[?2004h> "]
[99.031843, "o", "            table_name, columns = extract_index_info(normalized)\r\n\u001b[?2004l\r"]
[99.03186, "o", "\u001b[?2004h> "]
[99.031957, "o", "            if table_name and columns:\r\n\u001b[?2004l\r"]
[99.031973, "o", "\u001b[?2004h"]
[99.031989, "o", "> "]
[99.03213, "o", "                # Format columns for index name and definition\r\n\u001b[?2004l\r"]
[99.032146, "o", "\u001b[?2004h> "]
[99.032266, "o", "                col_str_name = '_'.join(columns)\r\n\u001b[?2004l\r"]
[99.032283, "o", "\u001b[?2004h"]
[99.032299, "o", "> "]
[99.032406, "o", "                col_str_def = ', '.join(columns)\r\n\u001b[?2004l\r"]
[99.032423, "o", "\u001b[?2004h"]
[99.032439, "o", "> "]
[99.032711, "o", "                index_suggestion = f\"CREATE INDEX idx_{table_name}_{col_str_name} ON {table_name}({col_str_def});\"\r\n\u001b[?2004l\r"]
[99.032736, "o", "\u001b[?2004h> "]
[99.032774, "o", "            else:\r\n\u001b[?2004l\r"]
[99.032795, "o", "\u001b[?2004h"]
[99.032813, "o", "> "]
[99.032888, "o", "                index_suggestion = None\r\n\u001b[?2004l\r"]
[99.032904, "o", "\u001b[?2004h"]
[99.032921, "o", "> "]
[99.032945, "o", "            \r\n\u001b[?2004l\r"]
[99.032963, "o", "\u001b[?2004h> "]
[99.033057, "o", "            query_data[normalized] = {\r\n\u001b[?2004l\r"]
[99.033074, "o", "\u001b[?2004h> "]
[99.03319, "o", "                'first_timestamp': timestamp,\r\n\u001b[?2004l\r"]
[99.033207, "o", "\u001b[?2004h> "]
[99.033277, "o", "                'count': 0,\r\n\u001b[?2004l\r"]
[99.033294, "o", "\u001b[?2004h"]
[99.033312, "o", "> "]
[99.033425, "o", "                'index_suggestion': index_suggestion\r\n\u001b[?2004l\r"]
[99.033452, "o", "\u001b[?2004h> "]
[99.033484, "o", "            }\r\n\u001b[?2004l\r"]
[99.033506, "o", "\u001b[?2004h> "]
[99.033533, "o", "        \r\n\u001b[?2004l\r"]
[99.033556, "o", "\u001b[?2004h> "]
[99.033674, "o", "        query_data[normalized]['count'] += 1\r\n\u001b[?2004l\r"]
[99.033699, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[99.033718, "o", "\u001b[?2004h"]
[99.033735, "o", "> "]
[99.033867, "o", "    # Prepare suggestions list sorted by first occurrence\r\n\u001b[?2004l\r"]
[99.03389, "o", "\u001b[?2004h> "]
[99.03394, "o", "    suggestions = []\r\n\u001b[?2004l\r"]
[99.033963, "o", "\u001b[?2004h> "]
[99.034075, "o", "    for normalized, data in query_data.items():\r\n\u001b[?2004l\r"]
[99.034092, "o", "\u001b[?2004h> "]
[99.034189, "o", "        if data['index_suggestion']:\r\n\u001b[?2004l\r"]
[99.034218, "o", "\u001b[?2004h> "]
[99.034287, "o", "            suggestions.append({\r\n\u001b[?2004l\r"]
[99.034312, "o", "\u001b[?2004h> "]
[99.034449, "o", "                'id': data['first_timestamp'].isoformat(),\r\n\u001b[?2004l\r"]
[99.034474, "o", "\u001b[?2004h> "]
[99.034579, "o", "                'normalized_sql': normalized,\r\n\u001b[?2004l\r"]
[99.034602, "o", "\u001b[?2004h> "]
[99.034768, "o", "                'index_suggestion': data['index_suggestion']\r\n\u001b[?2004l\r"]
[99.034787, "o", "\u001b[?2004h> "]
[99.03483, "o", "            })\r\n\u001b[?2004l\r"]
[99.034847, "o", "\u001b[?2004h"]
[99.034865, "o", ">     \r\n"]
[99.034882, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.034901, "o", "> "]
[99.035001, "o", "    suggestions.sort(key=lambda x: x['id'])\r\n\u001b[?2004l\r"]
[99.035022, "o", "\u001b[?2004h> "]
[99.035038, "o", "    \r\n\u001b[?2004l\r"]
[99.035058, "o", "\u001b[?2004h> "]
[99.035138, "o", "    return checksum, suggestions\r\n\u001b[?2004l\r"]
[99.035155, "o", "\u001b[?2004h"]
[99.035177, "o", "> \r\n\u001b[?2004l\r\u001b[?2004h"]
[99.035192, "o", "> "]
[99.035221, "o", "def main():\r\n\u001b[?2004l\r"]
[99.035244, "o", "\u001b[?2004h> "]
[99.035325, "o", "    # Parse command line arguments\r\n\u001b[?2004l\r"]
[99.035343, "o", "\u001b[?2004h> "]
[99.035525, "o", "    parser = argparse.ArgumentParser(description='SQL Query Log Analyzer')\r\n\u001b[?2004l\r"]
[99.035542, "o", "\u001b[?2004h"]
[99.035558, "o", "> "]
[99.035777, "o", "    parser.add_argument('--date', required=True, help='Date to analyze in YYYY-MM-DD format')\r\n\u001b[?2004l\r"]
[99.035795, "o", "\u001b[?2004h"]
[99.035819, "o", "> "]
[99.035879, "o", "    args = parser.parse_args()\r\n\u001b[?2004l\r"]
[99.035896, "o", "\u001b[?2004h"]
[99.035914, "o", ">     \r\n"]
[99.035931, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.035948, "o", "> "]
[99.036014, "o", "    # Validate date format\r\n\u001b[?2004l\r"]
[99.036035, "o", "\u001b[?2004h> "]
[99.036061, "o", "    try:\r\n\u001b[?2004l\r"]
[99.036081, "o", "\u001b[?2004h> "]
[99.036222, "o", "        datetime.fromisoformat(args.date.replace('-', ''))\r\n\u001b[?2004l\r"]
[99.036245, "o", "\u001b[?2004h"]
[99.036261, "o", "> "]
[99.036303, "o", "    except ValueError:\r\n\u001b[?2004l\r"]
[99.036319, "o", "\u001b[?2004h"]
[99.036336, "o", "> "]
[99.036497, "o", "        print(\"Invalid date format. Use YYYY-MM-DD\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[99.036518, "o", "\u001b[?2004h> "]
[99.036568, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[99.036585, "o", "\u001b[?2004h> "]
[99.036611, "o", "    \r\n\u001b[?2004l\r"]
[99.036629, "o", "\u001b[?2004h> "]
[99.036698, "o", "    # Load configuration\r\n\u001b[?2004l\r"]
[99.036719, "o", "\u001b[?2004h> "]
[99.036787, "o", "    config = load_config()\r\n\u001b[?2004l\r"]
[99.036809, "o", "\u001b[?2004h> "]
[99.036914, "o", "    log_path = config.get('log_file_path')\r\n\u001b[?2004l\r"]
[99.036931, "o", "\u001b[?2004h> "]
[99.037104, "o", "    min_duration_ms = config.getfloat('min_duration_ms', fallback=0.0)\r\n\u001b[?2004l\r"]
[99.03712, "o", "\u001b[?2004h"]
[99.037136, "o", "> "]
[99.037155, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[99.037176, "o", "> "]
[99.037218, "o", "    if not log_path:\r\n\u001b[?2004l\r"]
[99.037234, "o", "\u001b[?2004h> "]
[99.037401, "o", "        print(\"log_file_path not found in config\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[99.037418, "o", "\u001b[?2004h"]
[99.037436, "o", "> "]
[99.037478, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[99.037495, "o", "\u001b[?2004h"]
[99.037511, "o", "> "]
[99.037528, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[99.037548, "o", "> "]
[99.037597, "o", "    # Perform analysis\r\n\u001b[?2004l\r"]
[99.037622, "o", "\u001b[?2004h"]
[99.037642, "o", "> "]
[99.03782, "o", "    checksum, suggestions = analyze_queries(log_path, args.date, min_duration_ms)\r\n\u001b[?2004l\r"]
[99.037842, "o", "\u001b[?2004h> "]
[99.037902, "o", "    if checksum is None:\r\n\u001b[?2004l\r"]
[99.037919, "o", "\u001b[?2004h> "]
[99.037963, "o", "        return\r\n\u001b[?2004l\r"]
[99.03798, "o", "\u001b[?2004h"]
[99.037996, "o", "> "]
[99.038013, "o", "    \r\n\u001b[?2004l\r"]
[99.03803, "o", "\u001b[?2004h> "]
[99.038089, "o", "    # Prepare report data\r\n\u001b[?2004l\r"]
[99.038105, "o", "\u001b[?2004h"]
[99.038122, "o", "> "]
[99.038281, "o", "    total_queries = sum(data['count'] for data in query_data.values())\r\n\u001b[?2004l\r"]
[99.038298, "o", "\u001b[?2004h"]
[99.038321, "o", "> "]
[99.0384, "o", "    unique_patterns = len(query_data)\r\n\u001b[?2004l\r"]
[99.038416, "o", "\u001b[?2004h"]
[99.038433, "o", "> "]
[99.038451, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[99.038471, "o", "> "]
[99.038513, "o", "    report_data = {\r\n\u001b[?2004l\r"]
[99.038531, "o", "\u001b[?2004h"]
[99.038548, "o", "> "]
[99.038586, "o", "        'meta': {\r\n\u001b[?2004l\r"]
[99.038603, "o", "\u001b[?2004h"]
[99.038625, "o", "> "]
[99.038715, "o", "            'date_analyzed': args.date,\r\n\u001b[?2004l\r"]
[99.038732, "o", "\u001b[?2004h> "]
[99.038843, "o", "            'staging_file_sha256': checksum\r\n\u001b[?2004l\r"]
[99.038864, "o", "\u001b[?2004h> "]
[99.038896, "o", "        },\r\n\u001b[?2004l\r"]
[99.038917, "o", "\u001b[?2004h> "]
[99.038965, "o", "        'stats': {\r\n\u001b[?2004l\r"]
[99.038981, "o", "\u001b[?2004h"]
[99.038998, "o", "> "]
[99.039121, "o", "            'total_queries_extracted': total_queries,\r\n\u001b[?2004l\r"]
[99.039138, "o", "\u001b[?2004h> "]
[99.039255, "o", "            'unique_patterns': unique_patterns\r\n\u001b[?2004l\r"]
[99.039271, "o", "\u001b[?2004h"]
[99.039288, "o", "> "]
[99.039312, "o", "        },\r\n\u001b[?2004l\r"]
[99.03933, "o", "\u001b[?2004h> "]
[99.039418, "o", "        'suggestions': suggestions\r\n\u001b[?2004l\r"]
[99.039436, "o", "\u001b[?2004h"]
[99.039455, "o", "> "]
[99.039472, "o", "    }\r\n\u001b[?2004l\r\u001b[?2004h"]
[99.03949, "o", "> "]
[99.039511, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[99.039537, "o", "> "]
[99.039579, "o", "    # Write report JSON\r\n\u001b[?2004l\r"]
[99.039595, "o", "\u001b[?2004h"]
[99.039619, "o", "> "]
[99.039688, "o", "    report_path = 'report.json'\r\n\u001b[?2004l\r"]
[99.039705, "o", "\u001b[?2004h"]
[99.039723, "o", "> "]
[99.039742, "o", "    try:\r\n"]
[99.03976, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.039778, "o", "> "]
[99.039942, "o", "        with open(report_path, 'w', encoding='utf-8', newline='') as f:\r\n\u001b[?2004l\r"]
[99.039968, "o", "\u001b[?2004h> "]
[99.040079, "o", "            json.dump(report_data, f, indent=2)\r\n\u001b[?2004l\r"]
[99.040096, "o", "\u001b[?2004h"]
[99.040114, "o", "> "]
[99.040171, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[99.040187, "o", "\u001b[?2004h"]
[99.040204, "o", "> "]
[99.040346, "o", "        print(f\"Error writing report: {e}\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[99.040368, "o", "\u001b[?2004h> "]
[99.040417, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[99.040434, "o", "\u001b[?2004h"]
[99.040451, "o", ">     \r\n"]
[99.040468, "o", "\u001b[?2004l\r\u001b[?2004h"]
[99.040482, "o", "> "]
[99.040581, "o", "    # Print report to stdout as required\r\n\u001b[?2004l\r"]
[99.040598, "o", "\u001b[?2004h> "]
[99.040723, "o", "    print(json.dumps(report_data, indent=2))\r\n\u001b[?2004l\r"]
[99.040738, "o", "\u001b[?2004h"]
[99.040768, "o", "> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[99.040837, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r"]
[99.040854, "o", "\u001b[?2004h> "]
[99.040893, "o", "    main()\r\n\u001b[?2004l\r"]
[99.040907, "o", "\u001b[?2004h> "]
[99.040924, "o", "EOF"]
[103.419521, "i", "chmod +x analyze_and_optimize.py\n"]
[103.419698, "o", "chmod +x analyze_and_optimize.py\r\n\u001b[?2004l\r"]
[103.419725, "o", "\u001b[?2004h"]
[103.419746, "o", "> "]
[103.538629, "i", "./analyze_and_optimize.py --date 2023-03-15\n"]
[103.538839, "o", "./analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[103.538858, "o", "\u001b[?2004h"]
[103.538917, "o", "> "]
[110.449247, "i", "mkdir -p /home/user/db_tools\n"]
[110.449364, "o", "mkdir -p /home/user/db_tools\r\n"]
[110.449397, "o", "\u001b[?2004l\r\u001b[?2004h"]
[110.449418, "o", "> "]
[110.56341, "i", "cat > /home/user/db_tools/db_config.ini << 'EOF'\n[DEFAULT]\nlog_file_path = /home/user/query_logs/sql_queries.log\nmin_duration_ms = 10.0\nEOF"]
[110.563575, "o", "cat > /home/user/db_tools/db_config.ini << 'EOF'\r\n"]
[110.563625, "o", "\u001b[?2004l\r"]
[110.563635, "o", "\u001b[?2004h> "]
[110.563703, "o", "[DEFAULT]\r\n\u001b[?2004l\r\u001b[?2004h> "]
[110.563832, "o", "log_file_path = /home/user/query_logs/sql_queries.log\r\n\u001b[?2004l\r"]
[110.563855, "o", "\u001b[?2004h> "]
[110.563924, "o", "min_duration_ms = 10.0\r\n\u001b[?2004l\r"]
[110.563945, "o", "\u001b[?2004h"]
[110.563962, "o", "> EOF"]
[119.771242, "i", "mkdir -p /home/user/query_logs\n"]
[119.771367, "o", "mkdir -p /home/user/query_logs\r\n\u001b[?2004l\r"]
[119.771551, "o", "\u001b[?2004h> "]
[119.895403, "i", "cat > /home/user/query_logs/sql_queries.log << 'EOF'\n2023-03-15T08:30:00Z | 25ms | [QUERY] | SELECT * FROM users WHERE id = 123;\n2023-03-15T09:15:00Z | 5ms | [QUERY] | SELECT name FROM products WHERE category = 'electronics';\n2023-03-15T10:45:00Z | 150ms | [QUERY] | SELECT * FROM orders WHERE status = 'pending' AND created_at > '2023-03-14';\n2023-03-15T11:20:00Z | 8ms | [QUERY] | UPDATE users SET last_login = NOW() WHERE id = 456;\n2023-03-15T12:00:00Z | 200ms | [QUERY] | SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id WHERE u.active = true;\n2023-03-16T14:30:00Z | 30ms | [QUERY] | SELECT * FROM customers WHERE country = 'USA';\nEOF"]
[119.895586, "o", "cat > /home/user/query_logs/sql_queries.log << 'EOF'\r\n\u001b[?2004l\r"]
[119.895632, "o", "\u001b[?2004h> "]
[119.895833, "o", "2023-03-15T08:30:00Z | 25ms | [QUERY] | SELECT * FROM users WHERE id = 123;\r\n\u001b[?2004l\r"]
[119.895856, "o", "\u001b[?2004h> "]
[119.896101, "o", "2023-03-15T09:15:00Z | 5ms | [QUERY] | SELECT name FROM products WHERE category = 'electronics';\r\n\u001b[?2004l\r"]
[119.896119, "o", "\u001b[?2004h> "]
[119.89642, "o", "2023-03-15T10:45:00Z | 150ms | [QUERY] | SELECT * FROM orders WHERE status = 'pending' AND created_at > '2023-03-14';\r\n\u001b[?2004l\r"]
[119.896444, "o", "\u001b[?2004h> "]
[119.896679, "o", "2023-03-15T11:20:00Z | 8ms | [QUERY] | UPDATE users SET last_login = NOW() WHERE id = 456;\r\n\u001b[?2004l\r"]
[119.896699, "o", "\u001b[?2004h> "]
[119.897043, "o", "2023-03-15T12:00:00Z | 200ms | [QUERY] | SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id WHERE u.active = true;\r\n\u001b[?2004l\r"]
[119.897071, "o", "\u001b[?2004h> "]
[119.897289, "o", "2023-03-16T14:30:00Z | 30ms | [QUERY] | SELECT * FROM customers WHERE country = 'USA';\r\n\u001b[?2004l\r"]
[119.897309, "o", "\u001b[?2004h> "]
[119.897326, "o", "EOF"]
[123.911032, "i", "./analyze_and_optimize.py --date 2023-03-15\n"]
[123.911185, "o", "./analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[123.911218, "o", "\u001b[?2004h"]
[123.911237, "o", "> "]
[131.313601, "i", "cat /home/user/query_logs/sql_queries.log\n"]
[131.313775, "o", "cat /home/user/query_logs/sql_queries.log\r\n\u001b[?2004l\r"]
[131.31381, "o", "\u001b[?2004h> "]
[131.791282, "i", "ls -la staging/\n"]
[131.791458, "o", "ls -la staging/\r\n\u001b[?2004l\r"]
[131.791523, "o", "\u001b[?2004h> "]
[131.925963, "i", "cat staging/raw_extract.csv\n"]
[131.926066, "o", "cat staging/raw_extract.csv\r\n"]
[131.92608, "o", "\u001b[?2004l\r"]
[131.926188, "o", "\u001b[?2004h> "]
[137.480127, "i", "pwd\n"]
[137.480308, "o", "pwd\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.480325, "o", "> "]
[137.601202, "i", "ls -la\n"]
[137.601297, "o", "ls -la\r\n\u001b[?2004l\r"]
[137.601373, "o", "\u001b[?2004h> "]
[137.699943, "i", "python3 analyze_and_optimize.py --date 2023-03-15\n"]
[137.699977, "o", "python3 analyze_and_optimize.py --date 2023-03-15"]
[137.700106, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.700218, "o", "> "]
[146.3177, "i", "echo \"Testing environment...\"\n"]
[146.317858, "o", "echo \"Testing environment...\"\r\n\u001b[?2004l\r\u001b[?2004h> "]
[146.437574, "i", "ls -la /home/user/db_tools/db_config.ini\n"]
[146.43777, "o", "ls -la /home/user/db_tools/db_config.ini\r\n\u001b[?2004l\r\u001b[?2004h> "]
[146.544801, "i", "cat /home/user/db_tools/db_config.ini\n"]
[146.54498, "o", "cat /home/user/db_tools/db_config.ini\r\n\u001b[?2004l\r\u001b[?2004h> "]
[146.66114, "i", "python3 -c \"import sqlparse; print('sqlparse loaded')\"\n"]
[146.661407, "o", "python3 -c \"import sqlparse; print('sqlparse loaded')\"\r\n"]
[146.661433, "o", "\u001b[?2004l\r"]
[146.661494, "o", "\u001b[?2004h> "]
[151.276978, "i", "./analyze_and_optimize.py --date 2023-03-15 2>&1\n"]
[151.277179, "o", "./analyze_and_optimize.py --date 2023-03-15 2>&1\r\n"]
[151.277308, "o", "\u001b[?2004l\r\u001b[?2004h"]
[151.27738, "o", "> "]
[158.385518, "i", "sed -i 's/print(\"No qualifying queries found\")/print(\"DEBUG: No qualifying queries found\")/g' analyze_and_optimize.py\n"]
[158.385955, "o", "sed -i 's/print(\"No qualifying queries found\")/print(\"DEBUG: No qualifying queries found\")/g' analyze_and_optimize.py"]
[158.386141, "o", "\r\n"]
[158.386223, "o", "\u001b[?2004l\r"]
[158.38632, "o", "\u001b[?2004h"]
[158.386408, "o", "> "]
[158.613146, "i", "python3 analyze_and_optimize.py --date 2023-03-15\n"]
[158.613341, "o", "python3 analyze_and_optimize.py --date 2023-03-15"]
[158.613576, "o", "\r\n"]
[158.613626, "o", "\u001b[?2004l\r"]
[158.613668, "o", "\u001b[?2004h"]
[158.613711, "o", "> "]
[164.258791, "i", "cat /home/user/query_optimization/report.json\n"]
[164.258987, "o", "cat /home/user/query_optimization/report.json"]
[164.259081, "o", "\r\n"]
[164.259129, "o", "\u001b[?2004l\r"]
[164.259194, "o", "\u001b[?2004h"]
[164.259228, "o", "> "]
[169.80827, "i", "cat /home/user/query_optimization/report.json 2>/dev/null || echo \"Report not generated as expected\"\n"]
[169.808559, "o", "cat /home/user/query_optimization/report.json 2>/dev/null || echo \"Report not generated as expected\"\r\n"]
[169.808616, "o", "\u001b[?2004l\r\u001b[?2004h> "]
