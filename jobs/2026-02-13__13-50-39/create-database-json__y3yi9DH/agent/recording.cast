{"version": 2, "width": 160, "height": 40, "timestamp": 1771009184, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.008306, "o", "\u001b[?2004h"]
[0.008834, "o", "root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user# "]
[1.014696, "i", "clear\r"]
[1.014899, "o", "clear\r\n\u001b[?2004l\r"]
[1.016571, "o", "\u001b[H\u001b[J"]
[1.016809, "o", "\u001b[?2004h"]
[1.016871, "o", "root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user# "]
[16.564761, "i", "cd /home/user\n"]
[16.564941, "o", "cd /home/user\r\n\u001b[?2004l\r"]
[16.565093, "o", "\u001b[?2004hroot@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user# "]
[17.054646, "i", "mkdir -p query_optimization\n"]
[17.054893, "o", "mkdir -p query_optimization\r\n\u001b[?2004l\r"]
[17.057873, "o", "\u001b[?2004h"]
[17.05794, "o", "root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user# "]
[17.550571, "i", "cd query_optimization\n"]
[17.550726, "o", "cd query_optimization\r\n\u001b[?2004l\r"]
[17.550793, "o", "\u001b[?2004hroot@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[18.12654, "i", "cat /home/user/db_tools/db_config.ini\n"]
[18.126737, "o", "cat /home/user/db_tools/db_config.ini\r\n\u001b[?2004l\r"]
[18.128207, "o", "[settings]\r\nlog_file_path = /var/log/db_app/system.log\r\nmin_duration_ms = 20\r\n"]
[18.128516, "o", "\u001b[?2004hroot@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[33.293582, "i", "python3 -m venv env\n"]
[33.29374, "o", "python3 -m venv env\r\n\u001b[?2004l\r"]
[34.963053, "i", "source env/bin/activate\n"]
[34.963159, "o", "source env/bin/activate\r\n"]
[35.238503, "o", "\u001b[?2004hroot@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[35.238666, "o", "source env/bin/activate\r\n\u001b[?2004l\r"]
[35.239271, "o", "\u001b[?2004h(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[35.594131, "i", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\n"]
[35.594321, "o", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\r\n\u001b[?2004l\r"]
[35.870195, "o", "Collecting sqlparse==0.4.4\r\n"]
[35.904361, "o", "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\r\n"]
[35.904586, "o", "\u001b[?25l"]
[35.905516, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[35.913768, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[36.021289, "o", "Collecting psycopg2-binary==2.9.7\r\n"]
[36.02669, "o", "  Downloading psycopg2_binary-2.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n"]
[36.026872, "o", "\u001b[?25l"]
[36.027581, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[36.062983, "o", "\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m2.1/3.0 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"]
[36.082378, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[36.107735, "o", "Installing collected packages: sqlparse, psycopg2-binary\r\n"]
[36.174557, "o", "Successfully installed psycopg2-binary-2.9.7 sqlparse-0.4.4\r\n"]
[36.229303, "o", "\u001b[?2004h(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[112.091498, "i", "cat > analyze_and_optimize.py << 'EOF'\n#!/usr/bin/env python3\n\n\"\"\"\nSQL Query Log Analysis & Optimization Script\n\"\"\"\n\nimport sys\nimport os\nimport csv\nimport re\nimport hashlib\nimport json\nimport configparser\nfrom datetime import datetime\nfrom sqlparse import parse, format\nfrom sqlparse.sql import IdentifierList, Identifier, Where, Comparison\nfrom sqlparse.tokens import Keyword, DML\n\n# Configuration file path\nCONFIG_PATH = \"/home/user/db_tools/db_config.ini\"\n\n# Script argument validation\ndef validate_arguments():\n    if len(sys.argv) != 3:\n        print(\"Usage: {} --date YYYY-MM-DD\".format(sys.argv[0]))\n        sys.exit(1)\n    \n    if sys.argv[1].lower() != \"--date\":\n        print(\"Invalid argument. Use --date flag.\")\n        sys.exit(1)\n    \n    try:\n        datetime.strptime(sys.argv[2], \"%Y-%m-%d\")\n    except ValueError:\n        print(\"Invalid date format. Use YYYY-MM-DD.\")\n        sys.exit(1)\n\ndef read_configuration():\n    \"\"\"Read database configuration.\"\"\"\n    config = configparser.ConfigParser()\n    config"]
[112.091636, "i", ".read(CONFIG_PATH)\n    \n    return {\n        'log_file_path': config.get('settings', 'log_file_path'),\n        'min_duration_ms': int(config.get('settings', 'min_duration_ms'))\n    }\n\ndef parse_duration(duration_str):\n    \"\"\"Parse duration string (e.g., '15ms') to milliseconds.\"\"\"\n    match = re.match(r'^([0-9]+)ms$', duration_str)\n    if match:\n        return int(match.group(1))\n    return 0\n\ndef parse_log_line(line):\n    \"\"\"Parse pipe-delimited log line.\"\"\"\n    parts = [p.strip() for p in line.split('|')]\n    if len(parts) != 4:\n        return None\n    \n    timestamp = parts[0]\n    duration = parse_duration(parts[1])\n    level = parts[2]\n    message = parts[3]\n    \n    return {\n        'timestamp': timestamp,\n        'duration': duration,\n        'level': level,\n        'message': message\n    }\n\ndef normalize_sql(query):\n    \"\"\"Normalize SQL using sqlparse.\"\"\"\n    try:\n        parsed = parse(query)[0]\n        formatted = format(\n            parsed,\n            keyword_case='upper',\n            strip_comment"]
[112.09168, "o", "cat > analyze_and_optimize.py << 'EOF'\r\n\u001b[?2004l\r"]
[112.091734, "i", "s=True,\n            reindent=False\n        )\n        \n        # Remove trailing semicolon\n        sql_str = str(formatted).strip()\n        if sql_str.endswith(';'):\n            sql_str = sql_str[:-1]\n        \n        return sql_str\n    except Exception as e:\n        print(f\"Error normalizing SQL: {e}\")\n        return query\n\ndef extract_table_and_columns(parsed):\n    \"\"\"Extract table and columns from WHERE clause.\"\"\"\n    table = None\n    columns = []\n    \n    # Find FROM clause to identify table\n    from_seen = False\n    for token in parsed.tokens:\n        if isinstance(token, IdentifierList):\n            if from_seen:\n                table = ' '.join([str(t) for t in token.get_identifiers()])\n                break\n        elif token.value.upper() == 'FROM':\n            from_seen = True\n    \n    # Find WHERE clause\n    where = None\n    for token in parsed.tokens:\n        if isinstance(token, Where):\n            where = token\n            break\n    \n    if not where:\n        return table, columns\n    \n    # Pars"]
[112.091773, "o", "\u001b[?2004h> "]
[112.091809, "i", "e conditions in WHERE clause\n    for token in where.tokens:\n        if isinstance(token, Comparison):\n            # Simple equality: col = val\n            if token.tokens[1].value == '=' and isinstance(token.tokens[0], Identifier):\n                col_name = str(token.tokens[0])\n                columns.append(col_name)\n    \n    return table, columns\n\ndef main():\n    validate_arguments()\n    \n    config = read_configuration()\n    analysis_date = sys.argv[2]\n    \n    # Create staging directory\n    staging_dir = \"staging\"\n    if not os.path.exists(staging_dir):\n        os.makedirs(staging_dir)\n    \n    staging_file = os.path.join(staging_dir, \"raw_extract.csv\")\n    \n    # Phase 1: Filtering & Staging\n    raw_queries = []\n    \n    try:\n        with open(config['log_file_path'], 'r', encoding='utf-8') as log_file:\n            for line in log_file:\n                if not line.strip():\n                    continue\n                    \n                parsed_line = parse_log_line(line)\n                if not parsed_l"]
[112.091842, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r\u001b[?2004h> \r\n\u001b[?2004l\r"]
[112.091885, "i", "ine:\n                    continue\n                    \n                # Date match (first 10 characters of ISO timestamp)\n                if parsed_line['timestamp'][:10] != analysis_date:\n                    continue\n                    \n                # Type match\n                if parsed_line['level'] != '[QUERY]':\n                    continue\n                    \n                # Performance threshold\n                if parsed_line['duration'] < config['min_duration_ms']:\n                    continue\n                    \n                # Convert duration to integer for CSV\n                raw_queries.append({\n                    'timestamp': parsed_line['timestamp'],\n                    'duration_ms': str(parsed_line['duration']),\n                    'query_text': parsed_line['message']\n                })\n    except FileNotFoundError:\n        print(f\"Log file not found: {config['log_file_path']}\")\n        sys.exit(1)\n    \n    # Write to staging CSV with specific encoding and line endings\n    try:\n   "]
[112.091917, "o", "e conditions in WHERE clause\r\n    for token in where.tokens:\r\n        if isinstance(token, Comparison):\r\n            # Simple equality: col = val\r\n            if token.tokens[1].value =="]
[112.09196, "i", "     with open(staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\n            fieldnames = ['timestamp', 'duration_ms', 'query_text']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            for query in raw_queries:\n                writer.writerow(query)\n    except Exception as e:\n        print(f\"Error writing staging file: {e}\")\n        sys.exit(1)\n    \n    # Phase 2: Analysis & Normalization\n    query_patterns = {}\n    \n    try:\n        with open(staging_file, 'r', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                normalized_sql = normalize_sql(row['query_text'])\n                \n                if normalized_sql not in query_patterns:\n                    query_patterns[normalized_sql] = {\n                        'first_timestamp': row['timestamp'],\n                        'normalized_sql': normalized_sql,\n                        'index_sugg"]
[112.091994, "o", " '=' and isinstance(token.tokens[0], Identifier):\r\n                col_name = str(token.tokens[0])\r\n                columns.append(col_name)\r\n    \r\n    return table, columns\r\n\r\ndef main():\r\n    validate_arguments()\r\n    \r\n    config = read_configuration()\r\n    analysis_date = sys.argv[2]\r\n    \r\n    # Create s"]
[112.092041, "i", "estion': None\n                    }\n    except Exception as e:\n        print(f\"Error processing staging file: {e}\")\n        sys.exit(1)\n    \n    # Generate index suggestions\n    for pattern in query_patterns.values():\n        parsed = parse(pattern['normalized_sql'])[0]\n        table, columns = extract_table_and_columns(parsed)\n        \n        if table and columns:\n            # Format columns for index name and definition\n            index_name = f\"idx_{table.replace(' ', '_').replace('.', '_')}_{'_'.join(sorted(columns))}\"\n            index_def = f\"CREATE INDEX {index_name} ON {table} ({', '.join(columns)});\"\n            pattern['index_suggestion'] = index_def\n        else:\n            pattern['index_suggestion'] = None\n    \n    # Phase 3: Reporting\n    report = {\n        'meta': {\n            'date_analyzed': analysis_date,\n            'staging_file_sha256': None\n        },\n        'stats': {\n            'total_queries_extracted': len(raw_queries),\n            'unique_patterns': len(query_patterns)\n      "]
[112.09208, "o", "taging directory\r\n    staging_dir = \"staging\"\r\n    if not os.path.exists(staging_dir):\r\n        os.makedirs(staging_dir)\r\n    \r\n    staging_file = os.path.join(staging_dir, \"raw_extract.csv\")\r\n    \r\n    # Phase 1: Filtering & Staging\r\n    raw_queri"]
[112.092134, "i", "  },\n        'suggestions': []\n    }\n    \n    # Calculate SHA-256 checksum of staging file\n    sha256_hash = hashlib.sha256()\n    try:\n        with open(staging_file, 'rb') as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        report['meta']['staging_file_sha256'] = sha256_hash.hexdigest()\n    except Exception as e:\n        print(f\"Error calculating checksum: {e}\")\n        sys.exit(1)\n    \n    # Prepare suggestions in chronological order\n    for pattern in query_patterns.values():\n        if pattern['index_suggestion']:\n            report['suggestions'].append({\n                'id': pattern['first_timestamp'],\n                'normalized_sql': pattern['normalized_sql'],\n                'index_suggestion': pattern['index_suggestion']\n            })\n    \n    # Sort suggestions by timestamp\n    report['suggestions'].sort(key=lambda x: x['id'])\n    \n    # Write final report\n    report_file = \"report.json\"\n    try:\n        with open(report_file"]
[112.092168, "o", "es = []\r\n    \r\n    try:\r\n        with open(config['log_file_path'], 'r', encoding='utf-8') as log_file:\r\n            for line in log_file:\r\n                if not line.strip():\r\n                    continue\r\n                    \r\n                parsed_line = parse_log_line(line)\r\n                if not parsed_"]
[112.092205, "i", ", 'w', encoding='utf-8', newline='\\n') as f:\n            json.dump(report, f, indent=2)\n    except Exception as e:\n        print(f\"Error writing report: {e}\")\n        sys.exit(1)\n    \n    print(f\"Analysis complete. Report generated: {report_file}\")\n\nif __name__ == \"__main__\":\n    main()\nEOF\n"]
[112.092252, "o", "line:\r\n                    continue\r\n                    \r\n       \u001b[?2004h> \"\"\"\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.092416, "o", "SQL Query Log Analysis & Optimization Script\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.092472, "o", "> \"\"\"\r\n\u001b[?2004l\rim\u001b[?2004h> \r\n\u001b[?2004l\re\u001b[?2004h"]
[112.092527, "o", "> import sys\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.092589, "o", "> import os\r\n\u001b[?2004l\r"]
[112.092651, "o", "\u001b[?2004h> import csv\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.0927, "o", "> "]
[112.092757, "o", "import re\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.092808, "o", "import hashlib\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.092857, "o", "> import json\r\n\u001b[?2004l\r"]
[112.092907, "o", "\u001b[?2004h> "]
[112.092973, "o", "import configparser\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.09302, "o", "> "]
[112.093102, "o", "from datetime import datetime\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.09315, "o", "> "]
[112.093241, "o", "from sqlparse import parse, format\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.093288, "o", "> "]
[112.09349, "o", "from sqlparse.sql import IdentifierList, Identifier, Where, Comparison\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.093538, "o", "> "]
[112.093635, "o", "from sqlparse.tokens import Keyword, DML\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.093701, "o", "> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.093759, "o", "# Configuration file path\r\n\u001b[?2004l\r"]
[112.0938, "o", "\u001b[?2004h> "]
[112.093929, "o", "CONFIG_PATH = \"/home/user/db_tools/db_config.ini\"\r\n\u001b[?2004l\r"]
[112.093965, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.094067, "o", "# Script argument validation\r\n\u001b[?2004l\r"]
[112.094097, "o", "\u001b[?2004h> "]
[112.094168, "o", "def validate_arguments():\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.094207, "o", "> "]
[112.094281, "o", "    if len(sys.argv) != 3:\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.094323, "o", "> "]
[112.094509, "o", "        print(\"Usage: {} --date YYYY-MM-DD\".format(sys.argv[0]))\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.094545, "o", "> "]
[112.094587, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.094629, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[112.094673, "o", "\u001b[?2004h> "]
[112.094773, "o", "    if sys.argv[1].lower() != \"--date\":\r\n\u001b[?2004l\r"]
[112.094807, "o", "\u001b[?2004h> "]
[112.094953, "o", "        print(\"Invalid argument. Use --date flag.\")\r\n\u001b[?2004l\r"]
[112.094995, "o", "\u001b[?2004h> "]
[112.095037, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.095077, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r,"]
[112.095118, "o", "\u001b[?2004h> "]
[112.095163, "o", "    try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.095309, "o", "        datetime.strptime(sys.argv[2], \"%Y-%m-%d\")\r\n\u001b[?2004l\r"]
[112.095355, "o", "\u001b[?2004h> "]
[112.095409, "o", "    except ValueError:\r\n\u001b[?2004l\r"]
[112.095457, "o", "\u001b[?2004h> "]
[112.095587, "o", "        print(\"Invalid date format. Use YYYY-MM-DD.\")\r\n\u001b[?2004l\r"]
[112.095628, "o", "\u001b[?2004h> "]
[112.09567, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.095711, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.095753, "o", "> "]
[112.095811, "o", "def read_configuration():\r\n\u001b[?2004l\r"]
[112.095851, "o", "\u001b[?2004h> "]
[112.095957, "o", "    \"\"\"Read database configuration.\"\"\"\r\n\u001b[?2004l\r"]
[112.095995, "o", "\u001b[?2004h> "]
[112.096108, "o", "    config = configparser.ConfigParser()\r\n\u001b[?2004l\r"]
[112.096147, "o", "\u001b[?2004h> "]
[112.096223, "o", "    config.read(CONFIG_PATH)\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.096259, "o", ">     \r\n\u001b[?2004l\r"]
[112.096301, "o", "\u001b[?2004h> "]
[112.096362, "o", "    return {\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.096554, "o", "        'log_file_path': config.get('settings', 'log_file_path'),\r\n\u001b[?2004l\r"]
[112.096589, "o", "\u001b[?2004h> "]
[112.096792, "o", "        'min_duration_ms': int(config.get('settings', 'min_duration_ms'))\r\n\u001b[?2004l\r"]
[112.096831, "o", "\u001b[?2004h>     }\r\n\u001b[?2004l\r"]
[112.096873, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.096985, "o", "def parse_duration(duration_str):\r\n\u001b[?2004l\r"]
[112.097026, "o", "\u001b[?2004h> "]
[112.097201, "o", "    \"\"\"Parse duration string (e.g., '15ms') to milliseconds.\"\"\"\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.097245, "o", "> "]
[112.09738, "o", "    match = re.match(r'^([0-9]+)ms$', duration_str)\r\n\u001b[?2004l\r"]
[112.097429, "o", "\u001b[?2004h> "]
[112.097483, "o", "    if match:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.097574, "o", "        return int(match.group(1))\r\n\u001b[?2004l\r"]
[112.097615, "o", "\u001b[?2004h> "]
[112.097659, "o", "    return 0\r\n\u001b[?2004l\r\u001b[?2004h> \r\n\u001b[?2004l\r"]
[112.097702, "o", "\u001b[?2004h> "]
[112.097771, "o", "def parse_log_line(line):\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.097808, "o", "> "]
[112.09791, "o", "    \"\"\"Parse pipe-delimited log line.\"\"\"\r\n\u001b[?2004l\r"]
[112.097946, "o", "\u001b[?2004h> "]
[112.098078, "o", "    parts = [p.strip() for p in line.split('|')]\r\n\u001b[?2004l\r"]
[112.098115, "o", "\u001b[?2004h> "]
[112.098181, "o", "    if len(parts) != 4:\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.098219, "o", "> "]
[112.098261, "o", "        return None\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.098304, "o", ">     \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.098369, "o", "> "]
[112.09841, "o", "    timestamp = parts[0]\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.098452, "o", "> "]
[112.098556, "o", "    duration = parse_duration(parts[1])\r\n\u001b[?2004l\r"]
[112.098593, "o", "\u001b[?2004h> "]
[112.098636, "o", "    level = parts[2]\r\n\u001b[?2004l\r"]
[112.098681, "o", "\u001b[?2004h> "]
[112.098747, "o", "    message = parts[3]\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.098783, "o", ">     \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.098826, "o", "> "]
[112.098867, "o", "    return {\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.098964, "o", "        'timestamp': timestamp,\r\n\u001b[?2004l\r"]
[112.099, "o", "\u001b[?2004h> "]
[112.099077, "o", "        'duration': duration,\r\n\u001b[?2004l\r"]
[112.099114, "o", "\u001b[?2004h> "]
[112.099183, "o", "        'level': level,\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.099225, "o", "> "]
[112.099285, "o", "        'message': message\r\n\u001b[?2004l\r"]
[112.099324, "o", "\u001b[?2004h>     }\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.099376, "o", "> \r\n\u001b[?2004l\rt\u001b[?2004h> "]
[112.09946, "o", "def normalize_sql(query):\r\n\u001b[?2004l\r"]
[112.099497, "o", "\u001b[?2004h> "]
[112.099601, "o", "    \"\"\"Normalize SQL using sqlparse.\"\"\"\r\n\u001b[?2004l\r"]
[112.099637, "o", "\u001b[?2004h>     try:"]
[112.099679, "o", "\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.099777, "o", "        parsed = parse(query)[0]\r\n\u001b[?2004l\r"]
[112.099813, "o", "\u001b[?2004h> "]
[112.099886, "o", "        formatted = format(\r\n\u001b[?2004l\r"]
[112.09993, "o", "\u001b[?2004h> "]
[112.099972, "o", "            parsed,\r\n\u001b[?2004l\r"]
[112.100015, "o", "\u001b[?2004h> "]
[112.1001, "o", "            keyword_case='upper',\r\n\u001b[?2004l\r"]
[112.10014, "o", "\u001b[?2004h> "]
[112.100225, "o", "            strip_comments=True,\r\n\u001b[?2004l\r"]
[112.100254, "o", "\u001b[?2004h> "]
[112.100344, "o", "            reindent=False\r\n\u001b[?2004l\r"]
[112.100382, "o", "\u001b[?2004h> "]
[112.100425, "o", "        )\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.100467, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.100509, "o", "> "]
[112.100603, "o", "        # Remove trailing semicolon\r\n\u001b[?2004l\r"]
[112.100642, "o", "\u001b[?2004h> "]
[112.100738, "o", "        sql_str = str(formatted).strip()\r\n\u001b[?2004l\r"]
[112.100776, "o", "\u001b[?2004h> "]
[112.100866, "o", "        if sql_str.endswith(';'):\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.100976, "o", "            sql_str = sql_str[:-1]\r\n\u001b[?2004l\r"]
[112.101019, "o", "\u001b[?2004h>         \r\n\u001b[?2004l\r"]
[112.101061, "o", "\u001b[?2004h> "]
[112.101119, "o", "        return sql_str\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.101159, "o", "> "]
[112.101218, "o", "    except Exception as e:\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.101258, "o", "> "]
[112.101368, "o", "        print(f\"Error normalizing SQL: {e}\")\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.101405, "o", "> "]
[112.101461, "o", "        return query\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.101509, "o", "> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.101621, "o", "def extract_table_and_columns(parsed):\r\n\u001b[?2004l\r"]
[112.101655, "o", "\u001b[?2004h> "]
[112.101801, "o", "    \"\"\"Extract table and columns from WHERE clause.\"\"\"\r\n\u001b[?2004l\r"]
[112.101839, "o", "\u001b[?2004h> "]
[112.10188, "o", "    table = None\r\n\u001b[?2004l\r"]
[112.101922, "o", "\u001b[?2004h> "]
[112.101963, "o", "    columns = []\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.102006, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.102127, "o", "    # Find FROM clause to identify table\r\n\u001b[?2004l\r"]
[112.102163, "o", "\u001b[?2004h> "]
[112.102208, "o", "    from_seen = False\r\n\u001b[?2004l\r"]
[112.102251, "o", "\u001b[?2004h> "]
[112.102344, "o", "    for token in parsed.tokens:\r\n\u001b[?2004l\r"]
[112.10238, "o", "\u001b[?2004h> "]
[112.102506, "o", "        if isinstance(token, IdentifierList):\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.102542, "o", "> "]
[112.102605, "o", "            if from_seen:\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.102646, "o", "> "]
[112.102839, "o", "                table = ' '.join([str(t) for t in token.get_identifiers()])\r\n\u001b[?2004l\r"]
[112.102876, "o", "\u001b[?2004h> "]
[112.102935, "o", "                break\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.102977, "o", "> "]
[112.103083, "o", "        elif token.value.upper() == 'FROM':\r\n\u001b[?2004l\r"]
[112.103122, "o", "\u001b[?2004h> "]
[112.103196, "o", "            from_seen = True\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.103234, "o", ">     \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.103276, "o", "> "]
[112.103317, "o", "    # Find WHERE clause\r\n\u001b[?2004l\r"]
[112.103378, "o", "\u001b[?2004h> "]
[112.103423, "o", "    where = None\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.103526, "o", "    for token in parsed.tokens:\r\n\u001b[?2004l\r"]
[112.103561, "o", "\u001b[?2004h> "]
[112.103684, "o", "        if isinstance(token, Where):\r\n\u001b[?2004l\r"]
[112.103754, "o", "\u001b[?2004h> "]
[112.1038, "o", "            where = token\r\n\u001b[?2004l\r"]
[112.103841, "o", "\u001b[?2004h> "]
[112.103883, "o", "            break\r\n\u001b[?2004l\r"]
[112.103934, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[112.103958, "o", "\u001b[?2004h> "]
[112.104046, "o", "    if not where:\r\n\u001b[?2004l\r"]
[112.104087, "o", "\u001b[?2004h> "]
[112.104129, "o", "        return table, columns\r\n"]
[112.104167, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[112.104204, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.104348, "o", "    # Parse conditions in WHERE clause\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.10439, "o", "> "]
[112.104467, "o", "    for token in where.tokens:\r\n\u001b[?2004l\r"]
[112.10451, "o", "\u001b[?2004h> "]
[112.104616, "o", "        if isinstance(token, Comparison):\r\n\u001b[?2004l\r"]
[112.104659, "o", "\u001b[?2004h> "]
[112.104762, "o", "            # Simple equality: col = val\r\n\u001b[?2004l\r"]
[112.104804, "o", "\u001b[?2004h> "]
[112.105052, "o", "            if token.tokens[1].value == '=' and isinstance(token.tokens[0], Identifier):\r\n\u001b[?2004l\r"]
[112.105139, "o", "\u001b[?2004h> "]
[112.105236, "o", "                col_name = str(token.tokens[0])\r\n\u001b[?2004l\r"]
[112.105283, "o", "\u001b[?2004h> "]
[112.10539, "o", "                columns.append(col_name)\r\n\u001b[?2004l\r"]
[112.105434, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.105481, "o", "> "]
[112.105536, "o", "    return table, columns\r\n\u001b[?2004l\r"]
[112.10557, "o", "\u001b[?2004h> \r\n\u001b[?2004l\rx\u001b[?2004h"]
[112.10558, "o", "> "]
[112.105628, "o", "def main():\r\n\u001b[?2004l\r"]
[112.105659, "o", "\u001b[?2004h"]
[112.105692, "o", "> "]
[112.105759, "o", "    validate_arguments()\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.105812, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.105915, "o", "    config = read_configuration()\r\n\u001b[?2004l\r"]
[112.105958, "o", "\u001b[?2004h> "]
[112.106029, "o", "    analysis_date = sys.argv[2]\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.106083, "o", ">     \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.106181, "o", "    # Create staging directory\r\n\u001b[?2004l\r"]
[112.106223, "o", "\u001b[?2004h> "]
[112.106295, "o", "    staging_dir = \"staging\"\r\n\u001b[?2004l\r"]
[112.106359, "o", "\u001b[?2004h> "]
[112.106443, "o", "    if not os.path.exists(staging_dir):\r\n\u001b[?2004l\r"]
[112.106483, "o", "\u001b[?2004h> "]
[112.106571, "o", "        os.makedirs(staging_dir)\r\n\u001b[?2004l\r"]
[112.10661, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[112.106653, "o", "\u001b[?2004h> "]
[112.106874, "o", "    staging_file = os.path.join(staging_dir, \"raw_extract.csv\")\r\n\u001b[?2004l\r"]
[112.106911, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[112.106956, "o", "\u001b[?2004h> "]
[112.107055, "o", "    # Phase 1: Filtering & Staging\r\n\u001b[?2004l\r"]
[112.107104, "o", "\u001b[?2004h> "]
[112.10716, "o", "    raw_queries = []\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.107233, "o", ">     \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.107284, "o", "    try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.107571, "o", "        with open(config['log_file_path'], 'r', encoding='utf-8') as log_file:\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.107612, "o", "> "]
[112.107719, "o", "            for line in log_file:\r\n\u001b[?2004l\r"]
[112.107758, "o", "\u001b[?2004h> "]
[112.10788, "o", "                if not line.strip():\r\n\u001b[?2004l\r"]
[112.107917, "o", "\u001b[?2004h> "]
[112.108011, "o", "                    continue\r\n\u001b[?2004l\r"]
[112.108054, "o", "\u001b[?2004h> "]
[112.10811, "o", "                    \r\n\u001b[?2004l\r"]
[112.10816, "o", "\u001b[?2004h> "]
[112.108315, "o", "                parsed_line = parse_log_line(line)\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.108388, "o", "> "]
[112.108471, "o", "                if not parsed_line:\r\n\u001b[?2004l\r"]
[112.108521, "o", "\u001b[?2004h> "]
[112.108618, "o", "                    continue\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.108665, "o", "> "]
[112.108704, "o", "                    \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.10874, "o", "> "]
[112.108928, "o", "                # Date match (first 10 characters of ISO timestamp)\r\n\u001b[?2004l\r"]
[112.10896, "o", "\u001b[?2004h> "]
[112.109185, "o", "                if parsed_line['timestamp'][:10] != analysis_date:\r\n\u001b[?2004l\r"]
[112.109213, "o", "\u001b[?2004h> "]
[112.109343, "o", "                    continue\r\n\u001b[?2004l\r"]
[112.109375, "o", "\u001b[?2004h> "]
[112.109442, "o", "                    \r\n\u001b[?2004l\r"]
[112.109468, "o", "\u001b[?2004h> "]
[112.109559, "o", "                # Type match\r\n\u001b[?2004l\r"]
[112.109591, "o", "\u001b[?2004h> "]
[112.109739, "o", "                if parsed_line['level'] != '[QUERY]':\r\n\u001b[?2004l\r"]
[112.10977, "o", "\u001b[?2004h> "]
[112.109833, "o", "                    continue\r\n\u001b[?2004l\r"]
[112.109863, "o", "\u001b[?2004h> "]
[112.109907, "o", "                    \r\n\u001b[?2004l\r"]
[112.109936, "o", "\u001b[?2004h> "]
[112.110025, "o", "                # Performance threshold\r\n\u001b[?2004l\r"]
[112.110055, "o", "\u001b[?2004h> "]
[112.110218, "o", "                if parsed_line['duration'] < config['min_duration_ms']:\r\n\u001b[?2004l\r"]
[112.110248, "o", "\u001b[?2004h> "]
[112.11031, "o", "                    continue\r\n\u001b[?2004l\r"]
[112.110349, "o", "\u001b[?2004h> "]
[112.110389, "o", "                    \r\n\u001b[?2004l\r"]
[112.110419, "o", "\u001b[?2004h> "]
[112.110542, "o", "                # Convert duration to integer for CSV\r\n\u001b[?2004l\r"]
[112.110572, "o", "\u001b[?2004h> "]
[112.110653, "o", "                raw_queries.append({\r\n\u001b[?2004l\r"]
[112.110683, "o", "\u001b[?2004h> "]
[112.110821, "o", "                    'timestamp': parsed_line['timestamp'],\r\n\u001b[?2004l\r"]
[112.110874, "o", "\u001b[?2004h> "]
[112.111015, "o", "                    'duration_ms': str(parsed_line['duration']),\r\n\u001b[?2004l\r"]
[112.111049, "o", "\u001b[?2004h> "]
[112.111189, "o", "                    'query_text': parsed_line['message']\r\n\u001b[?2004l\r"]
[112.11122, "o", "\u001b[?2004h> "]
[112.111281, "o", "                })\r\n\u001b[?2004l\r"]
[112.111343, "o", "\u001b[?2004h> "]
[112.111396, "o", "    except FileNotFoundError:\r\n\u001b[?2004l\r"]
[112.111427, "o", "\u001b[?2004h"]
[112.111458, "o", "> "]
[112.111582, "o", "        print(f\"Log file not found: {config['log_file_path']}\")\r\n\u001b[?2004l\r"]
[112.111614, "o", "\u001b[?2004h> "]
[112.111663, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.111696, "o", "\u001b[?2004h> "]
[112.111717, "o", "    \r\n\u001b[?2004l\r"]
[112.111745, "o", "\u001b[?2004h> "]
[112.1119, "o", "    # Write to staging CSV with specific encoding and line endings\r\n\u001b[?2004l\r"]
[112.111932, "o", "\u001b[?2004h> "]
[112.111962, "o", "    try:\r\n\u001b[?2004l\r"]
[112.11199, "o", "\u001b[?2004h"]
[112.112021, "o", "> "]
[112.112201, "o", "        with open(staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\r\n\u001b[?2004l\r"]
[112.112234, "o", "\u001b[?2004h> "]
[112.112406, "o", "            fieldnames = ['timestamp', 'duration_ms', 'query_text']\r\n\u001b[?2004l\r"]
[112.112438, "o", "\u001b[?2004h> "]
[112.112602, "o", "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\r\n\u001b[?2004l\r"]
[112.112631, "o", "\u001b[?2004h"]
[112.112661, "o", "> "]
[112.112717, "o", "            writer.writeheader()\r\n\u001b[?2004l\r"]
[112.112749, "o", "\u001b[?2004h> "]
[112.112841, "o", "            for query in raw_queries:\r\n\u001b[?2004l\r"]
[112.112869, "o", "\u001b[?2004h> "]
[112.11297, "o", "                writer.writerow(query)\r\n\u001b[?2004l\r"]
[112.113, "o", "\u001b[?2004h"]
[112.113023, "o", "> "]
[112.113071, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[112.113104, "o", "\u001b[?2004h> "]
[112.113225, "o", "        print(f\"Error writing staging file: {e}\")\r\n\u001b[?2004l\r"]
[112.113235, "o", "\u001b[?2004h"]
[112.113271, "o", "> "]
[112.113307, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.113353, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[112.113387, "o", "\u001b[?2004h> "]
[112.113481, "o", "    # Phase 2: Analysis & Normalization\r\n\u001b[?2004l\r"]
[112.113514, "o", "\u001b[?2004h> "]
[112.113573, "o", "    query_patterns = {}\r\n\u001b[?2004l\r"]
[112.113606, "o", "\u001b[?2004h> "]
[112.113639, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.113671, "o", "    try:\r\n\u001b[?2004l\r"]
[112.1137, "o", "\u001b[?2004h"]
[112.11373, "o", "> "]
[112.113915, "o", "        with open(staging_file, 'r', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\r\n\u001b[?2004l\r"]
[112.113953, "o", "\u001b[?2004h> "]
[112.114057, "o", "            reader = csv.DictReader(csvfile)\r\n\u001b[?2004l\r"]
[112.114089, "o", "\u001b[?2004h> "]
[112.114166, "o", "            for row in reader:\r\n\u001b[?2004l\r"]
[112.114199, "o", "\u001b[?2004h> "]
[112.114364, "o", "                normalized_sql = normalize_sql(row['query_text'])\r\n\u001b[?2004l\r"]
[112.114398, "o", "\u001b[?2004h> "]
[112.114439, "o", "                \r\n\u001b[?2004l\r"]
[112.114472, "o", "\u001b[?2004h> "]
[112.114608, "o", "                if normalized_sql not in query_patterns:\r\n\u001b[?2004l\r"]
[112.114641, "o", "\u001b[?2004h> "]
[112.11477, "o", "                    query_patterns[normalized_sql] = {\r\n\u001b[?2004l\r"]
[112.114803, "o", "\u001b[?2004h> "]
[112.114951, "o", "                        'first_timestamp': row['timestamp'],\r\n\u001b[?2004l\r"]
[112.114983, "o", "\u001b[?2004h> "]
[112.115121, "o", "                        'normalized_sql': normalized_sql,\r\n\u001b[?2004l\r"]
[112.115154, "o", "\u001b[?2004h> "]
[112.115271, "o", "                        'index_suggestion': None\r\n\u001b[?2004l\r"]
[112.115304, "o", "\u001b[?2004h> "]
[112.115376, "o", "                    }\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.115386, "o", "> "]
[112.115466, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[112.115498, "o", "\u001b[?2004h> "]
[112.115624, "o", "        print(f\"Error processing staging file: {e}\")\r\n\u001b[?2004l\r"]
[112.115657, "o", "\u001b[?2004h> "]
[112.115706, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.115739, "o", "\u001b[?2004h> "]
[112.115769, "o", "    \r\n\u001b[?2004l\r"]
[112.115798, "o", "\u001b[?2004h> "]
[112.115864, "o", "    # Generate index suggestions\r\n\u001b[?2004l\r"]
[112.115896, "o", "\u001b[?2004h> "]
[112.116007, "o", "    for pattern in query_patterns.values():\r\n\u001b[?2004l\r"]
[112.11604, "o", "\u001b[?2004h> "]
[112.116166, "o", "        parsed = parse(pattern['normalized_sql'])[0]\r\n\u001b[?2004l\r"]
[112.116199, "o", "\u001b[?2004h> "]
[112.116349, "o", "        table, columns = extract_table_and_columns(parsed)\r\n\u001b[?2004l\r"]
[112.116379, "o", "\u001b[?2004h"]
[112.11641, "o", "> "]
[112.116432, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.116503, "o", "        if table and columns:\r\n\u001b[?2004l\r"]
[112.116536, "o", "\u001b[?2004h> "]
[112.116675, "o", "            # Format columns for index name and definition\r\n\u001b[?2004l\r"]
[112.116708, "o", "\u001b[?2004h> "]
[112.116991, "o", "            index_name = f\"idx_{table.replace(' ', '_').replace('.', '_')}_{'_'.join(sorted(columns))}\"\r\n\u001b[?2004l\r\u001b[?2004h"]
[112.117081, "o", "> "]
[112.11723, "o", "            index_def = f\"CREATE INDEX {index_name} ON {table} ({', '.join(columns)});\"\r\n\u001b[?2004l\r"]
[112.117305, "o", "\u001b[?2004h> "]
[112.117622, "o", "            pattern['index_suggestion'] = index_def\r\n\u001b[?2004l\r\u001b[?2004h>         else:\r\n\u001b[?2004l\r\u001b[?2004h>             pattern['index_suggestion'] = None\r\n\u001b[?2004l\r\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[112.11764, "o", "\u001b[?2004h> "]
[112.117712, "o", "    # Phase 3: Reporting\r\n\u001b[?2004l\r"]
[112.117735, "o", "\u001b[?2004h> "]
[112.11777, "o", "    report = {\r\n\u001b[?2004l\r"]
[112.117788, "o", "\u001b[?2004h> "]
[112.117841, "o", "        'meta': {\r\n\u001b[?2004l\r"]
[112.117868, "o", "\u001b[?2004h> "]
[112.117974, "o", "            'date_analyzed': analysis_date,\r\n\u001b[?2004l\r"]
[112.117996, "o", "\u001b[?2004h> "]
[112.118102, "o", "            'staging_file_sha256': None\r\n\u001b[?2004l\r"]
[112.118127, "o", "\u001b[?2004h> "]
[112.118159, "o", "        },\r\n\u001b[?2004l\r"]
[112.118174, "o", "\u001b[?2004h"]
[112.118191, "o", "> "]
[112.11824, "o", "        'stats': {\r\n\u001b[?2004l\r"]
[112.118256, "o", "\u001b[?2004h"]
[112.118272, "o", "> "]
[112.118417, "o", "            'total_queries_extracted': len(raw_queries),\r\n\u001b[?2004l\r"]
[112.118438, "o", "\u001b[?2004h"]
[112.118456, "o", "> "]
[112.118573, "o", "            'unique_patterns': len(query_patterns)\r\n\u001b[?2004l\r"]
[112.11859, "o", "\u001b[?2004h> "]
[112.118629, "o", "        },\r\n\u001b[?2004l\r"]
[112.118644, "o", "\u001b[?2004h"]
[112.118667, "o", "> "]
[112.118726, "o", "        'suggestions': []\r\n\u001b[?2004l\r"]
[112.118742, "o", "\u001b[?2004h"]
[112.118758, "o", "> "]
[112.118774, "o", "    }\r\n\u001b[?2004l\r"]
[112.118789, "o", "\u001b[?2004h"]
[112.118807, "o", ">     \r\n"]
[112.118824, "o", "\u001b[?2004l\r"]
[112.11884, "o", "\u001b[?2004h> "]
[112.118965, "o", "    # Calculate SHA-256 checksum of staging file\r\n\u001b[?2004l\r"]
[112.118981, "o", "\u001b[?2004h"]
[112.118998, "o", "> "]
[112.119081, "o", "    sha256_hash = hashlib.sha256()\r\n\u001b[?2004l\r"]
[112.119096, "o", "\u001b[?2004h"]
[112.119113, "o", "> "]
[112.11914, "o", "    try:\r\n\u001b[?2004l\r"]
[112.119159, "o", "\u001b[?2004h> "]
[112.119272, "o", "        with open(staging_file, 'rb') as f:\r\n\u001b[?2004l\r"]
[112.119287, "o", "\u001b[?2004h"]
[112.119306, "o", "> "]
[112.119466, "o", "            for byte_block in iter(lambda: f.read(4096), b\"\"):\r\n\u001b[?2004l\r"]
[112.119491, "o", "\u001b[?2004h> "]
[112.119606, "o", "                sha256_hash.update(byte_block)\r\n\u001b[?2004l\r"]
[112.119622, "o", "\u001b[?2004h"]
[112.119638, "o", "> "]
[112.11981, "o", "        report['meta']['staging_file_sha256'] = sha256_hash.hexdigest()\r\n\u001b[?2004l\r"]
[112.119829, "o", "\u001b[?2004h"]
[112.119847, "o", "> "]
[112.119905, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[112.119923, "o", "\u001b[?2004h"]
[112.119939, "o", "> "]
[112.12006, "o", "        print(f\"Error calculating checksum: {e}\")\r\n\u001b[?2004l\r"]
[112.120081, "o", "\u001b[?2004h> "]
[112.120138, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.120153, "o", "\u001b[?2004h"]
[112.12017, "o", "> "]
[112.120189, "o", "    \r\n\u001b[?2004l\r"]
[112.120205, "o", "\u001b[?2004h> "]
[112.120332, "o", "    # Prepare suggestions in chronological order\r\n\u001b[?2004l\r"]
[112.120356, "o", "\u001b[?2004h"]
[112.12038, "o", "> "]
[112.120486, "o", "    for pattern in query_patterns.values():\r\n\u001b[?2004l\r"]
[112.120506, "o", "\u001b[?2004h> "]
[112.120612, "o", "        if pattern['index_suggestion']:\r\n\u001b[?2004l\r"]
[112.120633, "o", "\u001b[?2004h> "]
[112.120748, "o", "            report['suggestions'].append({\r\n\u001b[?2004l\r"]
[112.120768, "o", "\u001b[?2004h"]
[112.120793, "o", "> "]
[112.120916, "o", "                'id': pattern['first_timestamp'],\r\n\u001b[?2004l\r"]
[112.120936, "o", "\u001b[?2004h"]
[112.120955, "o", "> "]
[112.121103, "o", "                'normalized_sql': pattern['normalized_sql'],\r\n\u001b[?2004l\r"]
[112.121119, "o", "\u001b[?2004h> "]
[112.121281, "o", "                'index_suggestion': pattern['index_suggestion']\r\n\u001b[?2004l\r"]
[112.1213, "o", "\u001b[?2004h"]
[112.121336, "o", "> "]
[112.121387, "o", "            })\r\n\u001b[?2004l\r"]
[112.121407, "o", "\u001b[?2004h> "]
[112.121425, "o", "    \r\n\u001b[?2004l\r"]
[112.121446, "o", "\u001b[?2004h"]
[112.121469, "o", "> "]
[112.121552, "o", "    # Sort suggestions by timestamp\r\n\u001b[?2004l\r"]
[112.121569, "o", "\u001b[?2004h"]
[112.121588, "o", "> "]
[112.121716, "o", "    report['suggestions'].sort(key=lambda x: x['id'])\r\n\u001b[?2004l\r"]
[112.121733, "o", "\u001b[?2004h"]
[112.121753, "o", ">     \r\n"]
[112.121771, "o", "\u001b[?2004l\r\u001b[?2004h"]
[112.121789, "o", "> "]
[112.121856, "o", "    # Write final report\r\n\u001b[?2004l\r"]
[112.121873, "o", "\u001b[?2004h> "]
[112.121958, "o", "    report_file = \"report.json\"\r\n\u001b[?2004l\r"]
[112.121976, "o", "\u001b[?2004h"]
[112.121994, "o", "> "]
[112.122015, "o", "    try:\r\n\u001b[?2004l\r"]
[112.122033, "o", "\u001b[?2004h"]
[112.122055, "o", "> "]
[112.122228, "o", "        with open(report_file, 'w', encoding='utf-8', newline='\\n') as f:\r\n\u001b[?2004l\r"]
[112.122243, "o", "\u001b[?2004h> "]
[112.122367, "o", "            json.dump(report, f, indent=2)\r\n\u001b[?2004l\r"]
[112.122384, "o", "\u001b[?2004h"]
[112.122404, "o", "> "]
[112.122469, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[112.122487, "o", "\u001b[?2004h"]
[112.122509, "o", "> "]
[112.122612, "o", "        print(f\"Error writing report: {e}\")\r\n\u001b[?2004l\r"]
[112.122629, "o", "\u001b[?2004h"]
[112.122648, "o", "> "]
[112.122698, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[112.122716, "o", "\u001b[?2004h"]
[112.122736, "o", "> "]
[112.122755, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[112.122773, "o", "> "]
[112.122943, "o", "    print(f\"Analysis complete. Report generated: {report_file}\")\r\n\u001b[?2004l\r"]
[112.122961, "o", "\u001b[?2004h> "]
[112.122988, "o", "\r\n\u001b[?2004l\r\u001b[?2004h> "]
[112.123066, "o", "if __name__ == \"__main__\":\r\n\u001b[?2004l\r"]
[112.123089, "o", "\u001b[?2004h"]
[112.123107, "o", "> "]
[112.123133, "o", "    main()\r\n\u001b[?2004l\r"]
[112.123151, "o", "\u001b[?2004h> "]
[112.12317, "o", "EOF\r\n\u001b[?2004l\r"]
[112.12519, "o", "\u001b[?2004h"]
[112.125226, "o", "(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[112.569237, "i", "chmod +x analyze_and_optimize.py\n"]
[112.5695, "o", "chmod +x analyze_and_optimize.py\r\n\u001b[?2004l\r"]
[112.570644, "o", "\u001b[?2004h"]
[112.570697, "o", "(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[125.508536, "i", "./analyze_and_optimize.py --date 2023-03-15\n"]
[125.508828, "o", "./analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[125.545996, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[125.546345, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[125.546668, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[125.54772, "o", "Analysis complete. Report generated: report.json\r\n"]
[125.551107, "o", "\u001b[?2004h"]
[125.551187, "o", "(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[130.435313, "i", "cat report.json\n"]
[130.435512, "o", "cat report.json\r\n\u001b[?2004l\r"]
[130.436877, "o", "{\r\n  \"meta\": {\r\n    \"date_analyzed\": \"2023-03-15\",\r\n    \"staging_file_sha256\": \"93ea43b0551e16504ee8491e23427d610811a86e898c04d51d7e1fed60074ffb\"\r\n  },\r\n  \"stats\": {\r\n    \"total_queries_extracted\": 3,\r\n    \"unique_patterns\": 3\r\n  },\r\n  \"suggestions\": []\r\n}"]
[130.437206, "o", "\u001b[?2004h"]
[130.437266, "o", "(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[204.7465, "i", "pip uninstall sqlparse -y\n"]
[204.746756, "o", "pip uninstall sqlparse -y\r\n\u001b[?2004l\r"]
[204.964324, "o", "Found existing installation: sqlparse 0.4.4\r\n"]
[204.974872, "o", "Uninstalling sqlparse-0.4.4:\r\n"]
[204.97601, "o", "  Successfully uninstalled sqlparse-0.4.4\r\n"]
[205.01352, "o", "\u001b[?2004h(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[205.533348, "i", "pip install sqlparse==0.4.3\n"]
[205.533475, "o", "pip install sqlparse==0.4.3\r\n\u001b[?2004l\r"]
[205.801478, "o", "Collecting sqlparse==0.4.3\r\n"]
[205.83719, "o", "  Downloading sqlparse-0.4.3-py3-none-any.whl (42 kB)\r\n"]
[205.837452, "o", "\u001b[?25l"]
[205.838389, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[205.847205, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[205.86145, "o", "Installing collected packages: sqlparse\r\n"]
[205.884583, "o", "Successfully installed sqlparse-0.4.3\r\n"]
[205.920145, "o", "\u001b[?2004h(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[207.570832, "i", "cat > analyze_and_optimize.py << 'EOF'\n#!/usr/bin/env python3\n\n\"\"\"\nSQL Query Log Analysis & Optimization Script\n\"\"\"\n\nimport sys\nimport os\nimport csv\nimport re\nimport hashlib\nimport json\nimport configparser\nfrom datetime import datetime\nfrom sqlparse import parse, format\nfrom sqlparse.sql import IdentifierList, Identifier, Where, Comparison\nfrom sqlparse.tokens import Keyword, DML\n\n# Configuration file path\nCONFIG_PATH = \"/home/user/db_tools/db_config.ini\"\n\n# Script argument validation\ndef validate_arguments():\n    if len(sys.argv) != 3:\n        print(\"Usage: {} --date YYYY-MM-DD\".format(sys.argv[0]))\n        sys.exit(1)\n    \n    if sys.argv[1].lower() != \"--date\":\n        print(\"Invalid argument. Use --date flag.\")\n        sys.exit(1)\n    \n    try:\n        datetime.strptime(sys.argv[2], \"%Y-%m-%d\")\n    except ValueError:\n        print(\"Invalid date format. Use YYYY-MM-DD.\")\n        sys.exit(1)\n\ndef read_configuration():\n    \"\"\"Read database configuration.\"\"\"\n    config = configparser.ConfigParser()\n    config"]
[207.570945, "i", ".read(CONFIG_PATH)\n    \n    return {\n        'log_file_path': config.get('settings', 'log_file_path'),\n        'min_duration_ms': int(config.get('settings', 'min_duration_ms'))\n    }\n\ndef parse_duration(duration_str):\n    \"\"\"Parse duration string (e.g., '15ms') to milliseconds.\"\"\"\n    match = re.match(r'^([0-9]+)ms$', duration_str)\n    if match:\n        return int(match.group(1))\n    return 0\n\ndef parse_log_line(line):\n    \"\"\"Parse pipe-delimited log line.\"\"\"\n    parts = [p.strip() for p in line.split('|')]\n    if len(parts) != 4:\n        return None\n    \n    timestamp = parts[0]\n    duration = parse_duration(parts[1])\n    level = parts[2]\n    message = parts[3]\n    \n    return {\n        'timestamp': timestamp,\n        'duration': duration,\n        'level': level,\n        'message': message\n    }\n\ndef normalize_sql(query):\n    \"\"\"Normalize SQL using sqlparse.\"\"\"\n    try:\n        parsed = parse(query)\n        # Use the first statement from the parsed result\n        if parsed:\n            stmt = parsed[0]\n     "]
[207.570973, "i", "       formatted = format(\n                stmt,\n                keyword_case='upper',\n                strip_comments=True,\n                reindent=False,\n                truncate_strings=1000\n            )\n            \n            # Remove trailing semicolon\n            sql_str = str(formatted).strip()\n            if sql_str.endswith(';'):\n                sql_str = sql_str[:-1]\n            \n            return sql_str\n    except Exception as e:\n        print(f\"Error normalizing SQL: {e}\")\n        return query\n\ndef extract_table_and_columns(parsed):\n    \"\"\"Extract table and columns from WHERE clause.\"\"\"\n    table = None\n    columns = []\n    \n    # Find FROM clause to identify table\n    from_seen = False\n    for token in parsed.tokens:\n        if isinstance(token, IdentifierList):\n            if from_seen:\n                table = ' '.join([str(t) for t in token.get_identifiers()])\n                break\n        elif token.value.upper() == 'FROM':\n            from_seen = True\n    \n    # Find WHERE clause\n    whe"]
[207.570997, "i", "re = None\n    for token in parsed.tokens:\n        if isinstance(token, Where):\n            where = token\n            break\n    \n    if not where:\n        return table, columns\n    \n    # Parse conditions in WHERE clause\n    for token in where.tokens:\n        if isinstance(token, Comparison):\n            # Simple equality: col = val\n            if token.tokens[1].value == '=' and isinstance(token.tokens[0], Identifier):\n                col_name = str(token.tokens[0])\n                columns.append(col_name)\n    \n    return table, columns\n\ndef main():\n    validate_arguments()\n    \n    config = read_configuration()\n    analysis_date = sys.argv[2]\n    \n    # Create staging directory\n    staging_dir = \"staging\"\n    if not os.path.exists(staging_dir):\n        os.makedirs(staging_dir)\n    \n    staging_file = os.path.join(staging_dir, \"raw_extract.csv\")\n    \n    # Phase 1: Filtering & Staging\n    raw_queries = []\n    \n    try:\n        with open(config['log_file_path'], 'r', encoding='utf-8') as log_file:\n            "]
[207.571018, "i", "for line in log_file:\n                if not line.strip():\n                    continue\n                    \n                parsed_line = parse_log_line(line)\n                if not parsed_line:\n                    continue\n                    \n                # Date match (first 10 characters of ISO timestamp)\n                if parsed_line['timestamp'][:10] != analysis_date:\n                    continue\n                    \n                # Type match\n                if parsed_line['level'] != '[QUERY]':\n                    continue\n                    \n                # Performance threshold\n                if parsed_line['duration'] < config['min_duration_ms']:\n                    continue\n                    \n                # Convert duration to integer for CSV\n                raw_queries.append({\n                    'timestamp': parsed_line['timestamp'],\n                    'duration_ms': str(parsed_line['duration']),\n                    'query_text': parsed_line['message']\n                })\n    exc"]
[207.571039, "i", "ept FileNotFoundError:\n        print(f\"Log file not found: {config['log_file_path']}\")\n        sys.exit(1)\n    \n    # Write to staging CSV with specific encoding and line endings\n    try:\n        with open(staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\n            fieldnames = ['timestamp', 'duration_ms', 'query_text']\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writeheader()\n            for query in raw_queries:\n                writer.writerow(query)\n    except Exception as e:\n        print(f\"Error writing staging file: {e}\")\n        sys.exit(1)\n    \n    # Phase 2: Analysis & Normalization\n    query_patterns = {}\n    \n    try:\n        with open(staging_file, 'r', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                normalized_sql = normalize_sql(row['query_text'])\n                \n                if normalized_sql not in query_patterns:\n                  "]
[207.571061, "i", "  query_patterns[normalized_sql] = {\n                        'first_timestamp': row['timestamp'],\n                        'normalized_sql': normalized_sql,\n                        'index_suggestion': None\n                    }\n    except Exception as e:\n        print(f\"Error processing staging file: {e}\")\n        sys.exit(1)\n    \n    # Generate index suggestions\n    for pattern in query_patterns.values():\n        parsed = parse(pattern['normalized_sql'])\n        if parsed:\n            table, columns = extract_table_and_columns(parsed[0])\n            \n            if table and columns:\n                # Format columns for index name and definition\n                index_name = f\"idx_{table.replace(' ', '_').replace('.', '_')}_{'_'.join(sorted(columns))}\"\n                index_def = f\"CREATE INDEX {index_name} ON {table} ({', '.join(columns)});\"\n                pattern['index_suggestion'] = index_def\n        else:\n            pattern['index_suggestion'] = None\n    \n    # Phase 3: Reporting\n    report = {\n        "]
[207.571083, "i", "'meta': {\n            'date_analyzed': analysis_date,\n            'staging_file_sha256': None\n        },\n        'stats': {\n            'total_queries_extracted': len(raw_queries),\n            'unique_patterns': len(query_patterns)\n        },\n        'suggestions': []\n    }\n    \n    # Calculate SHA-256 checksum of staging file\n    sha256_hash = hashlib.sha256()\n    try:\n        with open(staging_file, 'rb') as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        report['meta']['staging_file_sha256'] = sha256_hash.hexdigest()\n    except Exception as e:\n        print(f\"Error calculating checksum: {e}\")\n        sys.exit(1)\n    \n    # Prepare suggestions in chronological order\n    for pattern in query_patterns.values():\n        if pattern['index_suggestion']:\n            report['suggestions'].append({\n                'id': pattern['first_timestamp'],\n                'normalized_sql': pattern['normalized_sql'],\n                'index_suggestion': "]
[207.571104, "i", "pattern['index_suggestion']\n            })\n    \n    # Sort suggestions by timestamp\n    report['suggestions'].sort(key=lambda x: x['id'])\n    \n    # Write final report\n    report_file = \"report.json\"\n    try:\n        with open(report_file, 'w', encoding='utf-8', newline='\\n') as f:\n            json.dump(report, f, indent=2)\n    except Exception as e:\n        print(f\"Error writing report: {e}\")\n        sys.exit(1)\n    \n    print(f\"Analysis complete. Report generated: {report_file}\")\n\nif __name__ == \"__main__\":\n    main()\nEOF\n"]
[207.571112, "o", "cat > analyze_and_optimize.py << 'EOF'\r\n"]
[207.57114, "o", "\u001b[?2004l\r"]
[207.571169, "o", "\u001b[?2004h"]
[207.571188, "o", "> "]
[207.571371, "o", "#!/usr/bin/env python3\r\n"]
[207.571391, "o", "\u001b[?2004l\r"]
[207.571416, "o", "\u001b[?2004h"]
[207.571431, "o", "> \r\n"]
[207.571451, "o", "\u001b[?2004l\r"]
[207.571474, "o", "\u001b[?2004h"]
[207.571497, "o", "> "]
[207.571519, "o", "\"\"\"\r\n"]
[207.571532, "o", "\u001b[?2004l\r"]
[207.57156, "o", "\u001b[?2004h"]
[207.57158, "o", "> "]
[207.571858, "o", "SQL Query Log Analysis & Optimization Script\r\n\u001b[?2004l\r"]
[207.571878, "o", "\u001b[?2004h"]
[207.571896, "o", "> "]
[207.571924, "o", "\"\"\"\r\n\u001b[?2004l\r"]
[207.571956, "o", "\u001b[?2004h"]
[207.571975, "o", "> \r\n\u001b[?2004l\r"]
[207.57201, "o", "\u001b[?2004h"]
[207.572031, "o", "> "]
[207.572097, "o", "import sys"]
[207.572113, "o", "\r\n\u001b[?2004l\r"]
[207.572129, "o", "\u001b[?2004h"]
[207.572148, "o", "> "]
[207.572215, "o", "import os\r\n\u001b[?2004l\r"]
[207.572242, "o", "\u001b[?2004h"]
[207.57226, "o", "> "]
[207.572354, "o", "import csv\r\n"]
[207.572372, "o", "\u001b[?2004l\r"]
[207.572389, "o", "\u001b[?2004h"]
[207.572406, "o", "> "]
[207.572476, "o", "import re"]
[207.572492, "o", "\r\n\u001b[?2004l\r"]
[207.572508, "o", "\u001b[?2004h"]
[207.572525, "o", "> "]
[207.572623, "o", "import hashlib\r\n"]
[207.572639, "o", "\u001b[?2004l\r"]
[207.572657, "o", "\u001b[?2004h"]
[207.572673, "o", "> "]
[207.572756, "o", "import json\r\n"]
[207.572775, "o", "\u001b[?2004l\r"]
[207.572796, "o", "\u001b[?2004h> "]
[207.572927, "o", "import configparser\r\n\u001b[?2004l\r"]
[207.572953, "o", "\u001b[?2004h"]
[207.572971, "o", "> "]
[207.573156, "o", "from datetime import datetime\r\n"]
[207.573173, "o", "\u001b[?2004l\r"]
[207.573192, "o", "\u001b[?2004h"]
[207.57321, "o", "> "]
[207.573434, "o", "from sqlparse import parse, format\r\n\u001b[?2004l\r"]
[207.573462, "o", "\u001b[?2004h"]
[207.573479, "o", "> "]
[207.573899, "o", "from sqlparse.sql import IdentifierList, Identifier, Where, Comparison\r\n"]
[207.573915, "o", "\u001b[?2004l\r"]
[207.57393, "o", "\u001b[?2004h"]
[207.57395, "o", "> "]
[207.574198, "o", "from sqlparse.tokens import Keyword, DML\r\n"]
[207.574214, "o", "\u001b[?2004l\r"]
[207.574232, "o", "\u001b[?2004h"]
[207.574249, "o", "> \r\n\u001b[?2004l\r"]
[207.574283, "o", "\u001b[?2004h"]
[207.574299, "o", "> "]
[207.574473, "o", "# Configuration file path\r\n\u001b[?2004l\r"]
[207.5745, "o", "\u001b[?2004h"]
[207.574517, "o", "> "]
[207.574818, "o", "CONFIG_PATH = \"/home/user/db_tools/db_config.ini\"\r\n"]
[207.574834, "o", "\u001b[?2004l\r"]
[207.574852, "o", "\u001b[?2004h"]
[207.574868, "o", "> \r\n"]
[207.574888, "o", "\u001b[?2004l\r"]
[207.57491, "o", "\u001b[?2004h"]
[207.574927, "o", "> "]
[207.575112, "o", "# Script argument validation\r\n"]
[207.575127, "o", "\u001b[?2004l\r"]
[207.575144, "o", "\u001b[?2004h"]
[207.575161, "o", "> "]
[207.575319, "o", "def validate_arguments():\r\n"]
[207.575353, "o", "\u001b[?2004l\r"]
[207.575368, "o", "\u001b[?2004h"]
[207.575388, "o", "> "]
[207.575556, "o", "    if len(sys.argv) != 3:\r\n\u001b[?2004l\r"]
[207.575583, "o", "\u001b[?2004h"]
[207.575601, "o", "> "]
[207.575995, "o", "        print(\"Usage: {} --date YYYY-MM-DD\".format(sys.argv[0]))\r\n\u001b[?2004l\r"]
[207.576027, "o", "\u001b[?2004h"]
[207.576044, "o", "> "]
[207.576171, "o", "        sys.exit(1)\r\n"]
[207.576189, "o", "\u001b[?2004l\r"]
[207.576206, "o", "\u001b[?2004h"]
[207.576222, "o", "> "]
[207.576262, "o", "    \r\n"]
[207.576279, "o", "\u001b[?2004l\r"]
[207.576297, "o", "\u001b[?2004h"]
[207.576312, "o", "> "]
[207.57657, "o", "    if sys.argv[1].lower() != \"--date\":\r\n\u001b[?2004l\r"]
[207.576592, "o", "\u001b[?2004h"]
[207.57661, "o", "> "]
[207.576931, "o", "        print(\"Invalid argument. Use --date flag.\")\r\n\u001b[?2004l\r"]
[207.576959, "o", "\u001b[?2004h"]
[207.576977, "o", "> "]
[207.577108, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[207.577133, "o", "\u001b[?2004h"]
[207.577152, "o", "> "]
[207.577192, "o", "    \r\n"]
[207.577209, "o", "\u001b[?2004l\r"]
[207.577227, "o", "\u001b[?2004h"]
[207.577242, "o", "> "]
[207.577305, "o", "    try:\r\n"]
[207.577321, "o", "\u001b[?2004l\r"]
[207.577341, "o", "\u001b[?2004h"]
[207.577365, "o", "> "]
[207.577662, "o", "        datetime.strptime(sys.argv[2], \"%Y-%m-%d\")\r\n"]
[207.577678, "o", "\u001b[?2004l\r"]
[207.577696, "o", "\u001b[?2004h"]
[207.577713, "o", "> "]
[207.577864, "o", "    except ValueError:\r\n\u001b[?2004l\r"]
[207.577889, "o", "\u001b[?2004h"]
[207.577907, "o", "> "]
[207.578225, "o", "        print(\"Invalid date format. Use YYYY-MM-DD.\")\r\n\u001b[?2004l\r"]
[207.578253, "o", "\u001b[?2004h"]
[207.57827, "o", "> "]
[207.578414, "o", "        sys.exit(1)\r\n"]
[207.578431, "o", "\u001b[?2004l\r"]
[207.578449, "o", "\u001b[?2004h"]
[207.578465, "o", "> \r\n\u001b[?2004l\r"]
[207.578496, "o", "\u001b[?2004h"]
[207.578515, "o", "> "]
[207.578703, "o", "def read_configuration():\r\n"]
[207.57872, "o", "\u001b[?2004l\r"]
[207.578743, "o", "\u001b[?2004h"]
[207.578762, "o", "> "]
[207.579016, "o", "    \"\"\"Read database configuration.\"\"\"\r\n"]
[207.579033, "o", "\u001b[?2004l\r"]
[207.579049, "o", "\u001b[?2004h"]
[207.579067, "o", "> "]
[207.579362, "o", "    config = configparser.ConfigParser()"]
[207.579382, "o", "\r\n\u001b[?2004l\r"]
[207.579417, "o", "\u001b[?2004h"]
[207.579435, "o", "> "]
[207.57963, "o", "    config.read(CONFIG_PATH)\r\n"]
[207.579651, "o", "\u001b[?2004l\r"]
[207.579669, "o", "\u001b[?2004h"]
[207.579688, "o", "> "]
[207.579732, "o", "    \r\n"]
[207.579748, "o", "\u001b[?2004l\r"]
[207.57977, "o", "\u001b[?2004h"]
[207.579789, "o", "> "]
[207.579891, "o", "    return {\r\n"]
[207.579912, "o", "\u001b[?2004l\r"]
[207.579927, "o", "\u001b[?2004h"]
[207.579945, "o", "> "]
[207.580357, "o", "        'log_file_path': config.get('settings', 'log_file_path'),\r\n\u001b[?2004l\r"]
[207.580388, "o", "\u001b[?2004h"]
[207.580407, "o", "> "]
[207.580855, "o", "        'min_duration_ms': int(config.get('settings', 'min_duration_ms'))\r\n"]
[207.580872, "o", "\u001b[?2004l\r"]
[207.580895, "o", "\u001b[?2004h"]
[207.580914, "o", "> "]
[207.580966, "o", "    }\r\n"]
[207.580983, "o", "\u001b[?2004l\r"]
[207.581005, "o", "\u001b[?2004h"]
[207.581023, "o", "> "]
[207.581038, "o", "\r\n\u001b[?2004l\r"]
[207.581066, "o", "\u001b[?2004h"]
[207.581085, "o", "> "]
[207.58131, "o", "def parse_duration(duration_str):\r\n"]
[207.581331, "o", "\u001b[?2004l\r"]
[207.581347, "o", "\u001b[?2004h"]
[207.581366, "o", "> "]
[207.581767, "o", "    \"\"\"Parse duration string (e.g., '15ms') to milliseconds.\"\"\"\r\n"]
[207.581786, "o", "\u001b[?2004l\r"]
[207.581805, "o", "\u001b[?2004h"]
[207.581823, "o", "> "]
[207.58215, "o", "    match = re.match(r'^([0-9]+)ms$', duration_str)\r\n\u001b[?2004l\r"]
[207.582179, "o", "\u001b[?2004h"]
[207.582197, "o", "> "]
[207.582301, "o", "    if match:\r\n"]
[207.582317, "o", "\u001b[?2004l\r"]
[207.582338, "o", "\u001b[?2004h"]
[207.582359, "o", "> "]
[207.582592, "o", "        return int(match.group(1))\r\n"]
[207.58261, "o", "\u001b[?2004l\r"]
[207.582626, "o", "\u001b[?2004h"]
[207.582644, "o", "> "]
[207.582747, "o", "    return 0\r\n"]
[207.582763, "o", "\u001b[?2004l\r"]
[207.582779, "o", "\u001b[?2004h"]
[207.582797, "o", "> "]
[207.582813, "o", "\r\n\u001b[?2004l\r"]
[207.582841, "o", "\u001b[?2004h"]
[207.58286, "o", "> "]
[207.583041, "o", "def parse_log_line(line):\r\n"]
[207.583063, "o", "\u001b[?2004l\r"]
[207.583078, "o", "\u001b[?2004h"]
[207.583098, "o", "> "]
[207.583365, "o", "    \"\"\"Parse pipe-delimited log line.\"\"\"\r\n\u001b[?2004l\r"]
[207.583395, "o", "\u001b[?2004h"]
[207.583414, "o", "> "]
[207.583719, "o", "    parts = [p.strip() for p in line.split('|')]\r\n"]
[207.58374, "o", "\u001b[?2004l\r"]
[207.583755, "o", "\u001b[?2004h"]
[207.583773, "o", "> "]
[207.583937, "o", "    if len(parts) != 4:\r\n"]
[207.583954, "o", "\u001b[?2004l\r"]
[207.58397, "o", "\u001b[?2004h"]
[207.583989, "o", "> "]
[207.584131, "o", "        return None\r\n"]
[207.584147, "o", "\u001b[?2004l\r"]
[207.584163, "o", "\u001b[?2004h"]
[207.584181, "o", "> "]
[207.584225, "o", "    \r\n"]
[207.58424, "o", "\u001b[?2004l\r"]
[207.584263, "o", "\u001b[?2004h"]
[207.584282, "o", "> "]
[207.584465, "o", "    timestamp = parts[0]\r\n\u001b[?2004l\r"]
[207.5845, "o", "\u001b[?2004h"]
[207.584519, "o", "> "]
[207.584769, "o", "    duration = parse_duration(parts[1])\r\n"]
[207.584785, "o", "\u001b[?2004l\r"]
[207.584802, "o", "\u001b[?2004h"]
[207.58482, "o", "> "]
[207.584969, "o", "    level = parts[2]\r\n"]
[207.584986, "o", "\u001b[?2004l\r"]
[207.585001, "o", "\u001b[?2004h"]
[207.585019, "o", "> "]
[207.58518, "o", "    message = parts[3]\r\n"]
[207.585196, "o", "\u001b[?2004l\r"]
[207.585213, "o", "\u001b[?2004h"]
[207.585231, "o", "> "]
[207.585275, "o", "    \r\n"]
[207.58529, "o", "\u001b[?2004l\r"]
[207.585313, "o", "\u001b[?2004h"]
[207.585334, "o", "> "]
[207.585443, "o", "    return {\r\n"]
[207.595247, "o", "\u001b[?2004l\r\u001b[?2004h>         'timestamp': timestamp,\r\n\u001b[?2004l\r\u001b[?2004h>         'duration': duration,\r\n\u001b[?2004l\r\u001b[?2004h>         'level': level,\r\n\u001b[?2004l\r\u001b[?2004h>         'message': message\r\n\u001b[?2004l\r\u001b[?2004h>     }\r\n\u001b[?2004l\r\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h> def normalize_sql(query):\r\n\u001b[?2004l\r\u001b[?2004h>     \"\"\"Normalize SQL using sqlparse.\"\"\"\r\n\u001b[?2004l\r\u001b[?2004h>     try:\r\n\u001b[?2004l\r\u001b[?2004h>         parsed = parse(query)\r\n\u001b[?2004l\r\u001b[?2004h>         # Use the first statement from the parsed result\r\n\u001b[?2004l\r\u001b[?2004h>         if parsed:\r\n\u001b[?2004l\r\u001b[?2004h>             stmt = parsed[0]\r\n\u001b[?2004l\r\u001b[?2004h>             formatted = format(\r\n\u001b[?2004l\r\u001b[?2004h>                 stmt,\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.595421, "o", "                keyword_case='upper',\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.595523, "o", "                strip_comments=True,\r\n\u001b[?2004l\r"]
[207.595597, "o", "\u001b[?2004h> "]
[207.595695, "o", "                reindent=False,\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.596733, "o", "                truncate_strings=1000\r\n\u001b[?2004l\r\u001b[?2004h>             )\r\n\u001b[?2004l\r\u001b[?2004h>             \r\n\u001b[?2004l\r \u001b[?2004h>             # Remove trailing semicolon\r\n\u001b[?2004l\r\u001b[?2004h>             sql_str = str(formatted).strip()\r\n\u001b[?2004l\r\u001b[?2004h>             if sql_str.endswith(';'):\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.596822, "o", "                sql_str = sql_str[:-1]\r\n\u001b[?2004l\rn i\u001b[?2004h> "]
[207.596886, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.596959, "o", "            return sql_str\r\n\u001b[?2004l\r"]
[207.597, "o", "\u001b[?2004h> "]
[207.597083, "o", "    except Exception as e:\r\n\u001b[?2004l\r\u001b[?2004h"]
[207.5971, "o", "> "]
[207.597231, "o", "        print(f\"Error normalizing SQL: {e}\")\r\n\u001b[?2004l\r"]
[207.597271, "o", "\u001b[?2004h> "]
[207.597341, "o", "        return query\r\n\u001b[?2004l\r"]
[207.597382, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[207.597426, "o", "\u001b[?2004h> "]
[207.597508, "o", "def extract_table_and_columns(parsed):\r\n\u001b[?2004l\r"]
[207.597548, "o", "\u001b[?2004h> "]
[207.597699, "o", "    \"\"\"Extract table and columns from WHERE clause.\"\"\"\r\n\u001b[?2004l\r"]
[207.597736, "o", "\u001b[?2004h"]
[207.597778, "o", "> "]
[207.597799, "o", "    table = None\r\n\u001b[?2004l\r\u001b[?2004h"]
[207.597846, "o", "> "]
[207.597877, "o", "    columns = []\r\n\u001b[?2004l\r"]
[207.597919, "o", "\u001b[?2004h> "]
[207.597959, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.59806, "o", "    # Find FROM clause to identify table\r\n\u001b[?2004l\r"]
[207.598077, "o", "\u001b[?2004h"]
[207.598124, "o", "> "]
[207.598162, "o", "    from_seen = False\r\n\u001b[?2004l\r"]
[207.598199, "o", "\u001b[?2004h> "]
[207.598283, "o", "    for token in parsed.tokens:\r\n\u001b[?2004l\r"]
[207.598299, "o", "\u001b[?2004h"]
[207.598348, "o", "> "]
[207.598459, "o", "        if isinstance(token, IdentifierList):\r\n\u001b[?2004l\r"]
[207.598475, "o", "\u001b[?2004h> "]
[207.598561, "o", "            if from_seen:\r\n\u001b[?2004l\r"]
[207.598594, "o", "\u001b[?2004h> "]
[207.5988, "o", "                table = ' '.join([str(t) for t in token.get_identifiers()])\r\n\u001b[?2004l\r"]
[207.598833, "o", "\u001b[?2004h> "]
[207.598895, "o", "                break\r\n\u001b[?2004l\r"]
[207.598911, "o", "\u001b[?2004h> "]
[207.599042, "o", "        elif token.value.upper() == 'FROM':\r\n\u001b[?2004l\r"]
[207.599075, "o", "\u001b[?2004h"]
[207.599117, "o", "> "]
[207.599165, "o", "            from_seen = True\r\n\u001b[?2004l\r"]
[207.599198, "o", "\u001b[?2004h> "]
[207.599238, "o", "    \r\n\u001b[?2004l\r"]
[207.599278, "o", "\u001b[?2004h> "]
[207.599315, "o", "    # Find WHERE clause\r\n\u001b[?2004l\r"]
[207.599332, "o", "\u001b[?2004h"]
[207.59938, "o", "> "]
[207.59941, "o", "    where = None\r\n\u001b[?2004l\r"]
[207.599454, "o", "\u001b[?2004h> "]
[207.599533, "o", "    for token in parsed.tokens:\r\n\u001b[?2004l\r"]
[207.599572, "o", "\u001b[?2004h> "]
[207.599672, "o", "        if isinstance(token, Where):\r\n\u001b[?2004l\r"]
[207.59971, "o", "\u001b[?2004h> "]
[207.599779, "o", "            where = token\r\n\u001b[?2004l\r"]
[207.599795, "o", "\u001b[?2004h> "]
[207.599859, "o", "            break\r\n\u001b[?2004l\r"]
[207.599874, "o", "\u001b[?2004h> "]
[207.599919, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.599983, "o", "    if not where:\r\n\u001b[?2004l\r"]
[207.600014, "o", "\u001b[?2004h"]
[207.600057, "o", "> "]
[207.600108, "o", "        return table, columns\r\n\u001b[?2004l\r"]
[207.600148, "o", "\u001b[?2004h"]
[207.600191, "o", ">     \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.6003, "o", "    # Parse conditions in WHERE clause\r\n\u001b[?2004l\r"]
[207.600354, "o", "\u001b[?2004h> "]
[207.600425, "o", "    for token in where.tokens:\r\n\u001b[?2004l\r"]
[207.600441, "o", "\u001b[?2004h"]
[207.600487, "o", "> "]
[207.600582, "o", "        if isinstance(token, Comparison):\r\n\u001b[?2004l\r"]
[207.600622, "o", "\u001b[?2004h> "]
[207.60073, "o", "            # Simple equality: col = val\r\n\u001b[?2004l\r"]
[207.600746, "o", "\u001b[?2004h> "]
[207.601001, "o", "            if token.tokens[1].value == '=' and isinstance(token.tokens[0], Identifier):\r\n\u001b[?2004l\r"]
[207.601035, "o", "\u001b[?2004h"]
[207.601079, "o", "> "]
[207.601169, "o", "                col_name = str(token.tokens[0])\r\n\u001b[?2004l\r"]
[207.601198, "o", "\u001b[?2004h"]
[207.601242, "o", "> "]
[207.601331, "o", "                columns.append(col_name)\r\n\u001b[?2004l\r"]
[207.601345, "o", "\u001b[?2004h"]
[207.601389, "o", ">     \r\n\u001b[?2004l\r"]
[207.60142, "o", "\u001b[?2004h> "]
[207.601487, "o", "    return table, columns\r\n\u001b[?2004l\r"]
[207.60152, "o", "\u001b[?2004h"]
[207.601537, "o", "> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.601581, "o", "def main():\r\n\u001b[?2004l\r"]
[207.601608, "o", "\u001b[?2004h"]
[207.601652, "o", "> "]
[207.601696, "o", "    validate_arguments()\r\n\u001b[?2004l\r"]
[207.60173, "o", "\u001b[?2004h> "]
[207.601767, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.601872, "o", "    config = read_configuration()\r\n\u001b[?2004l\r"]
[207.60191, "o", "\u001b[?2004h"]
[207.601924, "o", "> "]
[207.602001, "o", "    analysis_date = sys.argv[2]\r\n\u001b[?2004l\r"]
[207.602038, "o", "\u001b[?2004h> "]
[207.602079, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[207.602118, "o", "> "]
[207.602176, "o", "    # Create staging directory\r\n\u001b[?2004l\r"]
[207.602215, "o", "\u001b[?2004h> "]
[207.602288, "o", "    staging_dir = \"staging\"\r\n\u001b[?2004l\r"]
[207.60234, "o", "\u001b[?2004h> "]
[207.60244, "o", "    if not os.path.exists(staging_dir):\r\n\u001b[?2004l\r"]
[207.602478, "o", "\u001b[?2004h> "]
[207.602563, "o", "        os.makedirs(staging_dir)\r\n\u001b[?2004l\r"]
[207.602579, "o", "\u001b[?2004h> "]
[207.602622, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[207.602651, "o", "> "]
[207.602817, "o", "    staging_file = os.path.join(staging_dir, \"raw_extract.csv\")\r\n\u001b[?2004l\r"]
[207.602849, "o", "\u001b[?2004h> "]
[207.602893, "o", "    \r\n\u001b[?2004l\r"]
[207.60293, "o", "\u001b[?2004h> "]
[207.603, "o", "    # Phase 1: Filtering & Staging\r\n\u001b[?2004l\r"]
[207.603039, "o", "\u001b[?2004h"]
[207.603074, "o", "> "]
[207.603111, "o", "    raw_queries = []\r\n\u001b[?2004l\r\u001b[?2004h"]
[207.603146, "o", ">     \r\n\u001b[?2004l\r"]
[207.603184, "o", "\u001b[?2004h> "]
[207.603223, "o", "    try:\r\n\u001b[?2004l\r"]
[207.603263, "o", "\u001b[?2004h> "]
[207.603478, "o", "        with open(config['log_file_path'], 'r', encoding='utf-8') as log_file:\r\n\u001b[?2004l\r"]
[207.603511, "o", "\u001b[?2004h> "]
[207.603612, "o", "            for line in log_file:\r\n\u001b[?2004l\r"]
[207.603626, "o", "\u001b[?2004h"]
[207.603682, "o", "> "]
[207.603753, "o", "                if not line.strip():\r\n\u001b[?2004l\r"]
[207.6038, "o", "\u001b[?2004h> "]
[207.603875, "o", "                    continue\r\n\u001b[?2004l\r"]
[207.603889, "o", "\u001b[?2004h"]
[207.603944, "o", "> "]
[207.603986, "o", "                    \r\n\u001b[?2004l\r\u001b[?2004h"]
[207.604028, "o", "> "]
[207.604161, "o", "                parsed_line = parse_log_line(line)\r\n\u001b[?2004l\r"]
[207.604207, "o", "\u001b[?2004h> "]
[207.604302, "o", "                if not parsed_line:\r\n\u001b[?2004l\r"]
[207.604316, "o", "\u001b[?2004h"]
[207.604335, "o", "> "]
[207.604431, "o", "                    continue\r\n\u001b[?2004l\r"]
[207.604444, "o", "\u001b[?2004h"]
[207.604454, "o", "> "]
[207.604535, "o", "                    \r\n\u001b[?2004l\r"]
[207.604579, "o", "\u001b[?2004h> "]
[207.604772, "o", "                # Date match (first 10 characters of ISO timestamp)\r\n\u001b[?2004l\r"]
[207.604786, "o", "\u001b[?2004h"]
[207.604841, "o", "> "]
[207.605, "o", "                if parsed_line['timestamp'][:10] != analysis_date:\r\n\u001b[?2004l\r"]
[207.605014, "o", "\u001b[?2004h> "]
[207.605115, "o", "                    continue\r\n\u001b[?2004l\r"]
[207.605129, "o", "\u001b[?2004h"]
[207.605139, "o", "> "]
[207.605211, "o", "                    \r\n\u001b[?2004l\r"]
[207.605225, "o", "\u001b[?2004h"]
[207.605239, "o", "> "]
[207.605344, "o", "                # Type match\r\n\u001b[?2004l\r"]
[207.605357, "o", "\u001b[?2004h> "]
[207.605536, "o", "                if parsed_line['level'] != '[QUERY]':\r\n\u001b[?2004l\r"]
[207.60555, "o", "\u001b[?2004h"]
[207.605603, "o", "> "]
[207.605656, "o", "                    continue\r\n\u001b[?2004l\r"]
[207.605699, "o", "\u001b[?2004h"]
[207.605746, "o", "> "]
[207.605794, "o", "                    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.605872, "o", "                # Performance threshold\r\n\u001b[?2004l\r"]
[207.605886, "o", "\u001b[?2004h"]
[207.605901, "o", "> "]
[207.606078, "o", "                if parsed_line['duration'] < config['min_duration_ms']:\r\n\u001b[?2004l\r"]
[207.606091, "o", "\u001b[?2004h"]
[207.606102, "o", "> "]
[207.606188, "o", "                    continue\r\n\u001b[?2004l\r"]
[207.606203, "o", "\u001b[?2004h"]
[207.606213, "o", "> "]
[207.606287, "o", "                    \r\n\u001b[?2004l\r\u001b[?2004h"]
[207.606301, "o", "> "]
[207.606442, "o", "                # Convert duration to integer for CSV\r\n\u001b[?2004l\r"]
[207.606456, "o", "\u001b[?2004h"]
[207.606508, "o", "> "]
[207.606567, "o", "                raw_queries.append({\r\n\u001b[?2004l\r"]
[207.60658, "o", "\u001b[?2004h"]
[207.60659, "o", "> "]
[207.606742, "o", "                    'timestamp': parsed_line['timestamp'],\r\n\u001b[?2004l\r"]
[207.606756, "o", "\u001b[?2004h"]
[207.606808, "o", "> "]
[207.606936, "o", "                    'duration_ms': str(parsed_line['duration']),\r\n\u001b[?2004l\r"]
[207.60695, "o", "\u001b[?2004h"]
[207.606959, "o", "> "]
[207.607101, "o", "                    'query_text': parsed_line['message']\r\n\u001b[?2004l\r"]
[207.607115, "o", "\u001b[?2004h"]
[207.60713, "o", "> "]
[207.607195, "o", "                })\r\n\u001b[?2004l\r\u001b[?2004h"]
[207.607208, "o", "> "]
[207.607288, "o", "    except FileNotFoundError:\r\n\u001b[?2004l\r"]
[207.607302, "o", "\u001b[?2004h"]
[207.607311, "o", "> "]
[207.607487, "o", "        print(f\"Log file not found: {config['log_file_path']}\")\r\n\u001b[?2004l\r"]
[207.6075, "o", "\u001b[?2004h"]
[207.607551, "o", "> "]
[207.607598, "o", "        sys.exit(1)\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.607613, "o", "    \r\n\u001b[?2004l\r"]
[207.607628, "o", "\u001b[?2004h"]
[207.607692, "o", "> "]
[207.607812, "o", "    # Write to staging CSV with specific encoding and line endings\r\n\u001b[?2004l\r"]
[207.607855, "o", "\u001b[?2004h"]
[207.607878, "o", ">     try:\r\n\u001b[?2004l\r\u001b[?2004h"]
[207.607934, "o", "> "]
[207.608111, "o", "        with open(staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\r\n\u001b[?2004l\r"]
[207.608157, "o", "\u001b[?2004h> "]
[207.608315, "o", "            fieldnames = ['timestamp', 'duration_ms', 'query_text']\r\n\u001b[?2004l\r"]
[207.608332, "o", "\u001b[?2004h"]
[207.608341, "o", "> "]
[207.608529, "o", "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\r\n\u001b[?2004l\r"]
[207.608542, "o", "\u001b[?2004h"]
[207.608598, "o", "> "]
[207.608644, "o", "            writer.writeheader()\r\n\u001b[?2004l\r"]
[207.608658, "o", "\u001b[?2004h"]
[207.608667, "o", "> "]
[207.608764, "o", "            for query in raw_queries:\r\n\u001b[?2004l\r"]
[207.608778, "o", "\u001b[?2004h"]
[207.608793, "o", "> "]
[207.608899, "o", "                writer.writerow(query)\r\n\u001b[?2004l\r"]
[207.608912, "o", "\u001b[?2004h"]
[207.608964, "o", "> "]
[207.609002, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[207.609045, "o", "\u001b[?2004h"]
[207.609093, "o", "> "]
[207.609163, "o", "        print(f\"Error writing staging file: {e}\")\r\n\u001b[?2004l\r"]
[207.609206, "o", "\u001b[?2004h"]
[207.609224, "o", "> "]
[207.609239, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[207.609305, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.609419, "o", "    # Phase 2: Analysis & Normalization\r\n\u001b[?2004l\r"]
[207.609432, "o", "\u001b[?2004h> "]
[207.609508, "o", "    query_patterns = {}\r\n\u001b[?2004l\r"]
[207.609521, "o", "\u001b[?2004h"]
[207.609574, "o", ">     \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.609621, "o", "    try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.60984, "o", "        with open(staging_file, 'r', encoding='ISO-8859-1', newline='\\r\\n') as csvfile:\r\n\u001b[?2004l\r"]
[207.609854, "o", "\u001b[?2004h"]
[207.609863, "o", "> "]
[207.609985, "o", "            reader = csv.DictReader(csvfile)\r\n\u001b[?2004l\r"]
[207.610029, "o", "\u001b[?2004h"]
[207.610073, "o", "> "]
[207.610117, "o", "            for row in reader:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.610286, "o", "                normalized_sql = normalize_sql(row['query_text'])\r\n\u001b[?2004l\r"]
[207.6103, "o", "\u001b[?2004h"]
[207.610369, "o", ">                 \r\n\u001b[?2004l\r"]
[207.610382, "o", "\u001b[?2004h> "]
[207.610532, "o", "                if normalized_sql not in query_patterns:\r\n\u001b[?2004l\r"]
[207.610545, "o", "\u001b[?2004h> "]
[207.61069, "o", "                    query_patterns[normalized_sql] = {\r\n\u001b[?2004l\r"]
[207.610704, "o", "\u001b[?2004h"]
[207.610757, "o", "> "]
[207.610873, "o", "                        'first_timestamp': row['timestamp'],\r\n\u001b[?2004l\r"]
[207.610887, "o", "\u001b[?2004h"]
[207.610896, "o", "> "]
[207.611048, "o", "                        'normalized_sql': normalized_sql,\r\n\u001b[?2004l\r"]
[207.611062, "o", "\u001b[?2004h"]
[207.611078, "o", "> "]
[207.611203, "o", "                        'index_suggestion': None\r\n\u001b[?2004l\r"]
[207.611217, "o", "\u001b[?2004h"]
[207.611232, "o", "> "]
[207.611298, "o", "                    }\r\n\u001b[?2004l\r"]
[207.611312, "o", "\u001b[?2004h> "]
[207.611399, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[207.611413, "o", "\u001b[?2004h"]
[207.611423, "o", "> "]
[207.611558, "o", "        print(f\"Error processing staging file: {e}\")\r\n\u001b[?2004l\r"]
[207.611572, "o", "\u001b[?2004h> "]
[207.611634, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[207.611648, "o", "\u001b[?2004h> "]
[207.611708, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.611785, "o", "    # Generate index suggestions\r\n\u001b[?2004l\r"]
[207.611828, "o", "\u001b[?2004h"]
[207.611872, "o", "> "]
[207.611931, "o", "    for pattern in query_patterns.values():\r\n\u001b[?2004l\r"]
[207.611945, "o", "\u001b[?2004h"]
[207.611954, "o", "> "]
[207.612088, "o", "        parsed = parse(pattern['normalized_sql'])\r\n\u001b[?2004l\r"]
[207.612102, "o", "\u001b[?2004h"]
[207.612116, "o", "> "]
[207.612179, "o", "        if parsed:\r\n\u001b[?2004l\r\u001b[?2004h"]
[207.612193, "o", "> "]
[207.612363, "o", "            table, columns = extract_table_and_columns(parsed[0])\r\n\u001b[?2004l\r"]
[207.612377, "o", "\u001b[?2004h"]
[207.612433, "o", ">             \r\n\u001b[?2004l\r"]
[207.612447, "o", "\u001b[?2004h> "]
[207.612542, "o", "            if table and columns:\r\n\u001b[?2004l\r"]
[207.612555, "o", "\u001b[?2004h> "]
[207.612715, "o", "                # Format columns for index name and definition\r\n\u001b[?2004l\r"]
[207.612729, "o", "\u001b[?2004h"]
[207.612783, "o", "> "]
[207.613008, "o", "                index_name = f\"idx_{table.replace(' ', '_').replace('.', '_')}_{'_'.join(sorted(columns))}\"\r\n\u001b[?2004l\r"]
[207.613022, "o", "\u001b[?2004h"]
[207.613032, "o", "> "]
[207.613266, "o", "                index_def = f\"CREATE INDEX {index_name} ON {table} ({', '.join(columns)});\"\r\n\u001b[?2004l\r"]
[207.613279, "o", "\u001b[?2004h"]
[207.613336, "o", "> "]
[207.613441, "o", "                pattern['index_suggestion'] = index_def\r\n\u001b[?2004l\r"]
[207.613455, "o", "\u001b[?2004h"]
[207.613508, "o", ">         else:\r\n\u001b[?2004l\r"]
[207.613522, "o", "\u001b[?2004h> "]
[207.613648, "o", "            pattern['index_suggestion'] = None\r\n\u001b[?2004l\r"]
[207.613667, "o", "\u001b[?2004h> "]
[207.613722, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.613781, "o", "    # Phase 3: Reporting\r\n\u001b[?2004l\r"]
[207.613795, "o", "\u001b[?2004h"]
[207.613804, "o", "> "]
[207.613873, "o", "    report = {\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.613926, "o", "        'meta': {\r\n\u001b[?2004l\r"]
[207.613971, "o", "\u001b[?2004h"]
[207.614017, "o", "> "]
[207.614072, "o", "            'date_analyzed': analysis_date,\r\n\u001b[?2004l\r"]
[207.614086, "o", "\u001b[?2004h"]
[207.614095, "o", "> "]
[207.614204, "o", "            'staging_file_sha256': None\r\n\u001b[?2004l\r"]
[207.614218, "o", "\u001b[?2004h"]
[207.614228, "o", "> "]
[207.614294, "o", "        },\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.614347, "o", "        'stats': {\r\n\u001b[?2004l\r"]
[207.614391, "o", "\u001b[?2004h"]
[207.614436, "o", "> "]
[207.614524, "o", "            'total_queries_extracted': len(raw_queries),\r\n\u001b[?2004l\r"]
[207.614538, "o", "\u001b[?2004h"]
[207.614591, "o", "> "]
[207.614683, "o", "            'unique_patterns': len(query_patterns)\r\n\u001b[?2004l\r"]
[207.614728, "o", "\u001b[?2004h"]
[207.614775, "o", ">         },\r\n\u001b[?2004l\r"]
[207.614819, "o", "\u001b[?2004h> "]
[207.614865, "o", "        'suggestions': []\r\n\u001b[?2004l\r"]
[207.614908, "o", "\u001b[?2004h>     }\r\n\u001b[?2004l\r"]
[207.614954, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[207.615001, "o", "\u001b[?2004h> "]
[207.615079, "o", "    # Calculate SHA-256 checksum of staging file\r\n\u001b[?2004l\r"]
[207.615093, "o", "\u001b[?2004h"]
[207.61517, "o", "> "]
[207.615224, "o", "    sha256_hash = hashlib.sha256()\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.615279, "o", "    try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[207.615396, "o", "        with open(staging_file, 'rb') as f:\r\n\u001b[?2004l\r"]
[207.615416, "o", "\u001b[?2004h"]
[207.615489, "o", "> "]
[207.615592, "o", "            for byte_block in iter(lambda: f.read(4096), b\"\"):\r\n\u001b[?2004l\r"]
[207.61561, "o", "\u001b[?2004h> "]
[207.615733, "o", "                sha256_hash.update(byte_block)\r\n\u001b[?2004l\r"]
[207.615807, "o", "\u001b[?2004h> "]
[207.615938, "o", "        report['meta']['staging_file_sha256'] = sha256_hash.hexdigest()\r\n\u001b[?2004l\r"]
[207.61598, "o", "\u001b[?2004h> "]
[207.616039, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[207.616075, "o", "\u001b[?2004h> "]
[207.616189, "o", "        print(f\"Error calculating checksum: {e}\")\r\n\u001b[?2004l\r"]
[207.616226, "o", "\u001b[?2004h> "]
[207.616271, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[207.616308, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[207.61635, "o", "\u001b[?2004h> "]
[207.616471, "o", "    # Prepare suggestions in chronological order\r\n\u001b[?2004l\r"]
[207.616507, "o", "\u001b[?2004h> "]
[207.616606, "o", "    for pattern in query_patterns.values():\r\n\u001b[?2004l\r"]
[207.61664, "o", "\u001b[?2004h> "]
[207.616735, "o", "        if pattern['index_suggestion']:\r\n\u001b[?2004l\r"]
[207.616769, "o", "\u001b[?2004h> "]
[207.616872, "o", "            report['suggestions'].append({\r\n\u001b[?2004l\r"]
[207.616887, "o", "\u001b[?2004h"]
[207.616894, "o", "> "]
[207.617023, "o", "                'id': pattern['first_timestamp'],\r\n\u001b[?2004l\r"]
[207.617057, "o", "\u001b[?2004h> "]
[207.6172, "o", "                'normalized_sql': pattern['normalized_sql'],\r\n\u001b[?2004l\r"]
[207.617237, "o", "\u001b[?2004h> "]
[207.617392, "o", "                'index_suggestion': pattern['index_suggestion']\r\n\u001b[?2004l\r"]
[207.617423, "o", "\u001b[?2004h> "]
[207.617466, "o", "            })\r\n\u001b[?2004l\r"]
[207.617501, "o", "\u001b[?2004h> "]
[207.617511, "o", "    \r\n\u001b[?2004l\r"]
[207.617523, "o", "\u001b[?2004h"]
[207.617576, "o", "> "]
[207.617633, "o", "    # Sort suggestions by timestamp\r\n\u001b[?2004l\r"]
[207.617667, "o", "\u001b[?2004h> "]
[207.617794, "o", "    report['suggestions'].sort(key=lambda x: x['id'])\r\n\u001b[?2004l\r"]
[207.617829, "o", "\u001b[?2004h> "]
[207.617838, "o", "    \r\n\u001b[?2004l\r"]
[207.61785, "o", "\u001b[?2004h"]
[207.617902, "o", "> "]
[207.617934, "o", "    # Write final report\r\n\u001b[?2004l\r"]
[207.617967, "o", "\u001b[?2004h> "]
[207.618043, "o", "    report_file = \"report.json\"\r\n\u001b[?2004l\r"]
[207.618076, "o", "\u001b[?2004h> "]
[207.618107, "o", "    try:\r\n\u001b[?2004l\r"]
[207.618138, "o", "\u001b[?2004h> "]
[207.618306, "o", "        with open(report_file, 'w', encoding='utf-8', newline='\\n') as f:\r\n\u001b[?2004l\r"]
[207.61835, "o", "\u001b[?2004h> "]
[207.618454, "o", "            json.dump(report, f, indent=2)\r\n\u001b[?2004l\r"]
[207.618492, "o", "\u001b[?2004h> "]
[207.618554, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[207.618588, "o", "\u001b[?2004h> "]
[207.618692, "o", "        print(f\"Error writing report: {e}\")\r\n\u001b[?2004l\r"]
[207.618715, "o", "\u001b[?2004h"]
[207.618744, "o", "> "]
[207.618783, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[207.618813, "o", "\u001b[?2004h"]
[207.618823, "o", "> "]
[207.618836, "o", "    \r\n\u001b[?2004l\r"]
[207.618889, "o", "\u001b[?2004h> "]
[207.619028, "o", "    print(f\"Analysis complete. Report generated: {report_file}\")\r\n\u001b[?2004l\r"]
[207.619065, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[207.619075, "o", "\u001b[?2004h"]
[207.619087, "o", "> "]
[207.619162, "o", "if __name__ == \"__main__\":\r\n\u001b[?2004l\r"]
[207.619195, "o", "\u001b[?2004h> "]
[207.619222, "o", "    main()\r\n\u001b[?2004l\r"]
[207.619256, "o", "\u001b[?2004h> EOF\r\n"]
[207.619266, "o", "\u001b[?2004l\r"]
[207.621692, "o", "\u001b[?2004h(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[208.020262, "i", "chmod +x analyze_and_optimize.py\n"]
[208.020525, "o", "chmod +x analyze_and_optimize.py\r\n\u001b[?2004l\r"]
[208.022305, "o", "\u001b[?2004h(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[208.483968, "i", "./analyze_and_optimize.py --date 2023-03-15\n"]
[208.48427, "o", "./analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[208.520538, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[208.520855, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[208.52118, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[208.522374, "o", "Analysis complete. Report generated: report.json\r\n"]
[208.525621, "o", "\u001b[?2004h(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[213.486104, "i", "cat report.json\n"]
[213.486202, "o", "cat report.json\r\n\u001b[?2004l\r"]
[213.487051, "o", "{\r\n  \"meta\": {\r\n    \"date_analyzed\": \"2023-03-15\",\r\n    \"staging_file_sha256\": \"93ea43b0551e16504ee8491e23427d610811a86e898c04d51d7e1fed60074ffb\"\r\n  },\r\n  \"stats\": {\r\n    \"total_queries_extracted\": 3,\r\n    \"unique_patterns\": 3\r\n  },\r\n  \"suggestions\": []\r\n}"]
[213.487232, "o", "\u001b[?2004h"]
[213.487261, "o", "(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[234.325274, "i", "./analyze_and_optimize.py --date 2023-03-15\n"]
[234.325452, "o", "./analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[234.351933, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[234.352251, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[234.352591, "o", "Error normalizing SQL: Expected text or file-like object, got <class 'sqlparse.sql.Statement'>\r\n"]
[234.353753, "o", "Analysis complete. Report generated: report.json\r\n"]
[234.357098, "o", "\u001b[?2004h"]
[234.357111, "o", "(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
[239.25331, "i", "cat report.json\n"]
[239.253518, "o", "cat report.json\r\n\u001b[?2004l\r"]
[239.254497, "o", "{\r\n  \"meta\": {\r\n    \"date_analyzed\": \"2023-03-15\",\r\n    \"staging_file_sha256\": \"93ea43b0551e16504ee8491e23427d610811a86e898c04d51d7e1fed60074ffb\"\r\n  },\r\n  \"stats\": {\r\n    \"total_queries_extracted\": 3,\r\n    \"unique_patterns\": 3\r\n  },\r\n  \"suggestions\": []\r\n}"]
[239.254722, "o", "\u001b[?2004h"]
[239.254783, "o", "(env) root@e9fdca32-c56e-4278-926e-3af86e5d810d:/home/user/query_optimization# "]
