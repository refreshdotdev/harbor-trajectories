{
  "analysis": "The backup directory `/home/user/db_backups` contains several `.sql` files with varying permissions and naming conventions, including a file with leading spaces (`' orders.sql'`). I also see the decoy `batch_config.txt` which I need to ignore. The log directory `/home/user/backup_logs` has been successfully created. My next step is to retrieve the authoritative Batch ID from `/home/user/.config/backup_system.conf` and prepare the Python script for processing.",
  "plan": "I will first read the Batch ID from the specified configuration file. Then, I will create a Python script to handle the normalization (BOM stripping, trailing whitespace removal), compression, manifest generation, and compliance report generation. The script will need to carefully handle filenames, sorting, and character encoding as per requirements.",
  "commands": [
    {
      "keystrokes": "cat /home/user/.config/backup_system.conf 2>/dev/null || echo \"BATCH_ID_PLACEHOLDER\"\n",
      "duration": 0.5
    },
    {
      "keystrokes": "cat > process_backups.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport gzip\nimport json\nimport hashlib\nimport re\nfrom pathlib import Path\n\n# Configuration\nBACKUP_DIR = Path('/home/user/db_backups')\nLOG_DIR = Path('/home/user/backup_logs')\nLEGACY_CONFIG_FILE = Path('/home/user/.config/backup_system.conf')\n\ndef get_batch_id():\n    \"\"\"Read Batch ID from config file, stripping any trailing newlines.\"\"\"\n    try:\n        with open(LEGACY_CONFIG_FILE, 'r') as f:\n            batch_id = f.read().strip()\n            return batch_id if batch_id else \"UNKNOWN\"\n    except FileNotFoundError:\n        return \"UNKNOWN\"\n\ndef normalize_sql_content(content_bytes):\n    \"\"\"Strip UTF-8 BOM and trim trailing whitespace from every line.\"\"\"\n    # Strip UTF-8 BOM if present\n    content_bytes = content_bytes.lstrip(b'\\xef\\xbb\\xbf')\n    \n    # Trim trailing whitespace from every line\n    normalized_lines = []\n    for line in content_bytes.splitlines():\n        # Keep original line endings for now, process later\n        normalized_lines.append(line.rstrip(b' \\t') + b'\\n')\n    \n    return b''.join(normalized_lines)\n\ndef generate_manifest(normalized_sizes):\n    \"\"\"Generate manifest.json.\"\"\"\n    manifest_path = LOG_DIR / 'manifest.json'\n    with open(manifest_path, 'w', encoding='utf-8') as f:\n        json.dump(normalized_sizes, f, indent=2, ensure_ascii=False)\n\ndef generate_compliance_report(batch_id, file_records, manifest_path):\n    \"\"\"Generate compliance_report.log in Latin-1 with CRLF.\"\"\"\n    report_path = LOG_DIR / 'compliance_report.log'\n    \n    # Read manifest for its SHA256\n    with open(manifest_path, 'rb') as f:\n        manifest_sha256 = hashlib.sha256(f.read()).hexdigest()\n    \n    # Prepare file records with custom sorting\n    # Custom sort: case-insensitive, but '_' comes before '-'\n    file_records_sorted = sorted(\n        file_records.items(),\n        key=lambda item: (item[0].lower(), item[0].count('_'), item[0].count('-'))\n    )\n    \n    # Write report in Latin-1 with CRLF\n    with open(report_path, 'wb') as f:\n        # Header: BATCH: <Batch_ID>\n        header = f'BATCH: {batch_id}\\r\\n'.encode('latin-1')\n        f.write(header)\n        \n        # File Records\n        for filename, record in file_records_sorted:\n            line = (\n                f'FILE: {filename} | N_SIZE: {record[0]} | '\n                f'GZ_SIZE: {record[1]} | MD5: {record[2]}\\r\\n'\n            ).encode('latin-1')\n            f.write(line)\n        \n        # Footer: MANIFEST_SHA256: <hash>\n        footer = f'MANIFEST_SHA256: {manifest_sha256}\\r\\n'.encode('latin-1')\n        f.write(footer)\n\ndef process_sql_files():\n    \"\"\"Main processing function.\"\"\"\n    batch_id = get_batch_id()\n    print(f'Using Batch ID: {batch_id}')\n    \n    normalized_sizes = {}\n    file_records = {}\n    \n    # Find only .sql files (case-insensitive)\n    sql_files = list(BACKUP_DIR.glob('*.sql')) + list(BACKUP_DIR.glob('*.SQL'))\n    \n    for sql_file in sql_files:\n        print(f'Processing: {sql_file}')\n        try:\n            # Read raw file\n            with open(sql_file, 'rb') as f:\n                raw_content = f.read()\n            \n            # Normalize content\n            normalized_content = normalize_sql_content(raw_content)\n            n_size = len(normalized_content)\n            \n            # Store normalized size for manifest\n            normalized_sizes[str(sql_file.name)] = n_size\n            \n            # Compress normalized content\n            gz_path = sql_file.with_suffix('.sql.gz')\n            with gzip.open(gz_path, 'wb') as f:\n                f.write(normalized_content)\n            \n            # Get compressed file size\n            gz_size = gz_path.stat().st_size\n            \n            # Get MD5 of normalized content\n            md5_hash = hashlib.md5(normalized_content).hexdigest()\n            \n            # Store record for compliance report\n            file_records[str(sql_file.name)] = (n_size, gz_size, md5_hash)\n            \n            # Remove original .sql file\n            sql_file.unlink()\n            print(f'Processed: {sql_file.name} -> {gz_path.name}')\n            \n        except Exception as e:\n            print(f'Error processing {sql_file.name}: {e}')\n    \n    # Generate artifacts\n    if normalized_sizes:\n        generate_manifest(normalized_sizes)\n        generate_compliance_report(batch_id, file_records, LOG_DIR / 'manifest.json')\n        print('Backup processing complete. Manifest and compliance report generated.')\n    else:\n        print('No SQL files to process.')\n\nif __name__ == '__main__':\n    process_sql_files()\nEOF\n",
      "keystrokes": "chmod +x process_backups.py\n",
      "duration": 0.1
    }
  ],
  "task_complete": false
}