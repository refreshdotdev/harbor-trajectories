{"version": 2, "width": 160, "height": 40, "timestamp": 1771009464, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.00574, "o", "\u001b[?2004hroot@2c8c1f72-a726-4d8a-9ec0-ab27e94372c4:~# "]
[1.010974, "i", "clear\r"]
[1.011106, "o", "clear\r\n\u001b[?2004l\r"]
[1.012145, "o", "\u001b[H\u001b[J"]
[1.012295, "o", "\u001b[?2004h"]
[1.012321, "o", "root@2c8c1f72-a726-4d8a-9ec0-ab27e94372c4:~# "]
[10.46193, "i", "cd /home/user\n"]
[10.46206, "o", "cd /home/user\r\n\u001b[?2004l\r"]
[10.462097, "o", "\u001b[?2004hroot@2c8c1f72-a726-4d8a-9ec0-ab27e94372c4:/home/user# "]
[10.588884, "i", "mkdir -p mlops/experiments mlops/deprecated mlops/summary\n"]
[10.589217, "o", "mkdir -p mlops/experiments mlops/deprecated mlops/summary\r\n\u001b[?2004l\r"]
[10.591448, "o", "\u001b[?2004h"]
[10.5915, "o", "root@2c8c1f72-a726-4d8a-9ec0-ab27e94372c4:/home/user# "]
[10.707372, "i", "ls -la mlops/\n"]
[10.707501, "o", "ls -la mlops/\r\n\u001b[?2004l\r"]
[10.708958, "o", "total 0\r\n"]
[10.709004, "o", "drwxr-xr-x 1 user user 29 Feb 13 19:04 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 user user 27 Feb 12 23:58 \u001b[01;34m..\u001b[0m\r\ndrwxr-xr-x 1 user user 35 Feb 12 23:58 \u001b[01;34mdeprecated\u001b[0m\r\ndrwxr-xr-x 1 user user 94 Feb 12 23:58 \u001b[01;34mexperiments\u001b[0m\r\ndrwxr-xr-x 2 root root 10 Feb 13 19:04 \u001b[01;34msummary\u001b[0m\r\n"]
[10.709246, "o", "\u001b[?2004h"]
[10.709292, "o", "root@2c8c1f72-a726-4d8a-9ec0-ab27e94372c4:/home/user# "]
[60.928383, "i", "cat > mlops/process_artifacts.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport sys\nimport hashlib\nimport chardet\nimport yaml\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime\n\ndef detect_and_convert_encoding(file_path):\n    \"\"\"Detect file encoding and return decoded content as UTF-8 string\"\"\"\n    with open(file_path, 'rb') as f:\n        raw_data = f.read()\n    \n    if not raw_data:\n        return ''\n    \n    try:\n        # Try UTF-8 first\n        return raw_data.decode('utf-8')\n    except UnicodeDecodeError:\n        pass\n    \n    try:\n        # Try UTF-16LE with BOM\n        return raw_data.decode('utf-16-le')\n    except UnicodeDecodeError:\n        pass\n    \n    try:\n        # Try Latin-1\n        return raw_data.decode('latin-1')\n    except UnicodeDecodeError as e:\n        # Fallback to chardet detection\n        result = chardet.detect(raw_data)\n        encoding = result['encoding']\n        return raw_data.decode(encoding, errors='replace')\n\ndef is_line_corrupted(line):\n    \"\"\"Chec"]
[60.928443, "i", "k if line ends with odd number of trailing spaces\"\"\"\n    stripped_line = line.rstrip()\n    if len(line) == len(stripped_line):\n        return False  # No trailing spaces\n    return (len(line) - len(stripped_line)) % 2 != 0\n\ndef process_artifacts(base_path, deprecated_dir, summary_dir):\n    \"\"\"Main processing function\"\"\"\n    \n    # Create corruption report file\n    corruption_report_path = os.path.join(summary_dir, 'corruption_report.txt')\n    with open(corruption_report_path, 'w', encoding='utf-8') as corr_f:\n        corr_f.write('')  # Create empty file\n    \n    # Load deprecated patterns\n    deprecated_patterns = set()\n    for fname in os.listdir(deprecated_dir):\n        root_name = os.path.splitext(fname)[0].lower()\n        deprecated_patterns.add(root_name)\n    \n    # Process status override map\n    def load_status_map(override_map_path):\n        status_map = {}\n        if not os.path.exists(override_map_path):\n            return status_map\n        with open(override_map_path, 'r', encoding='utf-8', error"]
[60.928462, "i", "s='ignore') as f:\n            for line in f:\n                line = line.strip()\n                if line and '=' in line:\n                    key, val = line.split('=', 1)\n                    status_map[key.strip()] = val.strip()\n        return status_map\n    \n    # Main processing\n    valid_artifacts = defaultdict(list)\n    \n    for root, dirs, files in os.walk(os.path.join(base_path, 'mlops', 'experiments')):\n        for file in files:\n            if file not in ['artifacts.txt', 'artifacts.log']:\n                continue\n            \n            file_path = os.path.join(root, file)\n            \n            try:\n                content = detect_and_convert_encoding(file_path)\n            except Exception as e:\n                with open(corruption_report_path, 'a', encoding='utf-8') as corr_f:\n                    corr_f.write(f\"{file_path}: Error reading file - {str(e)}\\n\")\n                continue\n            \n            # Load status overrides for this experiment\n            status_map = load_status_map(o"]
[60.928479, "i", "s.path.join(root, 'status_override.map'))\n            \n            lines = content.splitlines()\n            for line_num, line in enumerate(lines):\n                try:\n                    original_line = line  # Preserve for corruption report\n                    line = line.strip()\n                    \n                    # Skip empty lines\n                    if not line:\n                        continue\n                    \n                    # Validate line integrity\n                    if is_line_corrupted(original_line):\n                        with open(corruption_report_path, 'a', encoding='utf-8') as corr_f:\n                            corr_f.write(f\"{file_path}:{line_num+1}: {original_line}\\n\")\n                        continue\n                    \n                    # Parse line\n                    parts = line.split('|', 2)\n                    if len(parts) < 3:\n                        continue\n                    \n                    name = parts[0].strip()\n                    status = parts[1]."]
[60.92871, "i", "strip()\n                    # metadata = parts[2].strip()  # Not used in final output\n                    \n                    # Apply status override\n                    if name in status_map:\n                        status = status_map[name]\n                    \n                    # Check if status is READY\n                    if status.upper() != 'READY':\n                        continue\n                    \n                    # Apply exclusion policy\n                    root_name = os.path.splitext(name)[0].lower()\n                    if root_name in deprecated_patterns:\n                        continue\n                    \n                    # Calculate relative path\n                    rel_path = os.path.relpath(root, os.path.join(base_path, 'mlops'))\n                    valid_artifacts[name].append(rel_path)\n                except Exception as e:\n                    with open(corruption_report_path, 'a', encoding='utf-8') as corr_f:\n                        corr_f.write(f\"{file_path}:{line_num+1}: Er"]
[60.928746, "i", "ror processing line - {str(e)}\\n\")\n    \n    # Generate inventory.csv\n    inventory_path = os.path.join(summary_dir, 'inventory.csv')\n    with open(inventory_path, 'w', encoding='utf-8') as inv_f:\n        inv_f.write(\"Artifact|NameHash|Locations\\n\")\n        \n        # Sort artifacts and write output\n        for name in sorted(valid_artifacts.keys()):\n            name_hash = hashlib.md5(name.encode('utf-8')).hexdigest()\n            locations = ';'.join(sorted(set(valid_artifacts[name])))\n            inv_f.write(f\"{name}|{name_hash}|{locations}\\n\")\n    \n    # Generate manifest.yaml\n    manifest_path = os.path.join(summary_dir, 'manifest.yaml')\n    manifest_data = {\n        'total_artifacts': len(valid_artifacts),\n        'generated_at': datetime.utcnow().strftime('%Y-%m-%d')\n    }\n    \n    # Calculate inventory hash\n    with open(inventory_path, 'rb') as inv_f:\n        manifest_data['inventory_hash'] = hashlib.sha256(inv_f.read()).hexdigest()\n    \n    with open(manifest_path, 'w', encoding='utf-8') as man_f:\n   "]
[60.928898, "i", "     yaml.safe_dump(manifest_data, man_f, default_flow_style=False)\n\nif __name__ == '__main__':\n    base_path = '/home/user'\n    deprecated_dir = os.path.join(base_path, 'mlops', 'deprecated')\n    summary_dir = os.path.join(base_path, 'mlops', 'summary')\n    process_artifacts(base_path, deprecated_dir, summary_dir)\nEOF"]
[60.92895, "o", "cat > mlops/process_artifacts.py << 'EOF'\r\n\u001b[?2004l\rstrip()\r\n                    # met"]
[60.929092, "o", "adata = \u001b[?2004h> "]
[60.929315, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[60.929379, "o", "\u001b[?2004h> "]
[60.929507, "o", "import os\r\n\u001b[?2004l\r"]
[60.929553, "o", "\u001b[?2004h"]
[60.929602, "o", "> "]
[60.929717, "o", "import sys\r\n\u001b[?2004l\r"]
[60.929759, "o", "\u001b[?2004h"]
[60.929795, "o", "> "]
[60.929967, "o", "import hashlib\r\n\u001b[?2004l\r"]
[60.930017, "o", "\u001b[?2004h"]
[60.930058, "o", "> "]
[60.930215, "o", "import chardet\r\n\u001b[?2004l\r"]
[60.930267, "o", "\u001b[?2004h"]
[60.930306, "o", "> "]
[60.93043, "o", "import yaml\r\n\u001b[?2004l\r"]
[60.930476, "o", "\u001b[?2004h"]
[60.930516, "o", "> "]
[60.93062, "o", "import re\r\n\u001b[?2004l\r"]
[60.930663, "o", "\u001b[?2004h"]
[60.930715, "o", "> "]
[60.931019, "o", "from collections import defaultdict\r\n\u001b[?2004l\r"]
[60.93112, "o", "\u001b[?2004h> "]
[60.931411, "o", "from datetime import datetime\r\n\u001b[?2004l\r"]
[60.931489, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[60.931561, "o", "\u001b[?2004h> "]
[60.932076, "o", "def detect_and_convert_encoding(file_path):\r\n\u001b[?2004l\r"]
[60.932123, "o", "\u001b[?2004h> "]
[60.932852, "o", "    \"\"\"Detect file encoding and return decoded content as UTF-8 string\"\"\"\r\n\u001b[?2004l\r"]
[60.932918, "o", "\u001b[?2004h> "]
[60.933236, "o", "    with open(file_path, 'rb') as f:\r\n\u001b[?2004l\r"]
[60.933289, "o", "\u001b[?2004h> "]
[60.933562, "o", "        raw_data = f.read()\r\n\u001b[?2004l\r"]
[60.933621, "o", "\u001b[?2004h> "]
[60.933681, "o", "    \r\n\u001b[?2004l\r"]
[60.933731, "o", "\u001b[?2004h"]
[60.933779, "o", "> "]
[60.933942, "o", "    if not raw_data:\r\n\u001b[?2004l\r"]
[60.933995, "o", "\u001b[?2004h> "]
[60.934159, "o", "        return ''\r\n\u001b[?2004l\r"]
[60.934205, "o", "\u001b[?2004h> "]
[60.934257, "o", "    \r\n\u001b[?2004l\r"]
[60.934308, "o", "\u001b[?2004h> "]
[60.934413, "o", "    try:\r\n\u001b[?2004l\r"]
[60.934467, "o", "\u001b[?2004h> "]
[60.934732, "o", "        # Try UTF-8 first\r\n\u001b[?2004l\r"]
[60.93479, "o", "\u001b[?2004h> "]
[60.935134, "o", "        return raw_data.decode('utf-8')\r\n\u001b[?2004l\r"]
[60.935199, "o", "\u001b[?2004h> "]
[60.935486, "o", "    except UnicodeDecodeError:\r\n\u001b[?2004l\r"]
[60.935537, "o", "\u001b[?2004h"]
[60.935588, "o", "> "]
[60.935674, "o", "        pass\r\n\u001b[?2004l\r"]
[60.935723, "o", "\u001b[?2004h"]
[60.93575, "o", "> "]
[60.935819, "o", "    \r\n\u001b[?2004l\r"]
[60.935882, "o", "\u001b[?2004h> "]
[60.935953, "o", "    try:\r\n\u001b[?2004l\r"]
[60.936001, "o", "\u001b[?2004h"]
[60.936047, "o", "> "]
[60.936328, "o", "        # Try UTF-16LE with BOM\r\n\u001b[?2004l\r"]
[60.936375, "o", "\u001b[?2004h"]
[60.936433, "o", "> "]
[60.93683, "o", "        return raw_data.decode('utf-16-le')\r\n\u001b[?2004l\r"]
[60.936888, "o", "\u001b[?2004h"]
[60.936936, "o", "> "]
[60.937243, "o", "    except UnicodeDecodeError:\r\n\u001b[?2004l\r"]
[60.937304, "o", "\u001b[?2004h"]
[60.937353, "o", "> "]
[60.937467, "o", "        pass\r\n\u001b[?2004l\r"]
[60.937518, "o", "\u001b[?2004h"]
[60.937561, "o", "> "]
[60.937606, "o", "    \r\n\u001b[?2004l\r"]
[60.937656, "o", "\u001b[?2004h> "]
[60.937755, "o", "    try:\r\n\u001b[?2004l\r"]
[60.937809, "o", "\u001b[?2004h> "]
[60.938048, "o", "        # Try Latin-1\r\n\u001b[?2004l\r"]
[60.938114, "o", "\u001b[?2004h> "]
[60.93861, "o", "        return raw_data.decode('latin-1')\r\n\u001b[?2004l\r"]
[60.938665, "o", "\u001b[?2004h> "]
[60.939104, "o", "    except UnicodeDecodeError as e:\r\n\u001b[?2004l\r"]
[60.939151, "o", "\u001b[?2004h> "]
[60.93952, "o", "        # Fallback to chardet detection\r\n\u001b[?2004l\r"]
[60.93957, "o", "\u001b[?2004h"]
[60.939618, "o", "> "]
[60.94009, "o", "        result = chardet.detect(raw_data)\r\n\u001b[?2004l\r"]
[60.940146, "o", "\u001b[?2004h> "]
[60.940573, "o", "        encoding = result['encoding']\r\n\u001b[?2004l\r"]
[60.940635, "o", "\u001b[?2004h> "]
[60.941184, "o", "        return raw_data.decode(encoding, errors='replace')\r\n\u001b[?2004l\r"]
[60.941245, "o", "\u001b[?2004h> \r\n"]
[60.941296, "o", "\u001b[?2004l\r\u001b[?2004h"]
[60.941341, "o", "> "]
[60.941607, "o", "def is_line_corrupted(line):\r\n\u001b[?2004l\r"]
[60.941661, "o", "\u001b[?2004h> "]
[60.942213, "o", "    \"\"\"Check if line ends with odd number of trailing spaces\"\"\"\r\n\u001b[?2004l\r"]
[60.942277, "o", "\u001b[?2004h> "]
[60.942613, "o", "    stripped_line = line.rstrip()\r\n\u001b[?2004l\r"]
[60.942656, "o", "\u001b[?2004h"]
[60.942698, "o", "> "]
[60.943014, "o", "    if len(line) == len(stripped_line):\r\n\u001b[?2004l\r"]
[60.943065, "o", "\u001b[?2004h> "]
[60.943431, "o", "        return False  # No trailing spaces\r\n\u001b[?2004l\r"]
[60.943481, "o", "\u001b[?2004h> "]
[60.943996, "o", "    return (len(line) - len(stripped_line)) % 2 != 0\r\n\u001b[?2004l\r"]
[60.944062, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[60.944125, "o", "\u001b[?2004h> "]
[60.944787, "o", "def process_artifacts(base_path, deprecated_dir, summary_dir):\r\n\u001b[?2004l\r"]
[60.944853, "o", "\u001b[?2004h> "]
[60.945196, "o", "    \"\"\"Main processing function\"\"\"\r\n\u001b[?2004l\r"]
[60.945256, "o", "\u001b[?2004h> "]
[60.945315, "o", "    \r\n\u001b[?2004l\r"]
[60.945391, "o", "\u001b[?2004h> "]
[60.945747, "o", "    # Create corruption report file\r\n\u001b[?2004l\r"]
[60.945811, "o", "\u001b[?2004h> "]
[60.946574, "o", "    corruption_report_path = os.path.join(summary_dir, 'corruption_report.txt')\r\n\u001b[?2004l\r"]
[60.946639, "o", "\u001b[?2004h> "]
[60.947355, "o", "    with open(corruption_report_path, 'w', encoding='utf-8') as corr_f:\r\n\u001b[?2004l\r"]
[60.947423, "o", "\u001b[?2004h"]
[60.947485, "o", "> "]
[60.947868, "o", "        corr_f.write('')  # Create empty file\r\n\u001b[?2004l\r"]
[60.947922, "o", "\u001b[?2004h> "]
[60.947987, "o", "    \r\n\u001b[?2004l\r"]
[60.948041, "o", "\u001b[?2004h> "]
[60.948354, "o", "    # Load deprecated patterns\r\n\u001b[?2004l\r"]
[60.948405, "o", "\u001b[?2004h"]
[60.948451, "o", "> "]
[60.948695, "o", "    deprecated_patterns = set()\r\n\u001b[?2004l\r"]
[60.948768, "o", "\u001b[?2004h> "]
[60.949194, "o", "    for fname in os.listdir(deprecated_dir):\r\n\u001b[?2004l\r"]
[60.949254, "o", "\u001b[?2004h> "]
[60.949804, "o", "        root_name = os.path.splitext(fname)[0].lower()\r\n\u001b[?2004l\r"]
[60.94985, "o", "\u001b[?2004h> "]
[60.950213, "o", "        deprecated_patterns.add(root_name)\r\n\u001b[?2004l\r"]
[60.950253, "o", "\u001b[?2004h"]
[60.950309, "o", ">     "]
[60.950367, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[60.950425, "o", "> "]
[60.950736, "o", "    # Process status override map\r\n\u001b[?2004l\r"]
[60.950793, "o", "\u001b[?2004h"]
[60.950841, "o", "> "]
[60.951177, "o", "    def load_status_map(override_map_path):\r\n\u001b[?2004l\r"]
[60.951237, "o", "\u001b[?2004h> "]
[60.951464, "o", "        status_map = {}\r\n\u001b[?2004l\r"]
[60.951513, "o", "\u001b[?2004h> "]
[60.952037, "o", "        if not os.path.exists(override_map_path):\r\n\u001b[?2004l\r"]
[60.952097, "o", "\u001b[?2004h> "]
[60.95239, "o", "            return status_map\r\n\u001b[?2004l\r"]
[60.952439, "o", "\u001b[?2004h> "]
[60.953222, "o", "        with open(override_map_path, 'r', encoding='utf-8', errors='ignore') as f:\r\n\u001b[?2004l\r"]
[60.953293, "o", "\u001b[?2004h> "]
[60.95354, "o", "            for line in f:\r\n\u001b[?2004l\r"]
[60.953601, "o", "\u001b[?2004h> "]
[60.95396, "o", "                line = line.strip()\r\n\u001b[?2004l\r"]
[60.954019, "o", "\u001b[?2004h"]
[60.954072, "o", "> "]
[60.954401, "o", "                if line and '=' in line:\r\n\u001b[?2004l\r"]
[60.954459, "o", "\u001b[?2004h> "]
[60.954971, "o", "                    key, val = line.split('=', 1)\r\n\u001b[?2004l\r"]
[60.955045, "o", "\u001b[?2004h> "]
[60.955505, "o", "                    status_map[key.strip()] = val.strip()\r\n\u001b[?2004l\r"]
[60.955567, "o", "\u001b[?2004h"]
[60.955618, "o", "> "]
[60.955794, "o", "        return status_map\r\n\u001b[?2004l\r"]
[60.955853, "o", "\u001b[?2004h> "]
[60.955914, "o", "    \r\n\u001b[?2004l\r"]
[60.955971, "o", "\u001b[?2004h> "]
[60.956194, "o", "    # Main processing\r\n\u001b[?2004l\r"]
[60.956242, "o", "\u001b[?2004h> "]
[60.956646, "o", "    valid_artifacts = defaultdict(list)\r\n\u001b[?2004l\r"]
[60.956701, "o", "\u001b[?2004h> "]
[60.956758, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[60.956808, "o", "> "]
[60.957372, "o", "    for root, dirs, files in os.walk(os.path.join(base_path, 'mlops', 'experiments')):\r\n\u001b[?2004l\r"]
[60.957432, "o", "\u001b[?2004h> "]
[60.957603, "o", "        for file in files:\r\n\u001b[?2004l\r"]
[60.957671, "o", "\u001b[?2004h> "]
[60.958067, "o", "            if file not in ['artifacts.txt', 'artifacts.log']:\r\n\u001b[?2004l\r"]
[60.95813, "o", "\u001b[?2004h> "]
[60.958309, "o", "                continue\r\n\u001b[?2004l\r"]
[60.958373, "o", "\u001b[?2004h> "]
[60.958452, "o", "            \r\n\u001b[?2004l\r"]
[60.958505, "o", "\u001b[?2004h> "]
[60.958827, "o", "            file_path = os.path.join(root, file)\r\n\u001b[?2004l\r"]
[60.958895, "o", "\u001b[?2004h> "]
[60.958982, "o", "            \r\n\u001b[?2004l\r"]
[60.959044, "o", "\u001b[?2004h"]
[60.959104, "o", "> "]
[60.959169, "o", "            try:\r\n\u001b[?2004l\r"]
[60.959208, "o", "\u001b[?2004h"]
[60.959253, "o", "> "]
[60.959649, "o", "                content = detect_and_convert_encoding(file_path)\r\n\u001b[?2004l\r"]
[60.95969, "o", "\u001b[?2004h"]
[60.959733, "o", "> "]
[60.959951, "o", "            except Exception as e:\r\n\u001b[?2004l\r"]
[60.960019, "o", "\u001b[?2004h> "]
[60.960552, "o", "                with open(corruption_report_path, 'a', encoding='utf-8') as corr_f:\r\n\u001b[?2004l\r"]
[60.960601, "o", "\u001b[?2004h> "]
[60.961154, "o", "                    corr_f.write(f\"{file_path}: Error reading file - {str(e)}\\n\")\r\n\u001b[?2004l\r"]
[60.961203, "o", "\u001b[?2004h> "]
[60.961367, "o", "                continue\r\n\u001b[?2004l\r"]
[60.96141, "o", "\u001b[?2004h> "]
[60.961522, "o", "            \r\n\u001b[?2004l\r"]
[60.961578, "o", "\u001b[?2004h> "]
[60.961954, "o", "            # Load status overrides for this experiment\r\n\u001b[?2004l\r"]
[60.962004, "o", "\u001b[?2004h> "]
[60.96255, "o", "            status_map = load_status_map(os.path.join(root, 'status_override.map'))\r\n\u001b[?2004l\r"]
[60.962621, "o", "\u001b[?2004h> "]
[60.962704, "o", "            \r\n\u001b[?2004l\r"]
[60.962761, "o", "\u001b[?2004h> "]
[60.963074, "o", "            lines = content.splitlines()\r\n\u001b[?2004l\r"]
[60.963148, "o", "\u001b[?2004h> "]
[60.963434, "o", "            for line_num, line in enumerate(lines):\r\n\u001b[?2004l\r"]
[60.963491, "o", "\u001b[?2004h> "]
[60.963643, "o", "                try:\r\n\u001b[?2004l\r"]
[60.963698, "o", "\u001b[?2004h> "]
[60.964181, "o", "                    original_line = line  # Preserve for corruption report\r\n\u001b[?2004l\r"]
[60.964225, "o", "\u001b[?2004h"]
[60.964274, "o", "> "]
[60.964509, "o", "                    line = line.strip()\r\n\u001b[?2004l\r"]
[60.964559, "o", "\u001b[?2004h"]
[60.964709, "o", ">                     \r\n\u001b[?2004l\r"]
[60.96476, "o", "\u001b[?2004h> "]
[60.965034, "o", "                    # Skip empty lines\r\n\u001b[?2004l\r"]
[60.965076, "o", "\u001b[?2004h"]
[60.965119, "o", "> "]
[60.96534, "o", "                    if not line:\r\n\u001b[?2004l\r"]
[60.965388, "o", "\u001b[?2004h"]
[60.965437, "o", "> "]
[60.965667, "o", "                        continue\r\n\u001b[?2004l\r"]
[60.965717, "o", "\u001b[?2004h"]
[60.965766, "o", "> "]
[60.965919, "o", "                    \r\n\u001b[?2004l\r"]
[60.965968, "o", "\u001b[?2004h"]
[60.966017, "o", "> "]
[60.966334, "o", "                    # Validate line integrity\r\n\u001b[?2004l\r"]
[60.966383, "o", "\u001b[?2004h"]
[60.96643, "o", "> "]
[60.966827, "o", "                    if is_line_corrupted(original_line):\r\n\u001b[?2004l\r"]
[60.966906, "o", "\u001b[?2004h> "]
[60.967588, "o", "                        with open(corruption_report_path, 'a', encoding='utf-8') as corr_f:\r\n\u001b[?2004l\r"]
[60.967646, "o", "\u001b[?2004h"]
[60.967694, "o", "> "]
[60.968335, "o", "                            corr_f.write(f\"{file_path}:{line_num+1}: {original_line}\\n\")\r\n\u001b[?2004l\r"]
[60.968381, "o", "\u001b[?2004h"]
[60.968431, "o", "> "]
[60.968647, "o", "                        continue\r\n\u001b[?2004l\r"]
[60.968699, "o", "\u001b[?2004h"]
[60.968747, "o", "> "]
[60.968883, "o", "                    \r\n\u001b[?2004l\r"]
[61.012287, "o", "\u001b[?2004h> "]
[61.012497, "o", "                    # Parse line\r\n\u001b[?2004l\r"]
[61.012542, "o", "\u001b[?2004h"]
[61.012581, "o", "> "]
[61.012857, "o", "                    parts = line.split('|', 2)\r\n\u001b[?2004l\r"]
[61.0129, "o", "\u001b[?2004h"]
[61.012937, "o", "> "]
[61.013191, "o", "                    if len(parts) < 3:\r\n\u001b[?2004l\r"]
[61.013229, "o", "\u001b[?2004h"]
[61.013266, "o", "> "]
[61.013473, "o", "                        continue\r\n\u001b[?2004l\r"]
[61.01351, "o", "\u001b[?2004h"]
[61.013547, "o", "> "]
[61.013665, "o", "                    \r\n\u001b[?2004l\r"]
[61.013703, "o", "\u001b[?2004h"]
[61.013741, "o", "> "]
[61.014011, "o", "                    name = parts[0].strip()\r\n\u001b[?2004l\r"]
[61.014071, "o", "\u001b[?2004h"]
[61.014109, "o", "> "]
[61.014376, "o", "                    status = parts[1].strip()\r\n\u001b[?2004l\r"]
[61.014413, "o", "\u001b[?2004h"]
[61.01445, "o", "> "]
[61.014956, "o", "                    # metadata = parts[2].strip()  # Not used in final output\r\n\u001b[?2004l\r"]
[61.014998, "o", "\u001b[?2004h"]
[61.015036, "o", "> "]
[61.01516, "o", "                    \r\n\u001b[?2004l\r"]
[61.015198, "o", "\u001b[?2004h"]
[61.015234, "o", "> "]
[61.015493, "o", "                    # Apply status override\r\n\u001b[?2004l\r"]
[61.015531, "o", "\u001b[?2004h"]
[61.015567, "o", "> "]
[61.01584, "o", "                    if name in status_map:\r\n\u001b[?2004l\r"]
[61.015893, "o", "\u001b[?2004h"]
[61.01593, "o", "> "]
[61.016232, "o", "                        status = status_map[name]\r\n\u001b[?2004l\r"]
[61.016271, "o", "\u001b[?2004h"]
[61.016308, "o", "> "]
[61.016438, "o", "                    \r\n\u001b[?2004l\r"]
[61.016479, "o", "\u001b[?2004h"]
[61.016517, "o", "> "]
[61.016803, "o", "                    # Check if status is READY\r\n\u001b[?2004l\r"]
[61.016841, "o", "\u001b[?2004h"]
[61.016895, "o", "> "]
[61.017209, "o", "                    if status.upper() != 'READY':\r\n\u001b[?2004l\r"]
[61.017259, "o", "\u001b[?2004h> "]
[61.017473, "o", "                        continue\r\n\u001b[?2004l\r"]
[61.01752, "o", "\u001b[?2004h> "]
[61.017672, "o", "                    \r\n\u001b[?2004l\r"]
[61.017722, "o", "\u001b[?2004h> "]
[61.018034, "o", "                    # Apply exclusion policy\r\n\u001b[?2004l\r"]
[61.018087, "o", "\u001b[?2004h> "]
[61.018498, "o", "                    root_name = os.path.splitext(name)[0].lower()\r\n\u001b[?2004l\r"]
[61.018554, "o", "\u001b[?2004h> "]
[61.018913, "o", "                    if root_name in deprecated_patterns:\r\n\u001b[?2004l\r"]
[61.018963, "o", "\u001b[?2004h"]
[61.019009, "o", "> "]
[61.019189, "o", "                        continue\r\n\u001b[?2004l\r"]
[61.019234, "o", "\u001b[?2004h"]
[61.019279, "o", "> "]
[61.019403, "o", "                    \r\n\u001b[?2004l\r"]
[61.019446, "o", "\u001b[?2004h> "]
[61.019755, "o", "                    # Calculate relative path\r\n\u001b[?2004l\r"]
[61.019801, "o", "\u001b[?2004h> "]
[61.020378, "o", "                    rel_path = os.path.relpath(root, os.path.join(base_path, 'mlops'))\r\n\u001b[?2004l\r"]
[61.020433, "o", "\u001b[?2004h> "]
[61.020809, "o", "                    valid_artifacts[name].append(rel_path)\r\n\u001b[?2004l\r"]
[61.020855, "o", "\u001b[?2004h> "]
[61.021136, "o", "                except Exception as e:\r\n\u001b[?2004l\r"]
[61.02119, "o", "\u001b[?2004h> "]
[61.021732, "o", "                    with open(corruption_report_path, 'a', encoding='utf-8') as corr_f:\r\n\u001b[?2004l\r"]
[61.021783, "o", "\u001b[?2004h> "]
[61.022462, "o", "                        corr_f.write(f\"{file_path}:{line_num+1}: Error processing line - {str(e)}\\n\")\r\n\u001b[?2004l\r"]
[61.02251, "o", "\u001b[?2004h> "]
[61.022566, "o", "    \r\n\u001b[?2004l\r"]
[61.022627, "o", "\u001b[?2004h> "]
[61.022807, "o", "    # Generate inventory.csv\r\n\u001b[?2004l\r"]
[61.02285, "o", "\u001b[?2004h"]
[61.022911, "o", "> "]
[61.023291, "o", "    inventory_path = os.path.join(summary_dir, 'inventory.csv')\r\n\u001b[?2004l\r"]
[61.023345, "o", "\u001b[?2004h"]
[61.023394, "o", "> "]
[61.02375, "o", "    with open(inventory_path, 'w', encoding='utf-8') as inv_f:\r\n\u001b[?2004l\r"]
[61.02381, "o", "\u001b[?2004h> "]
[61.024171, "o", "        inv_f.write(\"Artifact|NameHash|Locations\\n\")\r\n\u001b[?2004l\r"]
[61.024229, "o", "\u001b[?2004h> "]
[61.024282, "o", "        \r\n"]
[61.024333, "o", "\u001b[?2004l\r\u001b[?2004h"]
[61.024381, "o", "> "]
[61.024631, "o", "        # Sort artifacts and write output\r\n\u001b[?2004l\r"]
[61.02468, "o", "\u001b[?2004h> "]
[61.025025, "o", "        for name in sorted(valid_artifacts.keys()):\r\n\u001b[?2004l\r"]
[61.025076, "o", "\u001b[?2004h"]
[61.025135, "o", "> "]
[61.025541, "o", "            name_hash = hashlib.md5(name.encode('utf-8')).hexdigest()\r\n\u001b[?2004l\r"]
[61.025586, "o", "\u001b[?2004h> "]
[61.02604, "o", "            locations = ';'.join(sorted(set(valid_artifacts[name])))\r\n\u001b[?2004l\r"]
[61.026095, "o", "\u001b[?2004h> "]
[61.026502, "o", "            inv_f.write(f\"{name}|{name_hash}|{locations}\\n\")\r\n\u001b[?2004l\r"]
[61.026556, "o", "\u001b[?2004h> "]
[61.026611, "o", "    \r\n\u001b[?2004l\r"]
[61.026661, "o", "\u001b[?2004h> "]
[61.026834, "o", "    # Generate manifest.yaml\r\n\u001b[?2004l\r"]
[61.026891, "o", "\u001b[?2004h"]
[61.026942, "o", "> "]
[61.027315, "o", "    manifest_path = os.path.join(summary_dir, 'manifest.yaml')\r\n\u001b[?2004l\r"]
[61.027357, "o", "\u001b[?2004h"]
[61.027407, "o", "> "]
[61.027528, "o", "    manifest_data = {\r\n\u001b[?2004l\r"]
[61.027579, "o", "\u001b[?2004h> "]
[61.027913, "o", "        'total_artifacts': len(valid_artifacts),\r\n\u001b[?2004l\r"]
[61.02795, "o", "\u001b[?2004h"]
[61.028022, "o", "> "]
[61.028367, "o", "        'generated_at': datetime.utcnow().strftime('%Y-%m-%d')\r\n\u001b[?2004l\r"]
[61.028426, "o", "\u001b[?2004h> "]
[61.028477, "o", "    }\r\n\u001b[?2004l\r"]
[61.028525, "o", "\u001b[?2004h> "]
[61.02857, "o", "    \r\n\u001b[?2004l\r"]
[61.028618, "o", "\u001b[?2004h"]
[61.028672, "o", "> "]
[61.028845, "o", "    # Calculate inventory hash\r\n\u001b[?2004l\r"]
[61.028902, "o", "\u001b[?2004h"]
[61.028937, "o", "> "]
[61.029218, "o", "    with open(inventory_path, 'rb') as inv_f:\r\n\u001b[?2004l\r"]
[61.02928, "o", "\u001b[?2004h> "]
[61.029796, "o", "        manifest_data['inventory_hash'] = hashlib.sha256(inv_f.read()).hexdigest()\r\n\u001b[?2004l\r"]
[61.029851, "o", "\u001b[?2004h> "]
[61.029921, "o", "    \r\n\u001b[?2004l\r"]
[61.029974, "o", "\u001b[?2004h> "]
[61.030353, "o", "    with open(manifest_path, 'w', encoding='utf-8') as man_f:\r\n\u001b[?2004l\r"]
[61.030412, "o", "\u001b[?2004h"]
[61.030464, "o", "> "]
[61.030867, "o", "        yaml.safe_dump(manifest_data, man_f, default_flow_style=False)\r\n\u001b[?2004l\r"]
[61.030944, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[61.030994, "o", "\u001b[?2004h> "]
[61.031194, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r"]
[61.031243, "o", "\u001b[?2004h> "]
[61.031434, "o", "    base_path = '/home/user'\r\n\u001b[?2004l\r"]
[61.03148, "o", "\u001b[?2004h> "]
[61.031927, "o", "    deprecated_dir = os.path.join(base_path, 'mlops', 'deprecated')\r\n\u001b[?2004l\r"]
[61.031988, "o", "\u001b[?2004h> "]
[61.032375, "o", "    summary_dir = os.path.join(base_path, 'mlops', 'summary')\r\n\u001b[?2004l\r"]
[61.032427, "o", "\u001b[?2004h> "]
[61.03283, "o", "    process_artifacts(base_path, deprecated_dir, summary_dir)\r\n\u001b[?2004l\r"]
[61.032889, "o", "\u001b[?2004h> "]
[61.032941, "o", "EOF"]
[62.109033, "i", "chmod +x mlops/process_artifacts.py\n"]
[62.109174, "o", "chmod +x mlops/process_artifacts.py\r\n\u001b[?2004l\r"]
[62.109212, "o", "\u001b[?2004h"]
[62.109235, "o", "> "]
[93.009984, "i", "cat > mlops/create_test_data.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport codecs\nimport hashlib\nfrom datetime import datetime\n\ndef create_test_data(base_path):\n    \"\"\"Create comprehensive test dataset\"\"\"\n    experiments_dir = os.path.join(base_path, 'mlops', 'experiments')\n    deprecated_dir = os.path.join(base_path, 'mlops', 'deprecated')\n    \n    # Create sample experiments\n    experiments = [\n        ('exp_alpha', 'READY', 'model_alpha.pt'),\n        ('exp_beta', 'READY', 'model_beta.pt'),\n        ('exp_gamma', 'READY', 'model_gamma.pt'),\n        ('exp_delta', 'READY', 'model_delta.pt'),\n        ('exp_epsilon', 'READY', 'model_epsilon.pt')\n    ]\n    \n    # Create status override\n    override_dir = os.path.join(experiments_dir, 'exp_override')\n    os.makedirs(override_dir, exist_ok=True)\n    with open(os.path.join(override_dir, 'status_override.map'), 'w') as f:\n        f.write(\"model_gamma.pt=PROCESSED\\n\")\n        f.write(\"model_delta.pt=ERROR\\n\")\n    \n    # Create deprecated files\n    deprecated_fil"]
[93.010113, "i", "es = ['LEGACY_MODEL.OLD', 'OLD_ASSET.TXT', 'deprecated_file.bin']\n    for f in deprecated_files:\n        open(os.path.join(deprecated_dir, f), 'w').close()\n    \n    # Create test files with different encodings and content\n    for i, (exp_name, default_status, artifact) in enumerate(experiments):\n        exp_dir = os.path.join(experiments_dir, exp_name)\n        os.makedirs(exp_dir, exist_ok=True)\n        \n        # Create UTF-8 file\n        with open(os.path.join(exp_dir, 'artifacts.txt'), 'w', encoding='utf-8') as f:\n            f.write(f\"{artifact} | {default_status} | metadata1\\n\")\n            f.write(f\"  even_spaces.txt | READY | metadata2    \\n\")  # 4 trailing spaces - valid\n            f.write(f\"odd_spaces.log | READY | metadata3   \\n\")   # 3 trailing spaces - corrupted\n            f.write(f\"empty_line.log | READY |          \\n\")    # Empty line - valid\n            f.write(f\"corrupted_line.txt | READY | metadata4\\n\") # No trailing spaces - valid\n            f.write(f\"  indented_line.log | READY | metadat"]
[93.01013, "i", "a5   \\n\") # 2 trailing spaces - valid\n            \n            # Add additional artifacts for testing\n            f.write(f\"{artifact.replace('.pt', '_v2.pt')} | READY | metadata6\\n\")\n            f.write(f\"{artifact.replace('.pt', '_v3.pt')} | READY | metadata7\\n\")\n            \n            # Add deprecated artifact\n            deprecated_artifact = \"LEGACY_MODEL.OLD\"\n            f.write(f\"{deprecated_artifact} | READY | deprecated_meta\\n\")\n            \n            # Add artifact that should be overridden\n            if exp_name == 'exp_override':\n                f.write(f\"model_gamma.pt | READY | should_be_overridden\\n\")\n                f.write(f\"model_delta.pt | READY | should_be_overridden\\n\")\n            \n        # Create UTF-16LE file\n        utf16le_content = f\"utf16le_artifact.txt | READY | utf16le_meta\\n\".encode('utf-16-le')\n        with open(os.path.join(exp_dir, 'artifacts.log'), 'wb') as f:\n            f.write(utf16le_content)\n        \n        # Create Latin-1 file\n        latin1_content = f\"latin1_"]
[93.010196, "o", "cat > mlops/create_test_data.py << 'EOF'\r\n\u001b[?2004l\r"]
[93.010241, "i", "artifact.bin | READY | latin1_meta\\n\".encode('latin-1')\n        with open(os.path.join(exp_dir, 'artifacts.bin'), 'wb') as f:\n            f.write(latin1_content)\n    \n    print(\"Test data created successfully\")\n\nif __name__ == '__main__':\n    create_test_data('/home/user')\nEOF"]
[93.010286, "o", "\u001b[?2004h> "]
[93.010322, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[93.010335, "o", "\u001b[?2004h> "]
[93.010382, "o", "import os\r\n\u001b[?2004l\r\u001b[?2004h"]
[93.010395, "o", "> "]
[93.010447, "o", "import codecs\r\n\u001b[?2004l\r\u001b[?2004h"]
[93.010476, "o", "> "]
[93.010512, "o", "import hashlib\r\n\u001b[?2004l\r"]
[93.010556, "o", "\u001b[?2004h> "]
[93.010626, "o", "from datetime import datetime\r\n\u001b[?2004l\r"]
[93.010668, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h"]
[93.01068, "o", "> "]
[93.01077, "o", "def create_test_data(base_path):\r\n\u001b[?2004l\r"]
[93.010807, "o", "\u001b[?2004h"]
[93.010821, "o", "> "]
[93.010931, "o", "    \"\"\"Create comprehensive test dataset\"\"\"\r\n\u001b[?2004l\r"]
[93.010962, "o", "\u001b[?2004h> "]
[93.011135, "o", "    experiments_dir = os.path.join(base_path, 'mlops', 'experiments')\r\n\u001b[?2004l\r"]
[93.011188, "o", "\u001b[?2004h> "]
[93.011365, "o", "    deprecated_dir = os.path.join(base_path, 'mlops', 'deprecated')\r\n\u001b[?2004l\r"]
[93.011405, "o", "\u001b[?2004h> "]
[93.011432, "o", "    \r\n\u001b[?2004l\r"]
[93.011453, "o", "\u001b[?2004h> "]
[93.011534, "o", "    # Create sample experiments\r\n\u001b[?2004l\r"]
[93.011583, "o", "\u001b[?2004h> "]
[93.011619, "o", "    experiments = [\r\n\u001b[?2004l\r"]
[93.011644, "o", "\u001b[?2004h> "]
[93.011791, "o", "        ('exp_alpha', 'READY', 'model_alpha.pt'),\r\n\u001b[?2004l\r"]
[93.011842, "o", "\u001b[?2004h> "]
[93.011956, "o", "        ('exp_beta', 'READY', 'model_beta.pt'),\r\n\u001b[?2004l\r"]
[93.011991, "o", "\u001b[?2004h> "]
[93.01212, "o", "        ('exp_gamma', 'READY', 'model_gamma.pt'),\r\n\u001b[?2004l\r"]
[93.012155, "o", "\u001b[?2004h> "]
[93.012281, "o", "        ('exp_delta', 'READY', 'model_delta.pt'),\r\n\u001b[?2004l\r"]
[93.012318, "o", "\u001b[?2004h> "]
[93.01245, "o", "        ('exp_epsilon', 'READY', 'model_epsilon.pt')\r\n\u001b[?2004l\r"]
[93.01249, "o", "\u001b[?2004h> "]
[93.012502, "o", "    ]\r\n\u001b[?2004l\r"]
[93.012559, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[93.012608, "o", "\u001b[?2004h> "]
[93.012659, "o", "    # Create status override\r\n\u001b[?2004l\r"]
[93.012693, "o", "\u001b[?2004h> "]
[93.012852, "o", "    override_dir = os.path.join(experiments_dir, 'exp_override')\r\n\u001b[?2004l\r"]
[93.012906, "o", "\u001b[?2004h> "]
[93.013009, "o", "    os.makedirs(override_dir, exist_ok=True)\r\n\u001b[?2004l\r"]
[93.013045, "o", "\u001b[?2004h> "]
[93.013227, "o", "    with open(os.path.join(override_dir, 'status_override.map'), 'w') as f:\r\n\u001b[?2004l\r"]
[93.013257, "o", "\u001b[?2004h> "]
[93.013378, "o", "        f.write(\"model_gamma.pt=PROCESSED\\n\")\r\n\u001b[?2004l\r\u001b[?2004h"]
[93.01342, "o", "> "]
[93.013513, "o", "        f.write(\"model_delta.pt=ERROR\\n\")\r\n\u001b[?2004l\r"]
[93.01355, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[93.013593, "o", "\u001b[?2004h> "]
[93.013685, "o", "    # Create deprecated files\r\n\u001b[?2004l\r"]
[93.013726, "o", "\u001b[?2004h> "]
[93.013947, "o", "    deprecated_files = ['LEGACY_MODEL.OLD', 'OLD_ASSET.TXT', 'deprecated_file.bin']\r\n\u001b[?2004l\r"]
[93.013974, "o", "\u001b[?2004h"]
[93.013999, "o", "> "]
[93.014067, "o", "    for f in deprecated_files:\r\n\u001b[?2004l\r"]
[93.014092, "o", "\u001b[?2004h"]
[93.014123, "o", "> "]
[93.014261, "o", "        open(os.path.join(deprecated_dir, f), 'w').close()\r\n\u001b[?2004l\r"]
[93.014282, "o", "\u001b[?2004h> "]
[93.014303, "o", "    \r\n\u001b[?2004l\r"]
[93.014323, "o", "\u001b[?2004h"]
[93.014342, "o", "> "]
[93.014503, "o", "    # Create test files with different encodings and content\r\n\u001b[?2004l\r"]
[93.014521, "o", "\u001b[?2004h"]
[93.01454, "o", "> "]
[93.014734, "o", "    for i, (exp_name, default_status, artifact) in enumerate(experiments):\r\n\u001b[?2004l\r"]
[93.014751, "o", "\u001b[?2004h"]
[93.014771, "o", "> "]
[93.014927, "o", "        exp_dir = os.path.join(experiments_dir, exp_name)\r\n\u001b[?2004l\r"]
[93.014947, "o", "\u001b[?2004h> "]
[93.015074, "o", "        os.makedirs(exp_dir, exist_ok=True)\r\n\u001b[?2004l\r"]
[93.015098, "o", "\u001b[?2004h> "]
[93.01513, "o", "        \r\n\u001b[?2004l\r"]
[93.015153, "o", "\u001b[?2004h"]
[93.015174, "o", "> "]
[93.015252, "o", "        # Create UTF-8 file\r\n\u001b[?2004l\r"]
[93.015275, "o", "\u001b[?2004h> "]
[93.015505, "o", "        with open(os.path.join(exp_dir, 'artifacts.txt'), 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[93.015528, "o", "\u001b[?2004h> "]
[93.015709, "o", "            f.write(f\"{artifact} | {default_status} | metadata1\\n\")\r\n\u001b[?2004l\r"]
[93.015729, "o", "\u001b[?2004h"]
[93.015753, "o", "> "]
[93.016004, "o", "            f.write(f\"  even_spaces.txt | READY | metadata2    \\n\")  # 4 trailing spaces - valid\r\n\u001b[?2004l\r"]
[93.016056, "o", "\u001b[?2004h> "]
[93.016285, "o", "            f.write(f\"odd_spaces.log | READY | metadata3   \\n\")   # 3 trailing spaces - corrupted\r\n\u001b[?2004l\r"]
[93.01631, "o", "\u001b[?2004h> "]
[93.016577, "o", "            f.write(f\"empty_line.log | READY |          \\n\")    # Empty line - valid\r\n\u001b[?2004l\r"]
[93.016604, "o", "\u001b[?2004h> "]
[93.01691, "o", "            f.write(f\"corrupted_line.txt | READY | metadata4\\n\") # No trailing spaces - valid\r\n\u001b[?2004l\r"]
[93.016956, "o", "\u001b[?2004h> "]
[93.017226, "o", "            f.write(f\"  indented_line.log | READY | metadata5   \\n\") # 2 trailing spaces - valid\r\n\u001b[?2004l\r"]
[93.017264, "o", "\u001b[?2004h> "]
[93.017298, "o", "            \r\n\u001b[?2004l\r"]
[93.017325, "o", "\u001b[?2004h> "]
[93.017477, "o", "            # Add additional artifacts for testing\r\n\u001b[?2004l\r"]
[93.017527, "o", "\u001b[?2004h> "]
[93.017736, "o", "            f.write(f\"{artifact.replace('.pt', '_v2.pt')} | READY | metadata6\\n\")\r\n\u001b[?2004l\r"]
[93.017786, "o", "\u001b[?2004h> "]
[93.018026, "o", "            f.write(f\"{artifact.replace('.pt', '_v3.pt')} | READY | metadata7\\n\")\r\n\u001b[?2004l\r"]
[93.018061, "o", "\u001b[?2004h> "]
[93.01811, "o", "            \r\n\u001b[?2004l\r"]
[93.018146, "o", "\u001b[?2004h> "]
[93.018238, "o", "            # Add deprecated artifact\r\n\u001b[?2004l\r"]
[93.018275, "o", "\u001b[?2004h> "]
[93.018394, "o", "            deprecated_artifact = \"LEGACY_MODEL.OLD\"\r\n\u001b[?2004l\r"]
[93.018429, "o", "\u001b[?2004h> "]
[93.018598, "o", "            f.write(f\"{deprecated_artifact} | READY | deprecated_meta\\n\")\r\n\u001b[?2004l\r"]
[93.018631, "o", "\u001b[?2004h> "]
[93.018664, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[93.018808, "o", "            # Add artifact that should be overridden\r\n\u001b[?2004l\r"]
[93.018847, "o", "\u001b[?2004h> "]
[93.018942, "o", "            if exp_name == 'exp_override':\r\n\u001b[?2004l\r"]
[93.01897, "o", "\u001b[?2004h> "]
[93.019157, "o", "                f.write(f\"model_gamma.pt | READY | should_be_overridden\\n\")\r\n\u001b[?2004l\r"]
[93.019185, "o", "\u001b[?2004h> "]
[93.019375, "o", "                f.write(f\"model_delta.pt | READY | should_be_overridden\\n\")\r\n\u001b[?2004l\r"]
[93.019403, "o", "\u001b[?2004h> "]
[93.019429, "o", "            \r\n\u001b[?2004l\r"]
[93.019456, "o", "\u001b[?2004h> "]
[93.019538, "o", "        # Create UTF-16LE file\r\n\u001b[?2004l\r"]
[93.019566, "o", "\u001b[?2004h> "]
[93.0198, "o", "        utf16le_content = f\"utf16le_artifact.txt | READY | utf16le_meta\\n\".encode('utf-16-le')\r\n\u001b[?2004l\r"]
[93.019826, "o", "\u001b[?2004h> "]
[93.020004, "o", "        with open(os.path.join(exp_dir, 'artifacts.log'), 'wb') as f:\r\n\u001b[?2004l\r"]
[93.020031, "o", "\u001b[?2004h> "]
[93.020119, "o", "            f.write(utf16le_content)\r\n\u001b[?2004l\r"]
[93.020146, "o", "\u001b[?2004h> "]
[93.020173, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h"]
[93.0202, "o", "> "]
[93.020266, "o", "        # Create Latin-1 file\r\n\u001b[?2004l\r"]
[93.020292, "o", "\u001b[?2004h> "]
[93.020516, "o", "        latin1_content = f\"latin1_artifact.bin | READY | latin1_meta\\n\".encode('latin-1')\r\n\u001b[?2004l\r"]
[93.020542, "o", "\u001b[?2004h> "]
[93.020714, "o", "        with open(os.path.join(exp_dir, 'artifacts.bin'), 'wb') as f:\r\n\u001b[?2004l\r"]
[93.020741, "o", "\u001b[?2004h> "]
[93.020828, "o", "            f.write(latin1_content)\r\n\u001b[?2004l\r"]
[93.020856, "o", "\u001b[?2004h>     \r\n"]
[93.020895, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[93.021007, "o", "    print(\"Test data created successfully\")\r\n\u001b[?2004l\r"]
[93.021034, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[93.021062, "o", "\u001b[?2004h> "]
[93.021128, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r"]
[93.021155, "o", "\u001b[?2004h> "]
[93.021243, "o", "    create_test_data('/home/user')\r\n\u001b[?2004l\r"]
[93.021271, "o", "\u001b[?2004h> EOF"]
[93.402484, "i", "python3 mlops/create_test_data.py\n"]
[93.402696, "o", "python3 mlops/create_test_data.py\r\n"]
[93.402709, "o", "\u001b[?2004l\r"]
[93.402755, "o", "\u001b[?2004h"]
[93.402775, "o", "> "]
[106.903945, "i", "python3 mlops/create_test_data.py\n"]
[106.904074, "o", "python3 mlops/create_test_data.py\r\n"]
[106.904101, "o", "\u001b[?2004l\r"]
[106.904121, "o", "\u001b[?2004h> "]
[107.447632, "i", "ls -la mlops/experiments/\n"]
[107.447963, "o", "ls -la mlops/experiments/\r\n\u001b[?2004l\r"]
[107.448043, "o", "\u001b[?2004h> "]
[107.574528, "i", "ls -la mlops/experiments/exp_alpha/\n"]
[107.574707, "o", "ls -la mlops/experiments/exp_alpha/\r\n\u001b[?2004l\r"]
[107.574754, "o", "\u001b[?2004h> "]
[107.788282, "i", "head -5 mlops/experiments/exp_alpha/artifacts.txt\n"]
[107.788477, "o", "head -5 mlops/experiments/exp_alpha/artifacts.txt\r\n\u001b[?2004l\r"]
[107.788523, "o", "\u001b[?2004h> "]
[152.661426, "i", "mkdir -p mlops/experiments/exp_test1 mlops/experiments/exp_test2\n"]
[152.66182, "o", "mkdir -p mlops/experiments/exp_test1 mlops/experiments/exp_test2\r\n\u001b[?2004l\r"]
[152.661908, "o", "\u001b[?2004h> "]
[152.796484, "i", "cat > mlops/experiments/exp_test1/artifacts.txt << 'EOF'\nmodel_v1.pt | READY | version1\nmodel_v2.pt | READY | version2  \n  model_v3.pt | READY | version3\ninvalid_status.pt | PENDING | should_be_excluded\nEOF"]
[152.797022, "o", "cat > mlops/experiments/exp_test1/artifacts.txt << 'EOF'\r\n\u001b[?2004l\r\u001b[?2004h> model_v1.pt | READY | version1\r\n\u001b[?2004l\r\u001b[?2004h> model_v2.pt | READY | version2  \r\n\u001b[?2004l\r\u001b[?2004h> "]
[152.797094, "o", "  model_v3.pt | READY | version3\r\n\u001b[?2004l\r\u001b[?2004h> "]
[152.797199, "o", "invalid_status.pt | PENDING | should_be_excluded\r\n\u001b[?2004l\r"]
[152.797254, "o", "\u001b[?2004h> EOF"]
[153.067401, "i", "cat > mlops/experiments/exp_test2/artifacts.log << 'EOF'\nlegacy_model.old | READY | legacy\nutf16le_test.txt | READY | utf16le_test\nEOF"]
[153.067664, "o", "cat > mlops/experiments/exp_test2/artifacts.log << 'EOF'\r\n\u001b[?2004l\r"]
[153.067716, "o", "\u001b[?2004h> "]
[153.067831, "o", "legacy_model.old | READY | legacy\r\n\u001b[?2004l\r"]
[153.067885, "o", "\u001b[?2004h> "]
[153.068005, "o", "utf16le_test.txt | READY | utf16le_test\r\n\u001b[?2004l\r"]
[153.068047, "o", "\u001b[?2004h> EOF"]
[153.38234, "i", "echo -n \"legacy_model.old | READY | legacy\" > mlops/experiments/exp_test2/artifacts_utf16le.log\n"]
[153.382751, "o", "echo -n \"legacy_model.old | READY | legacy\" > mlops/experiments/exp_test2/artifacts_utf16le.log\r\n\u001b[?2004l\r"]
[153.382868, "o", "\u001b[?2004h> "]
[153.570349, "i", "iconv -f UTF-8 -t UTF-16LE mlops/experiments/exp_test2/artifacts_utf16le.log > mlops/experiments/exp_test2/artifacts_utf16le.log.tmp && mv mlops/experiments/exp_test2/artifacts_utf16le.log.tmp mlops/experiments/exp_test2/artifacts_utf16le.log\n"]
[153.571543, "o", "iconv -f UTF-8 -t UTF-16LE mlops/experiments/exp_test2/artifacts_utf16le.log > mlops/experiments/exp_test2/artifacts_utf16le.log.tmp && mv mlops/experiments/exp_test2/artifacts_utf16le.log.tmp mlops/experiments/exp_test2/artifacts_utf16le.log\r\n\u001b[?2004l\r"]
[153.571688, "o", "\u001b[?2004h> "]
[166.63645, "i", "ls -la mlops/experiments/\n"]
[166.636685, "o", "ls -la mlops/experiments/\r\n"]
[166.636715, "o", "\u001b[?2004l\r"]
[166.63675, "o", "\u001b[?2004h"]
[166.636774, "o", "> "]
[166.762423, "i", "ls -la mlops/experiments/exp_test1/\n"]
[166.762727, "o", "ls -la mlops/experiments/exp_test1/\r\n\u001b[?2004l\r"]
[166.762807, "o", "\u001b[?2004h> "]
[167.045094, "i", "cat mlops/experiments/exp_test1/artifacts.txt\n"]
[167.045282, "o", "cat mlops/experiments/exp_test1/artifacts.txt\r\n\u001b[?2004l\r"]
[167.045326, "o", "\u001b[?2004h> "]
[183.810734, "i", "pwd\n"]
[183.810912, "o", "pwd\r\n\u001b[?2004l\r"]
[183.810977, "o", "\u001b[?2004h> "]
[183.9079, "i", "mkdir -p mlops/experiments/exp_test1\n"]
[183.908059, "o", "mkdir -p mlops/experiments/exp_test1\r\n\u001b[?2004l\r"]
[183.908129, "o", "\u001b[?2004h> "]
[184.00518, "i", "echo \"model_v1.pt | READY | version1\" > mlops/experiments/exp_test1/artifacts.txt\n"]
[184.005972, "o", "echo \"model_v1.pt | READY | version1\" > mlops/experiments/exp_test1/artifacts.txt\r\n\u001b[?2004l\r"]
[184.006056, "o", "\u001b[?2004h"]
[184.006108, "o", "> "]
[184.216272, "i", "echo \"model_v2.pt | READY | version2\" >> mlops/experiments/exp_test1/artifacts.txt\n"]
[184.21689, "o", "echo \"model_v2.pt | READY | version2\" >> mlops/experiments/exp_test1/artifacts.txt\r\n"]
[184.216966, "o", "\u001b[?2004l\r"]
[184.217019, "o", "\u001b[?2004h> "]
[184.410215, "i", "ls -la mlops/experiments/exp_test1/\n"]
[184.410533, "o", "ls -la mlops/experiments/exp_test1/\r\n\u001b[?2004l\r"]
[184.410612, "o", "\u001b[?2004h"]
[184.410662, "o", "> "]
[195.600202, "i", "python3 mlops/process_artifacts.py\n"]
[195.600333, "o", "python3 mlops/process_artifacts.py\r\n\u001b[?2004l\r"]
[195.600389, "o", "\u001b[?2004h> "]
[196.523381, "i", "ls -la mlops/summary/\n"]
[196.5238, "o", "ls -la mlops/summary/\r\n\u001b[?2004l\r"]
[196.523967, "o", "\u001b[?2004h> "]
[208.284208, "i", "ls\n"]
[208.28434, "o", "ls\r\n\u001b[?2004l\r\u001b[?2004h"]
[208.284404, "o", "> "]
[208.429062, "i", "ls -la mlops/\n"]
[208.429163, "o", "ls -la mlops/\r\n\u001b[?2004l\r"]
[208.429193, "o", "\u001b[?2004h"]
[208.429214, "o", "> "]
[208.611153, "i", "find mlops/ -type f\n"]
[208.611299, "o", "find mlops/ -type f\r\n\u001b[?2004l\r"]
[208.611366, "o", "\u001b[?2004h"]
[208.611428, "o", "> "]
[220.35371, "i", "echo \"test content\" > test_file.txt\n"]
[220.353853, "o", "echo \"test content\" > test_file.txt\r\n\u001b[?2004l\r"]
[220.353909, "o", "\u001b[?2004h> "]
[220.483371, "i", "cat test_file.txt\n"]
[220.483466, "o", "cat test_file.txt\r\n\u001b[?2004l\r"]
[220.483504, "o", "\u001b[?2004h"]
[220.483525, "o", "> "]
[220.573741, "i", "ls -la test_file.txt\n"]
[220.573849, "o", "ls -la test_file.txt\r\n\u001b[?2004l\r"]
[220.57391, "o", "\u001b[?2004h"]
[220.57393, "o", "> "]
[234.400298, "i", "whoami\n"]
[234.400448, "o", "whoami\r\n\u001b[?2004l\r\u001b[?2004h> "]
[234.606351, "i", "date\n"]
[234.606487, "o", "date\r\n\u001b[?2004l\r"]
[234.606524, "o", "\u001b[?2004h"]
[234.606539, "o", "> "]
[234.711935, "i", "pwd\n"]
[234.712121, "o", "pwd\r\n\u001b[?2004l\r"]
[234.712206, "o", "\u001b[?2004h> "]
