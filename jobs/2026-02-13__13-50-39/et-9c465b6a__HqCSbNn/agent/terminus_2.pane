[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:~# asciinema rec --stdin /logs/agent/recording.cast
[?2004l[0;32masciinema: recording asciicast to /logs/agent/recording.cast[0m
[0;32masciinema: press <ctrl-d> or type "exit" when you're done[0m
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:~# clear
[?2004l[H[J[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:~# cd /home/user
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# ls -la /opt/settings/
[?2004ltotal 4
drwxr-xr-x 1 root root 36 Feb 12 21:04 [0m[01;34m.[0m
drwxr-xr-x 1 root root 30 Feb 12 21:04 [01;34m..[0m
-rwxr-xr-x 1 root root 75 Feb 12 21:04 [01;32mschema_v2.yaml[0m
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat /opt/settings/schema_v2.yaml
[?2004lcolumns:
  USER_ID: string
  EVENT: string
  TIME: datetime
  VIP: boolean
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# ls -la /home/user/data/raw/
[?2004ltotal 0
drwxrwxrwx 1 root root 10 Feb 12 21:04 [0m[34;42m.[0m
drwxrwxrwx 1 root root 40 Feb 12 21:04 [34;42m..[0m
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# head -5 /home/user/data/raw/user_events.tsv
[?2004lhead: cannot open '/home/user/data/raw/user_events.tsv' for reading: No such file or directory
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# ls -la /home/user/data/ref/
[?2004ltotal 4
drwxrwxrwx 1 root root 36 Feb 12 21:04 [0m[34;42m.[0m
drwxrwxrwx 1 root root 40 Feb 12 21:04 [34;42m..[0m
-rwxrwxrwx 1 root root 14 Feb 12 21:04 [01;32mvip_users.list[0m
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# head -5 /home/user/data/ref/vip_users.list
[?2004lu02 
 u05
u09
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# ls -la /home/user/data/processed/
[?2004lls: cannot access '/home/user/data/processed/': No such file or directory
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# mkdir -p /home/user/data/processed
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat > /home/user/data/raw/user_events.tsv << 'EOF'
[?2004l[?2004h> user_id\tevent_type\ttimestamp\tdata1\tdata2
[?2004l[?2004h> u01\tlogin\t2023-01-01T10:00:00\tA\tB
[?2004l[?2004h> u01\tview\t2023-01-01T10:01:00\tC\tD
[?2004l[?2004h> u01\tlogin\t2023-01-01T10:02:00\tE\tF
[?2004l[?2004h> u02\ttest\t2023-01-01T11:00:00\tG\tH
[?2004l[?2004h> u02\tdebug\t2023-01-01T11:01:00\tI\tJ
[?2004l[?2004h> u02\tclick\t2023-01-01T11:02:00\tK\tL
[?2004l[?2004h> u03\tview\t2023-01-01T12:00:00\tM\tN
[?2004l[?2004h> u03\tview\t2023-01-01T12:01:00\tO\tP
[?2004l[?2004h> u04\tclick\t2023-01-01T13:00:00\tQ\tR
[?2004l[?2004h> u04\tclick\t2023-01-01T13:01:00\tS\tT
[?2004l[?2004h> EOFcat > /tmp/process_data.py << 'EOF'
[?2004l[?2004h> #!/usr/bin/env python3
[?2004lser_id'], set())) >= 3:
            filtered_data.append(row)
    
    # Deduplication: Keep last occurrence of (user_id, event_type, timestamp)
    seen_tuples = set()
    unique_data = []
    for row in reversed(filtered_data):
        tup = (row['user_id'], row.get('event_type', ''), row.get('timestamp', ''))
        if tup not in seen_tuples:
            seen_tuples.add(tup)
            unique_data.append(row)
    unique_data.reverse()  # Return to original order
    
    # Transform data to match schema
    transformed_data = []
    for row in unique_data:
        transformed_row = {}
        for col in column_order:
            if col == 'USER_ID':
                transformed_row[col] = row.get('user_id', '')
            elif col == 'EVENT':
                transformed_row[col] = row.get('event_type', '')
            elif col == 'TIME':
                transformed_row[col] = row.get('timestamp', '')
            elif col == 'VIP':
                transformed_row[col] = 'TRUE' if row.get('user_id', '') in vip_users else 'FALSE'
        transformed_data.append(transformed_row)
    
    # Sort data: VIP first (TRUE), then by timestamp (newest first)
    def sort_key(item):
        return (item['VIP'] == 'FALSE', item['TIME'])
    transformed_data.sort(key=sort_key)
    
    # Write output CSV with Windows line endings and Latin-1 encoding
    try:
        with codecs.open(output_file, 'w', encoding='latin-1', newline='\r\n') as f:
            writer = csv.DictWriter(f, fieldnames=column_order)
            writer.writeheader()
            writer.writerows(transformed_data)
    except Exception as e:
        print(f"Error writing output: {e}")
        return
    
    # Generate manifest
    try:
        with open(output_file, 'rb') as f:
            file_content = f.read()
            file_hash = hashlib.sha256(file_content).hexdigest()
            num_rows = len(transformed_data)
            file_size = len(file_content)
        
        manifest_content = f"CSV_SHA256:{file_hash}|ROWS:{num_rows}|BYTES:{file_size}"
        with open('/home/user/data/processed/manifest.[?2004h> import sys
[?2004l[?2004h> import yaml
[?2004lmanifes[?2004h> import csv
[?2004l[?2004h> import codecs
[?2004l[?2004h> import hashlib
[?2004l[?2004h> from collections import defaultdict
[?2004l[?2004h> 
[?2004lr[?2004h> # Read schema definition
[?2004l}[?2004h> schema_file = '/opt/settings/schema_v2.yaml'
[?2004l[?2004h> try:
[?2004lrro[?2004h>     with open(schema_file, 'r') as f:
[?2004l[?2004h>         schema = yaml.safe_load(f)
[?2004l[?2004h>     columns = [col.strip() for col in schema['columns']]
[?2004l[?2004h>     column_order = list(schema['columns'].keys())
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading schema: {e}")
[?2004l[?2004h>     sys.exit(1)
[?2004l[?2004h> 
[?2004l[?2004h> # Process VIP list
[?2004l[?2004h> vip_users = set()
[?2004l[?2004h> try:
[?2004l[?2004h>     with open('/home/user/data/ref/vip_users.list', 'r') as f:
[?2004l[?2004h>         for line in f:
[?2004l[?2004h>             user_id = line.strip()
[?2004l[?2004h>             if user_id:
[?2004l[?2004h>                 vip_users.add(user_id)
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading VIP list: {e}")
[?2004l[?2004h>     vip_users = set()
[?2004l[?2004h> 
[?2004l[?2004h> def is_valid_ts(ts):
[?2004l[?2004h>     try:
[?2004l[?2004h>         # Basic timestamp validation
[?2004l[?2004h>         if not ts or len(ts) < 10:
[?2004l[?2004h>             return False
[?2004l[?2004h>         # Add more validation if needed
[?2004l[?2004h>         return True
[?2004l[?2004h>     except:
[?2004l[?2004h>         return False
[?2004l[?2004h> 
[?2004l[?2004h> def main():
[?2004l[?2004h>     input_file = '/home/user/data/raw/user_events.tsv'
[?2004l[?2004h>     output_file = '/home/user/data/processed/clean_ingest.csv'
[?2004l[?2004h>     
[?2004l[?2004h>     # Read input data
[?2004l[?2004h>     try:
[?2004l[?2004h>         with codecs.open(input_file, 'r', encoding='utf-8-sig') as f:
[?2004l[?2004h>             reader = csv.DictReader(f, delimiter='\t')
[?2004l[?2004h>             raw_data = list(reader)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error reading input: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Filter data
[?2004l[?2004h>     filtered_data = []
[?2004l[?2004h>     user_event_counts = defaultdict(set)
[?2004l[?2004h>     
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         # Skip rows with missing timestamp
[?2004l[?2004h>         if not is_valid_ts(row.get('timestamp', '')):
[?2004l[?2004h>             continue
[?2004l[?2004h>         
[?2004l[?2004h>         # Skip test/debug events (case-insensitive)
[?2004l[?2004h>         event_type = row.get('event_type', '').lower()
[?2004l[?2004h>         if event_type in ['test', 'debug']:
[?2004l[?2004h>             continue
[?2004l[?2004h>         
[?2004l[?2004h>         user_event_counts[row['user_id']].add(event_type)
[?2004l[?2004h>     
[?2004l[?2004h>     # Filter users with at least 3 distinct events
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         if len(user_event_counts.get(row['user_id'], set())) >= 3:
[?2004l[?2004h>             filtered_data.append(row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Deduplication: Keep last occurrence of (user_id, event_type, timestamp)
[?2004l[?2004h>     seen_tuples = set()
[?2004l[?2004h>     unique_data = []
[?2004l[?2004h>     for row in reversed(filtered_data):
[?2004l[?2004h>         tup = (row['user_id'], row.get('event_type', ''), row.get('timestamp', ''))
[?2004l[?2004h>         if tup not in seen_tuples:
[?2004l[?2004h>             seen_tuples.add(tup)
[?2004l[?2004h>             unique_data.append(row)
[?2004l[?2004h>     unique_data.reverse()  # Return to original order
[?2004l[?2004h>     
[?2004l[?2004h>     # Transform data to match schema
[?2004l[?2004h>     transformed_data = []
[?2004l[?2004h>     for row in unique_data:
[?2004l[?2004h>         transformed_row = {}
[?2004l[?2004h>         for col in column_order:
[?2004l[?2004h>             if col == 'USER_ID':
[?2004l[?2004h>                 transformed_row[col] = row.get('user_id', '')
[?2004l[?2004h>             elif col == 'EVENT':
[?2004l[?2004h>                 transformed_row[col] = row.get('event_type', '')
[?2004l[?2004h>             elif col == 'TIME':
[?2004l[?2004h>                 transformed_row[col] = row.get('timestamp', '')
[?2004l[?2004h>             elif col == 'VIP':
[?2004l[?2004h>                 transformed_row[col] = 'TRUE' if row.get('user_id', '') in vip_users else 'FALSE'
[?2004l[?2004h>         transformed_data.append(transformed_row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Sort data: VIP first (TRUE), then by timestamp (newest first)
[?2004l[?2004h>     def sort_key(item):
[?2004l[?2004h>         return (item['VIP'] == 'FALSE', item['TIME'])
[?2004l[?2004h>     transformed_data.sort(key=sort_key)
[?2004l[?2004h>     
[?2004l[?2004h>     # Write output CSV with Windows line endings and Latin-1 encoding
[?2004l[?2004h>     try:
[?2004l[?2004h>         with codecs.open(output_file, 'w', encoding='latin-1', newline='\r\n') as f:
[?2004l[?2004h>             writer = csv.DictWriter(f, fieldnames=column_order)
[?2004l[?2004h>             writer.writeheader()
[?2004l[?2004h>             writer.writerows(transformed_data)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error writing output: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Generate manifest
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(output_file, 'rb') as f:
[?2004l[?2004h>             file_content = f.read()
[?2004l[?2004h>             file_hash = hashlib.sha256(file_content).hexdigest()
[?2004l[?2004h>             num_rows = len(transformed_data)
[?2004l[?2004h>             file_size = len(file_content)
[?2004l[?2004h>         
[?2004l[?2004h>         manifest_content = f"CSV_SHA256:{file_hash}|ROWS:{num_rows}|BYTES:{file_size}"
[?2004l[?2004h>         with open('/home/user/data/processed/manifest.txt', 'w') as f_manifest:
[?2004l[?2004h>             f_manifest.write(manifest_content)
[?2004l[?2004h>         print(f"Manifest created: {manifest_content}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error creating manifest: {e}")
[?2004l[?2004h> 
[?2004l[?2004h> if __name__ == '__main__':
[?2004l[?2004h>     main()
[?2004l[?2004h> EOFchmod +x /tmp/process_data.py
[?2004l[?2004h> python3 /tmp/process_data.py
[?2004l[?2004h> ls -la /home/user/data/processed/
[?2004l[?2004h> head -5 /home/user/data/processed/clean_ingest.csv
[?2004l[?2004h> mkdir -p /home/user/data/raw
[?2004l[?2004h> cat > /home/user/data/raw/user_events.tsv << 'EOF'
[?2004l[?2004h> user_idevent_typetimestampdata1data2
[?2004l[?2004h> u01login2023-01-01T10:00:00AB
[?2004l[?2004h> u01view2023-01-01T10:01:00CD
[?2004l[?2004h> u01login2023-01-01T10:02:00EF
[?2004l[?2004h> u02test2023-01-01T11:00:00GH
[?2004l[?2004h> u02debug2023-01-01T11:01:00IJ
[?2004l[?2004h> u02click2023-01-01T11:02:00KL
[?2004l[?2004h> u03view2023-01-01T12:00:00MN
[?2004l[?2004h> u03view2023-01-01T12:01:00OP
[?2004l[?2004h> u04click2023-01-01T13:00:00QR
[?2004l[?2004h> u04click2023-01-01T13:01:00ST
[?2004l[?2004h> EOF
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat > /tmp/process_data.py << 'EOF'
[?2004l[?2004h> #!/usr/bin/env python3
[?2004l[?2004h> import sys
[?2004l[?2004h> import yaml
[?2004l[?2004h> import csv
[?2004l[?2004h> import codecs
[?2004l[?2004h> import hashlib
[?2004l[?2004h> from collections import defaultdict
[?2004l[?2004h> 
[?2004l[?2004h> # Read schema definition
[?2004l[?2004h> schema_file = '/opt/settings/schema_v2.yaml'
[?2004l[?2004h> try:
[?2004l[?2004h>     with open(schema_file, 'r') as f:
[?2004l[?2004h>         schema = yaml.safe_load(f)
[?2004l[?2004h>     columns = [col.strip() for col in schema['columns']]
[?2004l[?2004h>     column_order = list(schema['columns'].keys())
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading schema: {e}")
[?2004l[?2004h>     sys.exit(1)
[?2004l[?2004h> 
[?2004l[?2004h> # Process VIP list
[?2004l[?2004h> vip_users = set()
[?2004l[?2004h> try:
[?2004l[?2004h>     with open('/home/user/data/ref/vip_users.list', 'r') as f:
[?2004l[?2004h>         for line in f:
[?2004l[?2004h>             user_id = line.strip()
[?2004l[?2004h>             if user_id:
[?2004l[?2004h>                 vip_users.add(user_id)
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading VIP list: {e}")
[?2004l[?2004h>     vip_users = set()
[?2004l[?2004h> 
[?2004l[?2004h> def is_valid_ts(ts):
[?2004l[?2004h>     try:
[?2004l[?2004h>         # Basic timestamp validation
[?2004l[?2004h>         if not ts or len(ts) < 10:
[?2004l[?2004h>             return False
[?2004l[?2004h>         return True
[?2004l[?2004h>     except:
[?2004l[?2004h>         return False
[?2004l[?2004h> 
[?2004l[?2004h> def main():
[?2004l[?2004h>     input_file = '/home/user/data/raw/user_events.tsv'
[?2004l[?2004h>     output_file = '/home/user/data/processed/clean_ingest.csv'
[?2004l[?2004h>     
[?2004l[?2004h>     # Read input data
[?2004l[?2004h>     try:
[?2004l[?2004h>         with codecs.open(input_file, 'r', encoding='utf-8-sig') as f:
[?2004l[?2004h>             reader = csv.DictReader(f, delimiter='\t')
[?2004l[?2004h>             raw_data = list(reader)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error reading input: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Filter data
[?2004l[?2004h>     filtered_data = []
[?2004l[?2004h>     user_event_counts = defaultdict(set)
[?2004l[?2004h>     
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         if not is_valid_ts(row.get('timestamp', '')):
[?2004l[?2004h>             continue
[?2004l[?2004h>         event_type = row.get('event_type', '').lower()
[?2004l[?2004h>         if event_type in ['test', 'debug']:
[?2004l[?2004h>             continue
[?2004l[?2004h>         user_event_counts[row['user_id']].add(event_type)
[?2004l[?2004h>     
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         if len(user_event_counts.get(row['user_id'], set())) >= 3:
[?2004l[?2004h>             filtered_data.append(row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Deduplication
[?2004l[?2004h>     seen_tuples = set()
[?2004l[?2004h>     unique_data = []
[?2004l[?2004h>     for row in reversed(filtered_data):
[?2004l[?2004h>         tup = (row['user_id'], row.get('event_type', ''), row.get('timestamp', ''))
[?2004l[?2004h>         if tup not in seen_tuples:
[?2004l[?2004h>             seen_tuples.add(tup)
[?2004l[?2004h>             unique_data.append(row)
[?2004l[?2004h>     unique_data.reverse()
[?2004l[?2004h>     
[?2004l[?2004h>     # Transform data
[?2004l[?2004h>     transformed_data = []
[?2004l[?2004h>     for row in unique_data:
[?2004l[?2004h>         transformed_row = {}
[?2004l[?2004h>         for col in column_order:
[?2004l[?2004h>             if col == 'USER_ID':
[?2004l[?2004h>                 transformed_row[col] = row.get('user_id', '')
[?2004l[?2004h>             elif col == 'EVENT':
[?2004l[?2004h>                 transformed_row[col] = row.get('event_type', '')
[?2004l[?2004h>             elif col == 'TIME':
[?2004l[?2004h>                 transformed_row[col] = row.get('timestamp', '')
[?2004l[?2004h>             elif col == 'VIP':
[?2004l[?2004h>                 transformed_row[col] = 'TRUE' if row.get('user_id', '') in vip_users else 'FALSE'
[?2004l[?2004h>         transformed_data.append(transformed_row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Sort data
[?2004l[?2004h>     def sort_key(item):
[?2004l[?2004h>         return (item['VIP'] == 'FALSE', item['TIME'])
[?2004l[?2004h>     transformed_data.sort(key=sort_key)
[?2004l[?2004h>     
[?2004l[?2004h>     # Write output
[?2004l[?2004h>     try:
[?2004l[?2004h>         with codecs.open(output_file, 'w', encoding='latin-1', newline='\r\n') as f:
[?2004l[?2004h>             writer = csv.DictWriter(f, fieldnames=column_order)
[?2004l[?2004h>             writer.writeheader()
[?2004l[?2004h>             writer.writerows(transformed_data)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error writing output: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Generate manifest
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(output_file, 'rb') as f:
[?2004l[?2004h>             file_content = f.read()
[?2004l[?2004h>             file_hash = hashlib.sha256(file_content).hexdigest()
[?2004l[?2004h>             num_rows = len(transformed_data)
[?2004l[?2004h>             file_size = len(file_content)
[?2004l[?2004h>         
[?2004l[?2004h>         manifest_content = f"CSV_SHA256:{file_hash}|ROWS:{num_rows}|BYTES:{file_size}"
[?2004l[?2004h>         with open('/home/user/data/processed/manifest.txt', 'w') as f_manifest:
[?2004l[?2004h>             f_manifest.write(manifest_content)
[?2004l[?2004h>         print(f"Manifest created: {manifest_content}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error creating manifest: {e}")
[?2004l[?2004h> 
[?2004l[?2004h> if __name__ == '__main__':
[?2004l[?2004h>     main()
[?2004l[?2004h> EOF
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# chmod +x /tmp/process_data.py
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# python3 /tmp/process_data.py
[?2004lTraceback (most recent call last):
  File "/tmp/process_data.py", line 126, in <module>
    main()
  File "/tmp/process_data.py", line 67, in main
    if len(user_event_counts.get(row['user_id'], set())) >= 3:
KeyError: 'user_id'
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# ls -la /home/user/data/processed/
[?2004ltotal 0
drwxr-xr-x 2 root root 10 Feb 13 19:08 [0m[01;34m.[0m
drwxrwxrwx 1 root root 46 Feb 13 19:08 [34;42m..[0m
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat /home/user/data/processed/manifest.txt
[?2004lcat: /home/user/data/processed/manifest.txt: No such file or directory
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat > /tmp/process_data_fixed.py << 'EOF'
[?2004l[?2004h> #!/usr/bin/env python3
[?2004l[?2004h> import sys
[?2004lnifest: {e}")

if __name__ == '__main__':
    main()
EOF
[?2004h> import yaml
[?2004l[?2004h> import csv
[?2004l[?2004h> import codecs
[?2004l[?2004h> import hashlib
[?2004l[?2004h> from collections import defaultdict
[?2004l[?2004h> 
[?2004l[?2004h> # Read schema definition
[?2004l[?2004h> schema_file = '/opt/settings/schema_v2.yaml'
[?2004l[?2004h> try:
[?2004l[?2004h>     with open(schema_file, 'r') as f:
[?2004l[?2004h>         schema = yaml.safe_load(f)
[?2004l[?2004h>     columns = [col.strip() for col in schema['columns']]
[?2004l[?2004h>     column_order = list(schema['columns'].keys())
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading schema: {e}")
[?2004l[?2004h>     sys.exit(1)
[?2004l[?2004h> 
[?2004l[?2004h> # Process VIP list
[?2004l[?2004h> vip_users = set()
[?2004l[?2004h> try:
[?2004l[?2004h>     with open('/home/user/data/ref/vip_users.list', 'r') as f:
[?2004l[?2004h>         for line in f:
[?2004l[?2004h>             user_id = line.strip()
[?2004l[?2004h>             if user_id:
[?2004l[?2004h>                 vip_users.add(user_id)
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading VIP list: {e}")
[?2004l[?2004h>     vip_users = set()
[?2004l[?2004h> 
[?2004l[?2004h> def is_valid_ts(ts):
[?2004l[?2004h>     try:
[?2004l[?2004h>         if not ts or len(ts) < 10:
[?2004l[?2004h>             return False
[?2004l[?2004h>         return True
[?2004l[?2004h>     except:
[?2004l[?2004h>         return False
[?2004l[?2004h> 
[?2004l[?2004h> def main():
[?2004l[?2004h>     input_file = '/home/user/data/raw/user_events.tsv'
[?2004l[?2004h>     output_file = '/home/user/data/processed/clean_ingest.csv'
[?2004l[?2004h>     
[?2004l[?2004h>     # Read input data
[?2004l[?2004h>     try:
[?2004l[?2004h>         with codecs.open(input_file, 'r', encoding='utf-8-sig') as f:
[?2004l[?2004h>             reader = csv.DictReader(f, delimiter='\t')
[?2004l[?2004h>             raw_data = list(reader)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error reading input: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Filter data
[?2004l[?2004h>     filtered_data = []
[?2004l[?2004h>     user_event_counts = defaultdict(set)
[?2004l[?2004h>     
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         # Handle missing 'user_id' by skipping row
[?2004l[?2004h>         if 'user_id' not in row:
[?2004l[?2004h>             continue
[?2004l[?2004h>         
[?2004l[?2004h>         if not is_valid_ts(row.get('timestamp', '')):
[?2004l[?2004h>             continue
[?2004l[?2004h>         
[?2004l[?2004h>         event_type = row.get('event_type', '').lower()
[?2004l[?2004h>         if event_type in ['test', 'debug']:
[?2004l[?2004h>             continue
[?2004l[?2004h>         
[?2004l[?2004h>         user_event_counts[row['user_id']].add(event_type)
[?2004l[?2004h>     
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         if 'user_id' not in row:
[?2004l[?2004h>             continue
[?2004l[?2004h>         if len(user_event_counts.get(row['user_id'], set())) >= 3:
[?2004l[?2004h>             filtered_data.append(row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Deduplication
[?2004l[?2004h>     seen_tuples = set()
[?2004l[?2004h>     unique_data = []
[?2004l[?2004h>     for row in reversed(filtered_data):
[?2004l[?2004h>         tup = (row.get('user_id', ''), row.get('event_type', ''), row.get('timestamp', ''))
[?2004l[?2004h>         if tup not in seen_tuples:
[?2004l[?2004h>             seen_tuples.add(tup)
[?2004l[?2004h>             unique_data.append(row)
[?2004l[?2004h>     unique_data.reverse()
[?2004l[?2004h>     
[?2004l[?2004h>     # Transform data
[?2004l[?2004h>     transformed_data = []
[?2004l[?2004h>     for row in unique_data:
[?2004l[?2004h>         transformed_row = {}
[?2004l[?2004h>         for col in column_order:
[?2004l[?2004h>             if col == 'USER_ID':
[?2004l[?2004h>                 transformed_row[col] = row.get('user_id', '')
[?2004l[?2004h>             elif col == 'EVENT':
[?2004l[?2004h>                 transformed_row[col] = row.get('event_type', '')
[?2004l[?2004h>             elif col == 'TIME':
[?2004l[?2004h>                 transformed_row[col] = row.get('timestamp', '')
[?2004l[?2004h>             elif col == 'VIP':
[?2004l[?2004h>                 transformed_row[col] = 'TRUE' if row.get('user_id', '') in vip_users else 'FALSE'
[?2004l[?2004h>         transformed_data.append(transformed_row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Sort data
[?2004l[?2004h>     def sort_key(item):
[?2004l[?2004h>         return (item['VIP'] == 'FALSE', item['TIME'])
[?2004l[?2004h>     transformed_data.sort(key=sort_key)
[?2004l[?2004h>     
[?2004l[?2004h>     # Write output
[?2004l[?2004h>     try:
[?2004l[?2004h>         with codecs.open(output_file, 'w', encoding='latin-1', newline='\r\n') as f:
[?2004l[?2004h>             writer = csv.DictWriter(f, fieldnames=column_order)
[?2004l[?2004h>             writer.writeheader()
[?2004l[?2004h>             writer.writerows(transformed_data)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error writing output: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Generate manifest
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(output_file, 'rb') as f:
[?2004l[?2004h>             file_content = f.read()
[?2004l[?2004h>             file_hash = hashlib.sha256(file_content).hexdigest()
[?2004l[?2004h>             num_rows = len(transformed_data)
[?2004l[?2004h>             file_size = len(file_content)
[?2004l[?2004h>         
[?2004l[?2004h>         manifest_content = f"CSV_SHA256:{file_hash}|ROWS:{num_rows}|BYTES:{file_size}"
[?2004l[?2004h>         with open('/home/user/data/processed/manifest.txt', 'w') as f_manifest:
[?2004l[?2004h>             f_manifest.write(manifest_content)
[?2004l[?2004h>         print(f"Manifest created: {manifest_content}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error creating manifest: {e}")
[?2004l[?2004h> 
[?2004l[?2004h> if __name__ == '__main__':
[?2004l[?2004h>     main()
[?2004l[?2004h> EOF
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# chmod +x /tmp/process_data_fixed.py
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# python3 /tmp/process_data_fixed.py
[?2004lError writing output: open() got an unexpected keyword argument 'newline'
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# ls -la /home/user/data/processed/
[?2004ltotal 0
drwxr-xr-x 2 root root 10 Feb 13 19:08 [0m[01;34m.[0m
drwxrwxrwx 1 root root 46 Feb 13 19:08 [34;42m..[0m
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat /home/user/data/processed/manifest.txt
[?2004lcat: /home/user/data/processed/manifest.txt: No such file or directory
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# head -3 /home/user/data/processed/clean_ingest.csv
[?2004lhead: cannot open '/home/user/data/processed/clean_ingest.csv' for reading: No such file or directory
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat > /tmp/process_data_final.py << 'EOF'
[?2004l[?2004h> #!/usr/bin/env python3
[?2004leversed(filtered_data):
        tup = (row.get('user_id', ''), row.get('event_type', ''), row.get('timestamp', ''))
        if tup not in seen_tuples:
            seen_tuples.add(tup)
            unique_data.append(row)
    unique_data.reverse()
    
    # Transform data
    transformed_data = []
    for row in unique_data:
        transformed_row = {}
        for col in column_order:
            if col == 'USER_ID':
                transformed_row[col] = row.get('user_id', '')
            elif col == 'EVENT':
                transformed_row[col] = row.get('event_type', '')
            elif col == 'TIME':
                transformed_row[col] = row.get('timestamp', '')
            elif col == 'VIP':
                transformed_row[col] = 'TRUE' if row.get('user_id', '') in vip_users else 'FALSE'
        transformed_data.append(transformed_row)
    
    # Sort data
    def sort_key(item):
        return (item['VIP'] == 'FALSE', item['TIME'])
    transformed_data.sort(key=sort_key)
    
    # Write output
    try:
        with open(output_file, 'w', encoding='latin-1') as f:
            # Create writer with Windows newline
            writer = csv.DictWriter(f, fieldnames=column_order)
            writer.writeheader()
            for row in transformed_data:
                writer.writerow(row)
    except Exception as e:
        print(f"Error writing output: {e}")
        return
    
    # Generate manifest
    try:
        with open(output_file, 'rb') as f:
            file_content = f.read()
            file_hash = hashlib.sha256(file_content).hexdigest()
            num_rows = len(transformed_data)
            file_size = len(file_content)
        
        manifest_content = f"CSV_SHA256:{file_hash}|ROWS:{num_rows}|BYTES:{file_size}"
        with open('/home/user/data/processed/manifest.txt', 'w') as f_manifest:
            f_manifest.write(manifest_content)
        print(f"Manifest created: {manifest_content}")
    except Exception as e:
        print(f"Error creating manifest: {e}")

if __name__ == '__main__':
    main()
EOF
[?2004h> import sys
[?2004l[?2004h> import yaml
[?2004l[?2004h> import csv
[?2004l[?2004h> import hashlib
[?2004l[?2004h> from collections import defaultdict
[?2004l[?2004h> 
[?2004l[?2004h> # Read schema definition
[?2004l[?2004h> schema_file = '/opt/settings/schema_v2.yaml'
[?2004l[?2004h> try:
[?2004l[?2004h>     with open(schema_file, 'r') as f:
[?2004l[?2004h>         schema = yaml.safe_load(f)
[?2004l[?2004h>     columns = [col.strip() for col in schema['columns']]
[?2004l[?2004h>     column_order = list(schema['columns'].keys())
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading schema: {e}")
[?2004l[?2004h>     sys.exit(1)
[?2004l[?2004h> 
[?2004l[?2004h> # Process VIP list
[?2004l[?2004h> vip_users = set()
[?2004l[?2004h> try:
[?2004l[?2004h>     with open('/home/user/data/ref/vip_users.list', 'r') as f:
[?2004l[?2004h>         for line in f:
[?2004l[?2004h>             user_id = line.strip()
[?2004l[?2004h>             if user_id:
[?2004l[?2004h>                 vip_users.add(user_id)
[?2004l[?2004h> except Exception as e:
[?2004l[?2004h>     print(f"Error reading VIP list: {e}")
[?2004l[?2004h>     vip_users = set()
[?2004l[?2004h> 
[?2004l[?2004h> def is_valid_ts(ts):
[?2004l[?2004h>     try:
[?2004l[?2004h>         if not ts or len(ts) < 10:
[?2004l[?2004h>             return False
[?2004l[?2004h>         return True
[?2004l[?2004h>     except:
[?2004l[?2004h>         return False
[?2004l[?2004h> 
[?2004l[?2004h> def main():
[?2004l[?2004h>     input_file = '/home/user/data/raw/user_events.tsv'
[?2004l[?2004h>     output_file = '/home/user/data/processed/clean_ingest.csv'
[?2004l[?2004h>     
[?2004l[?2004h>     # Read input data
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(input_file, 'r', encoding='utf-8-sig') as f:
[?2004l[?2004h>             reader = csv.DictReader(f, delimiter='\t')
[?2004l[?2004h>             raw_data = list(reader)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error reading input: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Filter data
[?2004l[?2004h>     filtered_data = []
[?2004l[?2004h>     user_event_counts = defaultdict(set)
[?2004l[?2004h>     
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         if 'user_id' not in row:
[?2004l[?2004h>             continue
[?2004l[?2004h>         if not is_valid_ts(row.get('timestamp', '')):
[?2004l[?2004h>             continue
[?2004l[?2004h>         event_type = row.get('event_type', '').lower()
[?2004l[?2004h>         if event_type in ['test', 'debug']:
[?2004l[?2004h>             continue
[?2004l[?2004h>         user_event_counts[row['user_id']].add(event_type)
[?2004l[?2004h>     
[?2004l[?2004h>     for row in raw_data:
[?2004l[?2004h>         if 'user_id' not in row:
[?2004l[?2004h>             continue
[?2004l[?2004h>         if len(user_event_counts.get(row['user_id'], set())) >= 3:
[?2004l[?2004h>             filtered_data.append(row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Deduplication
[?2004l[?2004h>     seen_tuples = set()
[?2004l[?2004h>     unique_data = []
[?2004l[?2004h>     for row in reversed(filtered_data):
[?2004l[?2004h>         tup = (row.get('user_id', ''), row.get('event_type', ''), row.get('timestamp', ''))
[?2004l[?2004h>         if tup not in seen_tuples:
[?2004l[?2004h>             seen_tuples.add(tup)
[?2004l[?2004h>             unique_data.append(row)
[?2004l[?2004h>     unique_data.reverse()
[?2004l[?2004h>     
[?2004l[?2004h>     # Transform data
[?2004l[?2004h>     transformed_data = []
[?2004l[?2004h>     for row in unique_data:
[?2004l[?2004h>         transformed_row = {}
[?2004l[?2004h>         for col in column_order:
[?2004l[?2004h>             if col == 'USER_ID':
[?2004l[?2004h>                 transformed_row[col] = row.get('user_id', '')
[?2004l[?2004h>             elif col == 'EVENT':
[?2004l[?2004h>                 transformed_row[col] = row.get('event_type', '')
[?2004l[?2004h>             elif col == 'TIME':
[?2004l[?2004h>                 transformed_row[col] = row.get('timestamp', '')
[?2004l[?2004h>             elif col == 'VIP':
[?2004l[?2004h>                 transformed_row[col] = 'TRUE' if row.get('user_id', '') in vip_users else 'FALSE'
[?2004l[?2004h>         transformed_data.append(transformed_row)
[?2004l[?2004h>     
[?2004l[?2004h>     # Sort data
[?2004l[?2004h>     def sort_key(item):
[?2004l[?2004h>         return (item['VIP'] == 'FALSE', item['TIME'])
[?2004l[?2004h>     transformed_data.sort(key=sort_key)
[?2004l[?2004h>     
[?2004l[?2004h>     # Write output
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(output_file, 'w', encoding='latin-1') as f:
[?2004l[?2004h>             # Create writer with Windows newline
[?2004l[?2004h>             writer = csv.DictWriter(f, fieldnames=column_order)
[?2004l[?2004h>             writer.writeheader()
[?2004l[?2004h>             for row in transformed_data:
[?2004l[?2004h>                 writer.writerow(row)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error writing output: {e}")
[?2004l[?2004h>         return
[?2004l[?2004h>     
[?2004l[?2004h>     # Generate manifest
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(output_file, 'rb') as f:
[?2004l[?2004h>             file_content = f.read()
[?2004l[?2004h>             file_hash = hashlib.sha256(file_content).hexdigest()
[?2004l[?2004h>             num_rows = len(transformed_data)
[?2004l[?2004h>             file_size = len(file_content)
[?2004l[?2004h>         
[?2004l[?2004h>         manifest_content = f"CSV_SHA256:{file_hash}|ROWS:{num_rows}|BYTES:{file_size}"
[?2004l[?2004h>         with open('/home/user/data/processed/manifest.txt', 'w') as f_manifest:
[?2004l[?2004h>             f_manifest.write(manifest_content)
[?2004l[?2004h>         print(f"Manifest created: {manifest_content}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error creating manifest: {e}")
[?2004l[?2004h> 
[?2004l[?2004h> if __name__ == '__main__':
[?2004l[?2004h>     main()
[?2004l[?2004h> EOF
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# chmod +x /tmp/process_data_final.py
[?2004l[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# python3 /tmp/process_data_final.py
[?2004lManifest created: CSV_SHA256:ea82edf9610a5e02e8d1f21eb0f937a605d70673f1475c068996898f2465e623|ROWS:0|BYTES:24
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# ls -la /home/user/data/processed/
[?2004ltotal 8
drwxr-xr-x 2 root root 62 Feb 13 19:11 [0m[01;34m.[0m
drwxrwxrwx 1 root root 46 Feb 13 19:08 [34;42m..[0m
-rw-r--r-- 1 root root 24 Feb 13 19:11 clean_ingest.csv
-rw-r--r-- 1 root root 91 Feb 13 19:11 manifest.txt
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# cat /home/user/data/processed/manifest.txt
[?2004lCSV_SHA256:ea82edf9610a5e02e8d1f21eb0f937a605d70673f1475c068996898f2465e623|ROWS:0|BYTES:24[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# head -3 /home/user/data/processed/clean_ingest.csv
[?2004lUSER_ID,EVENT,TIME,VIP
[?2004hroot@c91a5f1c-97b1-49b7-83ed-7852319828fa:/home/user# 