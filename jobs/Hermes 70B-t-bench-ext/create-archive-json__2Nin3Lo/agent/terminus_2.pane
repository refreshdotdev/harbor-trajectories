[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# asciinema rec --stdin /logs/agent/recording.cast
[?2004lasciinema needs an ASCII or UTF-8 character encoding to run. Check the output of `locale` command.
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# clear
[?2004l[H[J[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# mkdir -p /home/user/experiments/ckpt_archive
[?2004l[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# cat > /home/user/experiments/manage_experiments.py << 'EOF'
[?2004l[?2004h> #!/usr/bin/env python3
[?2004l[?2004h> import os
[?2004l[?2004h> import re
[?2004l[?2004h> import json
[?2004l[?2004h> import hashlib
[?2004l[?2004h> import shutil
[?2004l[?2004h> import sys
[?2004l[?2004h> import csv
[?2004l[?2004h> import io
[?2004l[?2004h> import locale
[?2004l[?2004h> from pathlib import Path
[?2004l[?2004h> 
[?2004l[?2004h> # --- Configuration ---
[?2004l[?2004h> EXPERIMENTS_ROOT = Path('/home/user/experiments')
[?2004l[?2004h> CKPT_ARCHIVE_DIR = EXPERIMENTS_ROOT / 'ckpt_archive'
[?2004l[?2004h> INVENTORY_CSV_PATH = EXPERIMENTS_ROOT / 'artifact_inventory.csv'
[?2004l[?2004h> LOG_FILE_PATH = EXPERIMENTS_ROOT / 'cleanup.log'
[?2004l[?2004h> 
[?2004l[?2004h> # --- Global Counters & Data ---
[?2004l[?2004h> valid_ckpts_moved = 0
[?2004l[?2004h> all_loss_values = [] # For global average loss
[?2004l[?2004h> 
[?2004l[?2004h> # --- Helper Functions ---
[?2004l[?2004h> 
[?2004l[?2004h> def get_sha256(file_path):
[?2004l[?2004h>     """Calculate SHA256 hash of a file."""
[?2004l[?2004h>     sha256 = hashlib.sha256()
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(file_path, 'rb') as f:
[?2004l[?2004h>             while True:
[?2004l[?2004h>                 data = f.read(65536) # Read in 64k chunks
[?2004l[?2004h>                 if not data:
[?2004l[?2004h>                     break
[?2004l[?2004h>                 sha256.update(data)
[?2004l[?2004h>         return sha256.hexdigest()
[?2004l[?2004h>     except FileNotFoundError:
[?2004l[?2004h>         return 'ERROR_HASH_FILE_NOT_FOUND'
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         return f'ERROR_HASH_{type(e).__name__}'
[?2004l[?2004h> 
[?2004l[?2004h> def get_file_size_bytes(file_path):
[?2004l[?2004h>     try:
[?2004l[?2004h>         return os.path.getsize(file_path)
[?2004l[?2004h>     except FileNotFoundError:
[?2004l[?2004h>         return -1 # Indicate error
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         return f'ERROR_SIZE_{type(e).__name__}'
[?2004l[?2004h> 
[?2004l[?2004h> # --- Main Processing Logic ---
[?2004l[?2004h> def process_experiments():
[?2004l[?2004h>     global valid_ckpts_moved, all_loss_values
[?2004l[?2004h>     
[?2004l[?2004h>     if not EXPERIMENTS_ROOT.exists():
[?2004l[?2004h>         print("Error: Experiments root directory does not exist.")
[?2004l[?2004h>         sys.exit(1)
[?2004l[?2004h>     if not CKPT_ARCHIVE_DIR.exists():
[?2004l[?2004h>         CKPT_ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
[?2004l[?2004h>         print(f"Created archive directory: {CKPT_ARCHIVE_DIR}")
[?2004l[?2004h> 
[?2004l[?2004h>     all_files_for_inventory = []
[?2004l[?2004h>     
[?2004l[?2004h>     # Use locale.getlocale() to get current encoding, default to UTF-8 if not set
[?2004l[?2004h>     # For CRLF, we'll handle it during CSV writing.
[?2004l[?2004h>     
[?2004l[?2004h>     # 1. Checkpoint Archival and Global Average Loss Calculation
[?2004l[?2004h>     run_dirs = sorted([d for d in EXPERIMENTS_ROOT.iterdir() if d.is_dir() and re.match(r'^run_\d{4}-\d{2}-\d{2}$', d.name)])
[?2004l_for_[?2004h>     
[?2004l[?2004h>     print(f"Processing {len(run_dirs)} run directories...")
[?2004l[?2004h> 
[?2004l[?2004h>     for run_dir in run_dirs:
[?2004l[?2004h>         print(f"  Processing {run_dir}")
[?2004l[?2004h>         metrics_json_path = run_dir / 'metrics.json'
[?2004l[?2004h>         ckpt_files = list(run_dir.glob('*.ckpt'))
[?2004l[?2004h> 
[?2004l[?2004h>         # Read metrics.json for this run directory (for global average loss)
[?2004l[?2004h>         # This handles UTF-8, UTF-8-SIG, Latin-1 as per requirements.
[?2004l[?2004h>         loss_value_for_run = None
[?2004l[?2004h>         try:
[?2004l[?2004h>             with open(metrics_json_path, 'r', encoding='utf-8-sig') as f:
[?2004l[?2004h>                 content = f.read()
[?2004l[?2004h>             # Attempt decoding with UTF-8, then UTF-8-SIG, then Latin-1
[?2004l[?2004h>             decoded_content = None
[?2004l[?2004h>             try:
[?2004l[?2004h>                 decoded_content = json.loads(content)
[?2004l[?2004h>             except json.JSONDecodeError:
[?2004l[?2004h>                 try:
[?2004l[?2004h>                     # If utf-8-sig already removed BOM, re-adding for specific test might be complex.
[?2004l[?2004h>                     # Standard json.loads with utf-8-sig handles BOM. If it fails, it's not valid JSON.
[?2004l[?2004h>                     # The requirement is to try these encodings.
[?2004l[?2004h>                     # Re-read with latin1 as a fallback if utf-8-sig failed (which it shouldn't for BOM)
[?2004l[?2004h>                     # Let's re-read explicitly with latin-1 if the first pass failed.
[?2004l[?2004h>                     # This logic might need refinement for true sequential decoding attempts
[?2004l[?2004h>                     # For now, json.loads(content, encoding='utf-8-sig') is the primary attempt.
[?2004l[?2004h>                     # If that fails due to encoding, it will raise JSONDecodeError.
[?2004l[?2004h>                     # A more robust way:
[?2004l[?2004h>                     # 1. Try utf-8
[?2004l[?2004h>                     # 2. Try utf-8-sig (this usually works if BOM is present)
[?2004l[?2004h>                     # 3. Try latin-1
[?2004l[?2004h>                     
[?2004l[?2004h>                     # Simplified retry logic for this specific case:
[?2004l[?2004h>                     # If the first attempt with 'utf-8-sig' failed, let's try 'latin-1'
[?2004l[?2004h>                     # This is a bit of a hack; a proper `chardet` or structured try-except
[?2004l[?2004h>                     # for encodings would be better but is outside standard lib.
[?2004l[?2004h>                     # The problem states *how* to handle them (i.e. try them), not necessarily
[?2004l[?2004h>                     # the exact Python syntax for chained attempts if one fails mid-decode.
[?2004l[?2004h>                     # For now, we assume 'utf-8-sig' is a good default that handles BOM.
[?2004l[?2004h>                     # If it fails, it's likely not valid JSON or truly Latin-1.
[?2004l[?2004h>                     # Let's stick to 'utf-8-sig' for now and note if it's not enough.
[?2004l[?2004h>                     # print(f"  Warning: Could not parse {metrics_json_path} with utf-8-sig. Trying latin-1.")
[?2004l[?2004h>                     # with open(metrics_json_path, 'r', encoding='latin-1') as f_latin:
[?2004l[?2004h>                     #     content_latin = f_latin.read()
[?2004l[?2004h>                     #     decoded_content = json.loads(content_latin)
[?2004l[?2004h>                     pass # Sticking to utf-8-sig for simplicity for now
[?2004l[?2004h>                 except Exception as e:
[?2004l[?2004h>                     print(f"    Could not parse metrics.json: {e}")
[?2004l[?2004h>                     decoded_content = None
[?2004l[?2004h>             
[?2004l[?2004h>             if decoded_content and 'loss' in decoded_content and isinstance(decoded_content['loss'], (int, float)):
[?2004l[?2004h>                 loss_value_for_run = float(decoded_content['loss'])
[?2004l[?2004h>                 all_loss_values.append(loss_value_for_run)
[?2004l[?2004h>                 print(f"    Found loss: {loss_value_for_run} in {metrics_json_path.name}")
[?2004l[?2004h>             else:
[?2004l[?2004h>                 print(f"    No valid 'loss' float in {metrics_json_path.name} or file missing/unreadable")
[?2004l[?2004h>         except FileNotFoundError:
[?2004l[?2004h>             print(f"    {metrics_json_path.name} not found in {run_dir.name}")
[?2004l[?2004h>         except Exception as e:
[?2004l[?2004h>             print(f"    Error reading/parsing {metrics_json_path.name}: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>         # Process .ckpt files in this run directory
[?2004l[?2004h>         for ckpt_file in ckpt_files:
[?2004l[?2004h>             file_size = ckpt_file.stat().st_size
[?2004l[?2004h>             if file_size > 1024: # Valid checkpoint (size > 1KB)
[?2004l[?2004h>                 # Determine new name based on loss from this run's metrics.json
[?2004l[?2004h>                 loss_prefix = loss_value_for_run if loss_value_for_run is not None else 'unknown'
[?2004l[?2004h>                 new_filename = f"loss_{{:.4f}}__{{}}".format(loss_prefix, ckpt_file.name) 
[?2004l[?2004h>                 if loss_prefix == 'unknown':
[?2004l[?2004h>                     new_filename = f"loss_unknown__{ckpt_file.name}"
[?2004l[?2004h>                 
[?2004l[?2004h>                 new_ckpt_path = CKPT_ARCHIVE_DIR / new_filename
[?2004l[?2004h>                 
[?2004l[?2004h>                 if new_ckpt_path.exists():
[?2004l[?2004h>                     # Handle potential name collisions by appending a counter
[?2004l[?2004h>                     counter = 1
[?2004l[?2004h>                     base_name = new_filename
[?2004l[?2004h>                     while new_ckpt_path.exists():
[?2004l[?2004h>                         # Ensure not to re-append to an already appended name
[?2004l[?2004h>                         if counter > 1: 
[?2004l[?2004h>                             # Example: loss_0.1234__myckpt.ckpt.1 -> loss_0.1234__myckpt.ckpt.2
[?2004l[?2004h>                             # This logic is a bit complex if we want to be robust to existing numbers
[?2004l[?2004h>                             # For simplicity, let's make it loss_0.1234__myckpt.ckpt_copy<number>
[?2004l[?2004h>                             name_without_ext = ".".join(new_ckpt_path.name.split(".")[:-1]) 
[?2004l[?2004h>                             ext = new_ckpt_path.suffixes[-1] if new_ckpt_path.suffixes else ""
[?2004l[?2004h>                             # Find the base name without any _copy<number> suffix
[?2004l[?2004h>                             # This is complex, for now, just append _copyN to the original target name
[?2004l[?2004h>                             # This might not be perfect but avoids overwriting
[?2004l[?2004h>                             new_filename = f"{base_name}_copy{counter}"
[?2004l[?2004h>                             new_ckpt_path = CKPT_ARCHIVE_DIR / new_filename
[?2004l[?2004h>                         else:
[?2004l[?2004h>                             # First collision, append _copy1
[?2004l[?2004h>                             new_filename = f"{base_name}_copy{counter}"
[?2004l[?2004h>                             new_ckpt_path = CKPT_ARCHIVE_DIR / new_filename
[?2004l[?2004h>                         counter += 1
[?2004l[?2004h>                         
[?2004l[?2004h>                 
[?2004l[?2004h>                 try:
[?2004l[?2004h>                     shutil.move(str(ckpt_file), str(new_ckpt_path))
[?2004l[?2004h>                     valid_ckpts_moved += 1
[?2004l[?2004h>                     print(f"    Moved {ckpt_file.name} to {new_ckpt_path.name}")
[?2004l[?2004h>                 except Exception as e:
[?2004l[?2004h>                     print(f"    Error moving {ckpt_file.name} to {new_ckpt_path.name}: {e}")
[?2004l[?2004h>             else:
[?2004l[?2004h>                 print(f"    Skipping small/truncated .ckpt file: {ckpt_file.name} (size: {file_size} bytes)")
[?2004l[?2004h> 
[?2004l[?2004h>     # 2. Generate Artifact Inventory File
[?2004l[?2004h>     # Collect all files in the experiment tree (including ckpt_archive)
[?2004l[?2004h>     print("\nGenerating artifact inventory...")
[?2004l[?2004h>     for root, dirs, files in os.walk(EXPERIMENTS_ROOT):
[?2004l[?2004h>         for file in files:
[?2004l[?2004h>             file_path = Path(root) / file
[?2004l[?2004h>             if file_path == INVENTORY_CSV_PATH or file_path == LOG_FILE_PATH:
[?2004l[?2004h>                 continue 
[?2004l[?2004h>             relative_path = file_path.relative_to(EXPERIMENTS_ROOT).
[?2004l[?2004h>             item_size = get_file_size_bytes(file_path)
[?2004l[?2004h>             item_sha256 = get_sha256(file_path)
[?2004l[?2004h>             
[?2004l[?2004h>             all_files_for_inventory.append({
[?2004l[?2004h>                 'filepath': str(relative_path),
[?2004l[?2004h>                 'size_bytes': item_size,
[?2004l[?2004h>                 'sha256': item_sha256
[?2004l[?2004h>             })
[?2004l[?2004h>     
[?2004l[?2004h>     # Sort by filepath (case-insensitive)
[?2004l[?2004h>     all_files_for_inventory.sort(key=lambda x: x['filepath'].lower())
[?2004l[?2004h>     print(f"  Collected {len(all_files_for_inventory)} files for inventory.")
[?2004l[?2004h> 
[?2004l[?2004h>     # Write CSV with UTF-16LE, BOM, semicolon delimiter, CRLF line endings
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(INVENTORY_CSV_PATH, 'w', encoding='utf-16-le', newline='\r\n') as csvfile:
[?2004l[?2004h>             writer = csv.writer(csvfile, delimiter=';', quoting=csv.QUOTE_MINIMAL)
[?2004l[?2004h>             writer.writerow(['filepath', 'size_bytes', 'sha256']) # Header
[?2004l[?2004h>             for item in all_files_for_inventory:
[?2004l[?2004h>                 writer.writerow([item['filepath'], str(item['size_bytes']), item['sha256']])
[?2004l[?2004h>         print(f"  Inventory written to {INVENTORY_CSV_PATH}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"  Error writing inventory CSV: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>     # 3. Summary Log with Integrity Check
[?2004l[?2004h>     print("\nGenerating summary log...")
[?2004l[?2004h>     global_avg_loss = 0.0
[?2004l[?2004h>     if all_loss_values:
[?2004l[?2004h>         global_avg_loss = sum(all_loss_values) / len(all_loss_values)
[?2004l[?2004h>     
[?2004l[?2004h>     inventory_md5 = "ERROR_MD5_UNKNOWN"
[?2004l[?2004h>     try:
[?2004l[?2004h>         md5_hash = hashlib.md5()
[?2004l[?2004h>         with open(INVENTORY_CSV_PATH, 'rb') as f:
[?2004l[?2004h>             for byte_block in iter(lambda: f.read(4096), b""):
[?2004l[?2004h>                 md5_hash.update(byte_block)
[?2004l[?2004h>         inventory_md5 = md5_hash.hexdigest()
[?2004l[?2004h>     except FileNotFoundError:
[?2004l[?2004h>         print(f"  Warning: {INVENTORY_CSV_PATH} not found for MD5 calculation.")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"  Error calculating MD5 for inventory: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>     log_content = f"VALID_CKPTS_MOVED: {valid_ckpts_moved}\nGLOBAL_AVG_LOSS: {global_avg_loss:.4f}\nINVENTORY_MD5: {inventory_md5}"
[?2004l[?2004h>     
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(LOG_FILE_PATH, 'w') as f:
[?2004l[?2004h>             f.write(log_content)
[?2004l[?2004h>         os.chmod(LOG_FILE_PATH, 0o400) # Set permissions to r-------- (0400)
[?2004l[?2004h>         print(f"  Summary log written to {LOG_FILE_PATH}")
[?2004l[?2004h>         print(f"  Log content:\n{log_content}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"  Error writing summary log: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>     print("\nTask complete. Check /home/user/experiments/ for results.")
[?2004l[?2004h> 
[?2004l[?2004h> if __name__ == '__main__':
[?2004l[?2004h>     process_experiments()
[?2004l[?2004h> EOFchmod +x /home/user/experiments/manage_experiments.py
[?2004l[?2004h> python3 /home/user/experiments/manage_experiments.py
[?2004l[?2004h> ls -la /home/user/experiments/
[?2004l[?2004h> EOF
[?2004l[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# chmod +x /home/user/experiments/manage_experiments.py
[?2004l[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# python3 /home/user/experiments/manage_experiments.py
[?2004l  File "/home/user/experiments/manage_experiments.py", line 182
    relative_path = file_path.relative_to(EXPERIMENTS_ROOT).
                                                            ^
SyntaxError: invalid syntax
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# ls -la /home/user/experiments/
[?2004ltotal 12
drwxrwxrwx 1 user user    67 Feb 13 19:12 [0m[34;42m.[0m
drwxr-xr-x 1 user user    33 Feb 12 20:42 [01;34m..[0m
drwxr-xr-x 2 root root    10 Feb 13 19:12 [01;34mckpt_archive[0m
-rwxr-xr-x 1 root root 11538 Feb 13 19:12 [01;32mmanage_experiments.py[0m
drwxrwxrwx 1 user user    72 Feb 12 20:42 [34;42mrun_2023-08-01[0m
drwxrwxrwx 1 user user    44 Feb 12 20:42 [34;42mrun_2023-08-02[0m
drwxrwxrwx 1 user user    44 Feb 12 20:42 [34;42mrun_2023-08-03[0m
drwxrwxrwx 1 user user    56 Feb 12 20:42 [34;42mrun_2023-08-04[0m
drwxrwxrwx 1 user user    56 Feb 12 20:42 [34;42mrun_2023-08-05[0m
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# sed -i '182s/.$//' /home/user/experiments/manage_experiments.py
[?2004l[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# python3 /home/user/experiments/manage_experiments.py
[?2004l  File "/home/user/experiments/manage_experiments.py", line 242
    ls -la /home/user/experiments/
                                  ^
SyntaxError: invalid syntax
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# ls -la /home/user/experiments/
[?2004ltotal 12
drwxrwxrwx 1 user user    67 Feb 13 19:12 [0m[34;42m.[0m
drwxr-xr-x 1 user user    33 Feb 12 20:42 [01;34m..[0m
drwxr-xr-x 2 root root    10 Feb 13 19:12 [01;34mckpt_archive[0m
-rwxr-xr-x 1 root root 11537 Feb 13 19:12 [01;32mmanage_experiments.py[0m
drwxrwxrwx 1 user user    72 Feb 12 20:42 [34;42mrun_2023-08-01[0m
drwxrwxrwx 1 user user    44 Feb 12 20:42 [34;42mrun_2023-08-02[0m
drwxrwxrwx 1 user user    44 Feb 12 20:42 [34;42mrun_2023-08-03[0m
drwxrwxrwx 1 user user    56 Feb 12 20:42 [34;42mrun_2023-08-04[0m
drwxrwxrwx 1 user user    56 Feb 12 20:42 [34;42mrun_2023-08-05[0m
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# rm /home/user/experiments/manage_experiments.py
[?2004l[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# cat > /home/user/experiments/manage_experiments.py << 'EOF'
[?2004l[?2004h> #!/usr/bin/env python3
[?2004l[?2004h> import os
[?2004l[?2004h> import re
[?2004l[?2004h> import json
[?2004l[?2004h> import hashlib
[?2004l[?2004h> import shutil
[?2004l[?2004h> import sys
[?2004l[?2004h> import csv
[?2004l[?2004h> import io
[?2004l[?2004h> import locale
[?2004l[?2004h> from pathlib import Path
[?2004l[?2004h> 
[?2004l[?2004h> # --- Configuration ---
[?2004l[?2004h> EXPERIMENTS_ROOT = Path('/home/user/experiments')
[?2004l[?2004h> CKPT_ARCHIVE_DIR = EXPERIMENTS_ROOT / 'ckpt_archive'
[?2004l[?2004h> INVENTORY_CSV_PATH = EXPERIMENTS_ROOT / 'artifact_inventory.csv'
[?2004l[?2004h> LOG_FILE_PATH = EXPERIMENTS_ROOT / 'cleanup.log'
[?2004l[?2004h> 
[?2004l[?2004h> # --- Global Counters & Data ---
[?2004l[?2004h> valid_ckpts_moved = 0
[?2004l[?2004h> all_loss_values = [] # For global average loss
[?2004l[?2004h> 
[?2004l[?2004h> # --- Helper Functions ---
[?2004l[?2004h> 
[?2004l[?2004h> def get_sha256(file_path):
[?2004l[?2004h>     """Calculate SHA256 hash of a file."""
[?2004l[?2004h>     sha256 = hashlib.sha256()
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(file_path, 'rb') as f:
[?2004l[?2004h>             while True:
[?2004l[?2004h>                 data = f.read(65536) # Read in 64k chunks
[?2004l[?2004h>                 if not data:
[?2004l[?2004h>                     break
[?2004l[?2004h>                 sha256.update(data)
[?2004l[?2004h>         return sha256.hexdigest()
[?2004l[?2004h>     except FileNotFoundError:
[?2004l[?2004h>         return 'ERROR_HASH_FILE_NOT_FOUND'
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         return f'ERROR_HASH_{type(e).__name__}'
[?2004l[?2004h> 
[?2004l[?2004h> def get_file_size_bytes(file_path):
[?2004l[?2004h>     try:
[?2004l[?2004h>         return os.path.getsize(file_path)
[?2004l[?2004h>     except FileNotFoundError:
[?2004l[?2004h>         return -1 # Indicate error
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         return f'ERROR_SIZE_{type(e).__name__}'
[?2004l[?2004h> 
[?2004l[?2004h> # --- Main Processing Logic ---
[?2004l[?2004h> def process_experiments():
[?2004l[?2004h>     global valid_ckpts_moved, all_loss_values
[?2004l[?2004h>     
[?2004l[?2004h>     if not EXPERIMENTS_ROOT.exists():
[?2004l[?2004h>         print("Error: Experiments root directory does not exist.")
[?2004l[?2004h>         sys.exit(1)
[?2004l[?2004h>     if not CKPT_ARCHIVE_DIR.exists():
[?2004l[?2004h>         CKPT_ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)
[?2004l[?2004h>         print(f"Created archive directory: {CKPT_ARCHIVE_DIR}")
[?2004l[?2004h> 
[?2004l[?2004h>     all_files_for_inventory = []
[?2004l[?2004h>     
[?2004l[?2004h>     # 1. Checkpoint Archival and Global Average Loss Calculation
[?2004l[?2004h>     run_dirs = sorted([d for d in EXPERIMENTS_ROOT.iterdir() if d.is_dir() and re.match(r'^run_\\d{4}-\\d{2}-\\d{2}$', d.name)])
[?2004l[?2004h>     
[?2004l[?2004h>     print(f"Processing {len(run_dirs)} run directories...")
[?2004l[?2004h> 
[?2004l[?2004h>     for run_dir in run_dirs:
[?2004l[?2004h>         print(f"  Processing {run_dir}")
[?2004l[?2004h>         metrics_json_path = run_dir / 'metrics.json'
[?2004l[?2004h>         ckpt_files = list(run_dir.glob('*.ckpt'))
[?2004l[?2004h> 
[?2004l[?2004h>         # Read metrics.json for this run directory (for global average loss)
[?2004l[?2004h>         loss_value_for_run = None
[?2004l[?2004h>         try:
[?2004l[?2004h>             with open(metrics_json_path, 'r', encoding='utf-8-sig') as f:
[?2004l[?2004h>                 content = f.read()
[?2004l[?2004h>             decoded_content = None
[?2004l[?2004h>             try:
[?2004l[?2004h>                 decoded_content = json.loads(content)
[?2004l[?2004h>             except json.JSONDecodeError as e1:
[?2004l[?2004h>                 # Try Latin-1 as a fallback if UTF-8-SIG (which handles UTF-8 and BOM) fails
[?2004l[?2004h>                 print(f"    Warning: Could not parse {metrics_json_path} with utf-8-sig ({e1}). Trying latin-1.")
[?2004l[?2004h>                 try:
[?2004l[?2004h>                     with open(metrics_json_path, 'r', encoding='latin-1') as f_latin:
[?2004l[?2004h>                         content_latin = f_latin.read()
[?2004l[?2004h>                     decoded_content = json.loads(content_latin)
[?2004l[?2004h>                 except Exception as e2:
[?2004l[?2004h>                     print(f"    Error parsing {metrics_json_path} with latin-1: {e2}")
[?2004l[?2004h>                     decoded_content = None
[?2004l[?2004h>             
[?2004l[?2004h>             if decoded_content and 'loss' in decoded_content and isinstance(decoded_content['loss'], (int, float)):
[?2004l[?2004h>                 loss_value_for_run = float(decoded_content['loss'])
[?2004l[?2004h>                 all_loss_values.append(loss_value_for_run)
[?2004l[?2004h>                 print(f"    Found loss: {loss_value_for_run} in {metrics_json_path.name}")
[?2004l[?2004h>             else:
[?2004l[?2004h>                 print(f"    No valid 'loss' float in {metrics_json_path.name} or file missing/unreadable")
[?2004l[?2004h>         except FileNotFoundError:
[?2004l[?2004h>             print(f"    {metrics_json_path.name} not found in {run_dir.name}")
[?2004l[?2004h>         except Exception as e:
[?2004l[?2004h>             print(f"    Error reading/parsing {metrics_json_path.name}: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>         for ckpt_file in ckpt_files:
[?2004l[?2004h>             file_size = ckpt_file.stat().st_size
[?2004l[?2004h>             if file_size > 1024: # Valid checkpoint (size > 1KB)
[?2004l[?2004h>                 loss_prefix = loss_value_for_run if loss_value_for_run is not None else 'unknown'
[?2004l[?2004h>                 new_filename = f"loss_{{:.4f}}__{{}}".format(loss_prefix, ckpt_file.name) 
[?2004l[?2004h>                 if loss_prefix == 'unknown':
[?2004l[?2004h>                     new_filename = f"loss_unknown__{ckpt_file.name}"
[?2004l[?2004h>                 
[?2004l[?2004h>                 new_ckpt_path = CKPT_ARCHIVE_DIR / new_filename
[?2004l[?2004h>                 counter = 1
[?2004l[?2004h>                 while new_ckpt_path.exists():
[?2004l[?2004h>                     base_name = new_filename
[?2004l[?2004h>                     if counter > 1 and new_filename.endswith(f"_copy{counter-1}"):
[?2004l[?2004h>                          # Avoid appending copyN to a name that might already have it from a prior collision
[?2004l[?2004h>                         # This logic is tricky; for now, just append _copyN to the original target name
[?2004l[?2004h>                         pass # Use the simpler logic below
[?2004l[?2004h>                     new_filename = f"{base_name}_copy{counter}"
[?2004l[?2004h>                     new_ckpt_path = CKPT_ARCHIVE_DIR / new_filename
[?2004l[?2004h>                     counter += 1
[?2004l[?2004h>                 
[?2004l[?2004h>                 try:
[?2004l[?2004h>                     shutil.move(str(ckpt_file), str(new_ckpt_path))
[?2004l[?2004h>                     valid_ckpts_moved += 1
[?2004l[?2004h>                     print(f"    Moved {ckpt_file.name} to {new_ckpt_path.name}")
[?2004l[?2004h>                 except Exception as e:
[?2004l[?2004h>                     print(f"    Error moving {ckpt_file.name} to {new_ckpt_path.name}: {e}")
[?2004l[?2004h>             else:
[?2004l[?2004h>                 print(f"    Skipping small/truncated .ckpt file: {ckpt_file.name} (size: {file_size} bytes)")
[?2004l[?2004h> 
[?2004l[?2004h>     # 2. Generate Artifact Inventory File
[?2004l[?2004h>     print("\nGenerating artifact inventory...")
[?2004l[?2004h>     for root, dirs, files in os.walk(EXPERIMENTS_ROOT):
[?2004l[?2004h>         for file in files:
[?2004l[?2004h>             file_path = Path(root) / file
[?2004l[?2004h>             if file_path == INVENTORY_CSV_PATH or file_path == LOG_FILE_PATH:
[?2004l[?2004h>                 continue
[?2004l[?2004h>             relative_path = file_path.relative_to(EXPERIMENTS_ROOT)
[?2004l[?2004h>             item_size = get_file_size_bytes(file_path)
[?2004l[?2004h>             item_sha256 = get_sha256(file_path)
[?2004l[?2004h> 
[?2004l[?2004h>             all_files_for_inventory.append({
[?2004l[?2004h>                 'filepath': str(relative_path),
[?2004l[?2004h>                 'size_bytes': item_size,
[?2004l[?2004h>                 'sha256': item_sha256
[?2004l[?2004h>             })
[?2004l[?2004h>     
[?2004l[?2004h>     all_files_for_inventory.sort(key=lambda x: x['filepath'].lower())
[?2004l[?2004h>     print(f"  Collected {len(all_files_for_inventory)} files for inventory.")
[?2004l[?2004h> 
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(INVENTORY_CSV_PATH, 'w', encoding='utf-16-le', newline='\r\n') as csvfile:
[?2004l[?2004h>             writer = csv.writer(csvfile, delimiter=';', quoting=csv.QUOTE_MINIMAL)
[?2004l[?2004h>             writer.writerow(['filepath', 'size_bytes', 'sha256'])
[?2004l[?2004h>             for item in all_files_for_inventory:
[?2004l[?2004h>                 writer.writerow([item['filepath'], str(item['size_bytes']), item['sha256']])
[?2004l[?2004h>         print(f"  Inventory written to {INVENTORY_CSV_PATH}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"  Error writing inventory CSV: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>     # 3. Summary Log with Integrity Check
[?2004l[?2004h>     print("\nGenerating summary log...")
[?2004l[?2004h>     global_avg_loss = 0.0
[?2004l[?2004h>     if all_loss_values:
[?2004l[?2004h>         global_avg_loss = sum(all_loss_values) / len(all_loss_values)
[?2004l[?2004h>     
[?2004l[?2004h>     inventory_md5 = "ERROR_MD5_UNKNOWN"
[?2004l[?2004h>     try:
[?2004l[?2004h>         md5_hash = hashlib.md5()
[?2004l[?2004h>         with open(INVENTORY_CSV_PATH, 'rb') as f:
[?2004l[?2004h>             for byte_block in iter(lambda: f.read(4096), b""):
[?2004l[?2004h>                 md5_hash.update(byte_block)
[?2004l[?2004h>         inventory_md5 = md5_hash.hexdigest()
[?2004l[?2004h>     except FileNotFoundError:
[?2004l[?2004h>         print(f"  Warning: {INVENTORY_CSV_PATH} not found for MD5 calculation.")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"  Error calculating MD5 for inventory: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>     log_content = f"VALID_CKPTS_MOVED: {valid_ckpts_moved}\nGLOBAL_AVG_LOSS: {global_avg_loss:.4f}\nINVENTORY_MD5: {inventory_md5}"
[?2004l[?2004h>     
[?2004l[?2004h>     try:
[?2004l[?2004h>         with open(LOG_FILE_PATH, 'w') as f:
[?2004l[?2004h>             f.write(log_content)
[?2004l[?2004h>         os.chmod(LOG_FILE_PATH, 0o400) 
[?2004l[?2004h>         print(f"  Summary log written to {LOG_FILE_PATH}")
[?2004l[?2004h>         print(f"  Log content:\n{log_content}")
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"  Error writing summary log: {e}")
[?2004l[?2004h> 
[?2004l[?2004h>     print("\nTask complete. Check /home/user/experiments/ for results.")
[?2004l[?2004h> 
[?2004l[?2004h> if __name__ == '__main__':
[?2004l[?2004h>     process_experiments()
[?2004l[?2004h> EOF
[?2004l[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# chmod +x /home/user/experiments/manage_experiments.py
[?2004l[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# python3 /home/user/experiments/manage_experiments.py
[?2004lProcessing 0 run directories...

Generating artifact inventory...
  Collected 12 files for inventory.
  Inventory written to /home/user/experiments/artifact_inventory.csv

Generating summary log...
  Summary log written to /home/user/experiments/cleanup.log
  Log content:
VALID_CKPTS_MOVED: 0
GLOBAL_AVG_LOSS: 0.0000
INVENTORY_MD5: 2a58a6b527a0b14187584ea53272b723

Task complete. Check /home/user/experiments/ for results.
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# ls -la /home/user/experiments/
[?2004ltotal 16
drwxrwxrwx 1 user user  124 Feb 13 19:14 [0m[34;42m.[0m
drwxr-xr-x 1 user user   33 Feb 12 20:42 [01;34m..[0m
-rw-r--r-- 1 root root 2402 Feb 13 19:14 artifact_inventory.csv
drwxr-xr-x 2 root root   10 Feb 13 19:12 [01;34mckpt_archive[0m
-r-------- 1 root root   92 Feb 13 19:14 cleanup.log
-rwxr-xr-x 1 root root 8117 Feb 13 19:14 [01;32mmanage_experiments.py[0m
drwxrwxrwx 1 user user   72 Feb 12 20:42 [34;42mrun_2023-08-01[0m
drwxrwxrwx 1 user user   44 Feb 12 20:42 [34;42mrun_2023-08-02[0m
drwxrwxrwx 1 user user   44 Feb 12 20:42 [34;42mrun_2023-08-03[0m
drwxrwxrwx 1 user user   56 Feb 12 20:42 [34;42mrun_2023-08-04[0m
drwxrwxrwx 1 user user   56 Feb 12 20:42 [34;42mrun_2023-08-05[0m
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# head -n 5 /home/user/experiments/artifact_inventory.csv | od -t x1 -c -A n
[?2004l  66  00  69  00  6c  00  65  00  70  00  61  00  74  00  68  00
   f  \0   i  \0   l  \0   e  \0   p  \0   a  \0   t  \0   h  \0
  3b  00  73  00  69  00  7a  00  65  00  5f  00  62  00  79  00
   ;  \0   s  \0   i  \0   z  \0   e  \0   _  \0   b  \0   y  \0
  74  00  65  00  73  00  3b  00  73  00  68  00  61  00  32  00
   t  \0   e  \0   s  \0   ;  \0   s  \0   h  \0   a  \0   2  \0
  35  00  36  00  0d  00  0d  00  0a  00  6d  00  61  00  6e  00
   5  \0   6  \0  \r  \0  \r  \0  \n  \0   m  \0   a  \0   n  \0
  61  00  67  00  65  00  5f  00  65  00  78  00  70  00  65  00
   a  \0   g  \0   e  \0   _  \0   e  \0   x  \0   p  \0   e  \0
  72  00  69  00  6d  00  65  00  6e  00  74  00  73  00  2e  00
   r  \0   i  \0   m  \0   e  \0   n  \0   t  \0   s  \0   .  \0
  70  00  79  00  3b  00  38  00  31  00  31  00  37  00  3b  00
   p  \0   y  \0   ;  \0   8  \0   1  \0   1  \0   7  \0   ;  \0
  63  00  38  00  34  00  32  00  63  00  31  00  61  00  65  00
   c  \0   8  \0   4  \0   2  \0   c  \0   1  \0   a  \0   e  \0
  66  00  36  00  36  00  33  00  33  00  63  00  30  00  36  00
   f  \0   6  \0   6  \0   3  \0   3  \0   c  \0   0  \0   6  \0
  33  00  65  00  63  00  63  00  37  00  39  00  66  00  61  00
   3  \0   e  \0   c  \0   c  \0   7  \0   9  \0   f  \0   a  \0
  31  00  35  00  66  00  63  00  32  00  63  00  33  00  64  00
   1  \0   5  \0   f  \0   c  \0   2  \0   c  \0   3  \0   d  \0
  39  00  35  00  61  00  36  00  31  00  63  00  61  00  31  00
   9  \0   5  \0   a  \0   6  \0   1  \0   c  \0   a  \0   1  \0
  33  00  66  00  62  00  37  00  62  00  30  00  35  00  31  00
   3  \0   f  \0   b  \0   7  \0   b  \0   0  \0   5  \0   1  \0
  65  00  66  00  35  00  62  00  32  00  65  00  38  00  38  00
   e  \0   f  \0   5  \0   b  \0   2  \0   e  \0   8  \0   8  \0
  62  00  38  00  35  00  65  00  61  00  63  00  30  00  62  00
   b  \0   8  \0   5  \0   e  \0   a  \0   c  \0   0  \0   b  \0
  0d  00  0d  00  0a  00  72  00  75  00  6e  00  5f  00  32  00
  \r  \0  \r  \0  \n  \0   r  \0   u  \0   n  \0   _  \0   2  \0
  30  00  32  00  33  00  2d  00  30  00  38  00  2d  00  30  00
   0  \0   2  \0   3  \0   -  \0   0  \0   8  \0   -  \0   0  \0
  31  00  2f  00  6c  00  6f  00  67  00  73  00  2f  00  74  00
   1  \0   /  \0   l  \0   o  \0   g  \0   s  \0   /  \0   t  \0
  72  00  61  00  69  00  6e  00  2e  00  6c  00  6f  00  67  00
   r  \0   a  \0   i  \0   n  \0   .  \0   l  \0   o  \0   g  \0
  3b  00  39  00  3b  00  34  00  33  00  62  00  63  00  63  00
   ;  \0   9  \0   ;  \0   4  \0   3  \0   b  \0   c  \0   c  \0
  38  00  36  00  61  00  64  00  66  00  30  00  35  00  30  00
   8  \0   6  \0   a  \0   d  \0   f  \0   0  \0   5  \0   0  \0
  61  00  35  00  61  00  34  00  65  00  37  00  33  00  64  00
   a  \0   5  \0   a  \0   4  \0   e  \0   7  \0   3  \0   d  \0
  61  00  35  00  35  00  32  00  38  00  35  00  39  00  65  00
   a  \0   5  \0   5  \0   2  \0   8  \0   5  \0   9  \0   e  \0
  38  00  66  00  32  00  39  00  66  00  62  00  39  00  31  00
   8  \0   f  \0   2  \0   9  \0   f  \0   b  \0   9  \0   1  \0
  32  00  37  00  33  00  36  00  38  00  38  00  33  00  32  00
   2  \0   7  \0   3  \0   6  \0   8  \0   8  \0   3  \0   2  \0
  33  00  38  00  66  00  39  00  39  00  34  00  65  00  38  00
   3  \0   8  \0   f  \0   9  \0   9  \0   4  \0   e  \0   8  \0
  30  00  32  00  35  00  31  00  31  00  66  00  62  00  34  00
   0  \0   2  \0   5  \0   1  \0   1  \0   f  \0   b  \0   4  \0
  35  00  34  00  33  00  0d  00  0d  00  0a  00  72  00  75  00
   5  \0   4  \0   3  \0  \r  \0  \r  \0  \n  \0   r  \0   u  \0
  6e  00  5f  00  32  00  30  00  32  00  33  00  2d  00  30  00
   n  \0   _  \0   2  \0   0  \0   2  \0   3  \0   -  \0   0  \0
  38  00  2d  00  30  00  31  00  2f  00  6d  00  65  00  74  00
   8  \0   -  \0   0  \0   1  \0   /  \0   m  \0   e  \0   t  \0
  72  00  69  00  63  00  73  00  2e  00  6a  00  73  00  6f  00
   r  \0   i  \0   c  \0   s  \0   .  \0   j  \0   s  \0   o  \0
  6e  00  3b  00  32  00  35  00  3b  00  65  00  34  00  31  00
   n  \0   ;  \0   2  \0   5  \0   ;  \0   e  \0   4  \0   1  \0
  65  00  32  00  63  00  37  00  33  00  64  00  37  00  36  00
   e  \0   2  \0   c  \0   7  \0   3  \0   d  \0   7  \0   6  \0
  32  00  61  00  31  00  33  00  32  00  34  00  36  00  37  00
   2  \0   a  \0   1  \0   3  \0   2  \0   4  \0   6  \0   7  \0
  63  00  39  00  30  00  66  00  64  00  32  00  31  00  34  00
   c  \0   9  \0   0  \0   f  \0   d  \0   2  \0   1  \0   4  \0
  65  00  66  00  34  00  62  00  64  00  37  00  31  00  61  00
   e  \0   f  \0   4  \0   b  \0   d  \0   7  \0   1  \0   a  \0
  39  00  66  00  38  00  36  00  32  00  35  00  34  00  33  00
   9  \0   f  \0   8  \0   6  \0   2  \0   5  \0   4  \0   3  \0
  62  00  32  00  36  00  30  00  32  00  39  00  36  00  36  00
   b  \0   2  \0   6  \0   0  \0   2  \0   9  \0   6  \0   6  \0
  63  00  65  00  61  00  31  00  36  00  31  00  37  00  35  00
   c  \0   e  \0   a  \0   1  \0   6  \0   1  \0   7  \0   5  \0
  33  00  62  00  34  00  38  00  64  00  0d  00  0d  00  0a  00
   3  \0   b  \0   4  \0   8  \0   d  \0  \r  \0  \r  \0  \n  \0
  72  00  75  00  6e  00  5f  00  32  00  30  00  32  00  33  00
   r  \0   u  \0   n  \0   _  \0   2  \0   0  \0   2  \0   3  \0
  2d  00  30  00  38  00  2d  00  30  00  31  00  2f  00  6d  00
   -  \0   0  \0   8  \0   -  \0   0  \0   1  \0   /  \0   m  \0
  6f  00  64  00  65  00  6c  00  2e  00  63  00  6b  00  70  00
   o  \0   d  \0   e  \0   l  \0   .  \0   c  \0   k  \0   p  \0
  74  00  3b  00  32  00  30  00  34  00  38  00  3b  00  33  00
   t  \0   ;  \0   2  \0   0  \0   4  \0   8  \0   ;  \0   3  \0
  61  00  33  00  34  00  63  00  38  00  64  00  63  00  34  00
   a  \0   3  \0   4  \0   c  \0   8  \0   d  \0   c  \0   4  \0
  61  00  65  00  63  00  31  00  35  00  35  00  34  00  63  00
   a  \0   e  \0   c  \0   1  \0   5  \0   5  \0   4  \0   c  \0
  30  00  34  00  65  00  30  00  64  00  30  00  65  00  36  00
   0  \0   4  \0   e  \0   0  \0   d  \0   0  \0   e  \0   6  \0
  31  00  31  00  37  00  39  00  64  00  30  00  38  00  33  00
   1  \0   1  \0   7  \0   9  \0   d  \0   0  \0   8  \0   3  \0
  36  00  32  00  62  00  33  00  32  00  39  00  30  00  32  00
   6  \0   2  \0   b  \0   3  \0   2  \0   9  \0   0  \0   2  \0
  39  00  64  00  62  00  34  00  36  00  33  00  32  00  66  00
   9  \0   d  \0   b  \0   4  \0   6  \0   3  \0   2  \0   f  \0
  35  00  66  00  30  00  38  00  36  00  63  00  33  00  37  00
   5  \0   f  \0   0  \0   8  \0   6  \0   c  \0   3  \0   7  \0
  62  00  65  00  37  00  34  00  63  00  61  00  61  00  0d  00
   b  \0   e  \0   7  \0   4  \0   c  \0   a  \0   a  \0  \r  \0
  0d  00  0a
  \r  \0  \n
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# cat /home/user/experiments/cleanup.log
[?2004lVALID_CKPTS_MOVED: 0
GLOBAL_AVG_LOSS: 0.0000
INVENTORY_MD5: 2a58a6b527a0b14187584ea53272b723[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# ls -la /home/user/experiments/ckpt_archive/
[?2004ltotal 0
drwxr-xr-x 2 root root  10 Feb 13 19:12 [0m[01;34m.[0m
drwxrwxrwx 1 user user 124 Feb 13 19:14 [34;42m..[0m
[?2004hroot@4c3e0f60-19ab-4fc9-abf2-cb2cad73f30f:~# 