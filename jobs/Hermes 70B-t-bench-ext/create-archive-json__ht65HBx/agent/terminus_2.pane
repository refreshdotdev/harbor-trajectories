[?2004hroot@01fa814d-51fb-4f1e-88a8-a6652736186b:~# asciinema rec --stdin /logs/agent/recording.cast
[?2004lasciinema needs an ASCII or UTF-8 character encoding to run. Check the output of `locale` command.
[?2004hroot@01fa814d-51fb-4f1e-88a8-a6652736186b:~# clear
[?2004l[H[J[?2004hroot@01fa814d-51fb-4f1e-88a8-a6652736186b:~# mkdir -p /tmp
[?2004l[?2004hroot@01fa814d-51fb-4f1e-88a8-a6652736186b:~# cat > /tmp/archive_script.py << 'EOF'
[?2004l[?2004h> #!/usr/bin/env python3
[?2004l[?2004h> import os
[?2004l[?2004h> import shutil
[?2004l[?2004h> import json
[?2004l[?2004h> import hashlib
[?2004l[?2004h> import csv
[?2004l[?2004h> import sys
[?2004l[?2004h> import re
[?2004l[?2004h> from pathlib import Path
[?2004l[?2004h> import locale
[?2004l[?2004h> import codecs
[?2004l[?2004h> 
[?2004l[?2004h> def get_loss_from_metrics_json(metrics_path):
[?2004l[?2004h>     if not metrics_path.exists():
[?2004l[?2004h>         return None
[?2004l[?2004h>     
[?2004l[?2004h>     # Try common encodings
[?2004l[?2004h>     encodings_to_try = ['utf-8-sig', 'utf-8', 'iso-8859-1']
[?2004l[?2004h>     loss_value = None
[?2004l[?2004h>     
[?2004l[?2004h>     for enc in encodings_to_try:
[?2004l[?2004h>         try:
[?2004l[?2004h>             with codecs.open(metrics_path, 'r', encoding=enc) as f:
[?2004l[?2004h>                 content = f.read()
[?2004l[?2004h>             data = json.loads(content)
[?2004l[?2004h>             if 'loss' in data:
[?2004l[?2004h>                 loss_val = data['loss']
[?2004l[?2004h>                 # Try to convert to float
[?2004l[?2004h>                 try:
[?2004l[?2004h>                     loss_value = float(loss_val)
[?2004l[?2004h>                     return loss_value
[?2004l[?2004h>                 except (TypeError, ValueError):
[?2004l[?2004h>                     # Loss value exists but isn't a number we can use
[?2004l[?2004h>                     return None 
[?2004l[?2004h>         except json.JSONDecodeError:
[?2004l[?2004h>             continue
[?2004l[?2004h>         except UnicodeDecodeError:
[?2004l[?2004h>             continue
[?2004l[?2004h>     
[?2004l[?2004h>     return None
[?2004l[?2004h> 
[?2004l[?2004h> def get_md5_checksum(file_path):
[?2004l[?2004h>     hash_md5 = hashlib.md5()
[?2004l[?2004h>     with open(file_path, "rb") as f:
[?2004l[?2004h>         for chunk in iter(lambda: f.read(4096), b""):
[?2004l[?2004h>             hash_md5.update(chunk)
[?2004l[?2004h>     return hash_md5.hexdigest()
[?2004l[?2004h> 
[?2004l[?2004h> def main():
[?2004l[?2004h>     experiments_base_dir = Path('/home/user/experiments')
[?2004l[?2004h>     ckpt_archive_dir = experiments_base_dir / 'ckpt_archive'
[?2004l[?2004h>     
[?2004l[?2004h>     ckpt_archive_dir.mkdir(parents=True, exist_ok=True)
[?2004l[?2004h>     
[?2004l[?2004h>     total_moved_ckpt_count = 0
[?2004l[?2004h>     all_readable_losses = []
[?2004l[?2004h>     
[?2004l[?2004h>     # Phase 1: Archival and Loss Collection
[?2004l[?2004h>     for run_dir_path in experiments_base_dir.iterdir():
[?2004l[?2004h>         if run_dir_path.is_dir() and run_dir_path.name.startswith('run_'):
[?2004l[?2004h>             for ckpt_file_path in run_dir_path.glob('*.ckpt'):
[?2004l[?2004h>                 metrics_file_path = run_dir_path / 'metrics.json'
[?2004l[?2004h>                 
[?2004l[?2004h>                 # Validate checkpoint size
[?2004l[?2004h>                 if ckpt_file_path.stat().st_size <= 1024:
[?2004l[?2004h>                     # Skip corrupted/truncated files
[?2004l[?2004h>                     continue
[?2004l[?2004h>                 
[?2004l[?2004h>                 # Extract loss for renaming and averaging
[?2004l[?2004h>                 loss_value = get_loss_from_metrics_json(metrics_file_path)
[?2004l[?2004h>                 if loss_value is not None:
[?2004l[?2004h>                     all_readable_losses.append(loss_value)
[?2004l[?2004h>                     prefix = f"loss_{loss_value:.4f}"
[?2004l[?2004h>                 else:
[?2004l[?2004h>                     prefix = "loss_unknown"
[?2004l[?2004h>                 
[?2004l[?2004h>                 # Determine new filename and path
[?2004l[?2004h>                 new_filename = f"{prefix}__{ckpt_file_path.name}"
[?2004l[?2004h>                 new_ckpt_path = ckpt_archive_dir / new_filename
[?2004l[?2004h>                 
[?2004l[?2004h>                 # Move checkpoint
[?2004l[?2004h>                 shutil.move(str(ckpt_file_path), str(new_ckpt_path))
[?2004l[?2004h>                 total_moved_ckpt_count += 1
[?2004l[?2004h>                 
[?2004l[?2004h>     # Phase 2: Generate Inventory CSV
[?2004l[?2004h>     inventory_records = []
[?2004l[?2004h>     
[?2004l[?2004h>     # Define columns and CRLF
[?2004l[?2004h>     columns = ['filepath', 'size_bytes', 'sha256']
[?2004l[?2004h>     crlf = b'\r\n'
[?2004l[?2004h>     
[?2004l[?2004h>     for root, _, files in os.walk(experiments_base_dir):
[?2004l[?2004h>         for file_name in files:
[?2004l[?2004h>             current_file_path = Path(root) / file_name
[?2004l[?2004h>             if current_file_path.name == 'artifact_inventory.csv' or current_file_path.name == 'cleanup.log':
[?2004l[?2004h>                 continue
[?2004l[?2004h>             
[?2004l[?2004h>             try:
[?2004l[?2004h>                 filepath_str = str(current_file_path.relative_to(experiments_base_dir))
[?2004l[?2004h>                 size_bytes = current_file_path.stat().st_size
[?2004l[?2004h>                 
[?2004l[?2004h>                 # Calculate SHA256 hash
[?2004l[?2004h>                 sha256_hash = hashlib.sha256()
[?2004l[?2004h>                 with current_file_path.open('rb') as f:
[?2004l[?2004h>                     for chunk in iter(lambda: f.read(4096), b""):
[?2004l[?2004h>                         sha256_hash.update(chunk)
[?2004l[?2004h>                 sha256_sum = sha256_hash.hexdigest()
[?2004l[?2004h>                 
[?2004l[?2004h>                 record = [filepath_str, str(size_bytes), sha256_sum]
[?2004l[?2004h>                 inventory_records.append(record)
[?2004l[?2004h>             except Exception as e:
[?2004l[?2004h>                 # Skip files that cause errors during hashing/stats
[?2004l[?2004h>                 print(f"Warning: Could not process {current_file_path} for inventory: {e}")
[?2004l[?2004h>                 continue
[?2004l[?2004h>     
[?2004l[?2004h>     # Sort records case-insensitively by filepath
[?2004l[?2004h>     inventory_records.sort(key=lambda x: x[0].lower())
[?2004l[?2004h>     
[?2004l[?2004h>     inventory_csv_path = experiments_base_dir / 'artifact_inventory.csv'
[?2004l[?2004h>     
[?2004l[?2004h>     # Write CSV with UTF-16LE, BOM, and CRLF
[?2004l[?2004h>     try:
[?2004l[?2004h>         with inventory_csv_path.open('wb') as f:
[?2004l[?2004h>             # UTF-16LE BOM is 0xFF, 0xFE
[?2004l[?2004h>             f.write(b'\xff\xfe') 
[?2004l[?2004h>             writer = csv.writer(f, delimiter=';', lineterminator=crlf)
[?2004l[?2004h>             writer.writerow(columns)
[?2004l[?2004h>             for record in inventory_records:
[?2004l[?2004h>                 # Ensure all field strings are UTF-8 encoded before writing to UTF-16LE
[?2004l[?2004h>                 utf8_encoded_record = [field.encode('utf-8') for field in record]
[?2004l[?2004h>                 writer.writerow(utf8_encoded_record)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error writing inventory CSV: {e}")
[?2004l[?2004h>         sys.exit(1)
[?2004l[?2004h>     
[?2004l[?2004h>     # Phase 3: Calculate Inventory MD5
[?2004l[?2004h>     inventory_md5 = get_md5_checksum(inventory_csv_path)
[?2004l[?2004h>     
[?2004l[?2004h>     # Phase 4: Calculate Global Average Loss
[?2004l[?2004h>     global_avg_loss = 0.0
[?2004l[?2004h>     if all_readable_losses:
[?2004l[?2004h>         global_avg_loss = sum(all_readable_losses) / len(all_readable_losses)
[?2004l[?2004h>     formatted_avg_loss = f"{global_avg_loss:.4f}"
[?2004l[?2004h>     
[?2004l[?2004h>     # Phase 5: Write Cleanup Log
[?2004l[?2004h>     log_file_path = experiments_base_dir / 'cleanup.log'
[?2004l[?2004h>     log_content = f"VALID_CKPTS_MOVED: {total_moved_ckpt_count}\nGLOBAL_AVG_LOSS: {formatted_avg_loss}\nINVENTORY_MD5: {inventory_md5}"
[?2004l[?2004h>     
[?2004l[?2004h>     try:
[?2004l[?2004h>         with log_file_path.open('w') as f:
[?2004l[?2004h>             f.write(log_content)
[?2004l[?2004h>         
[?2004l[?2004h>         # Set permissions to 0400 (read-only for owner)
[?2004l[?2004h>         os.chmod(log_file_path, 0o400)
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error writing cleanup log: {e}")
[?2004l[?2004h>         sys.exit(1)
[?2004l[?2004h>     
[?2004l[?2004h>     print("Script execution completed successfully.")
[?2004l[?2004h>     print(f"Moved {total_moved_ckpt_count} valid checkpoints.")
[?2004l[?2004h>     print(f"Global average loss: {formatted_avg_loss}")
[?2004l[?2004h>     print(f"Inventory MD5: {inventory_md5}")
[?2004l[?2004h> 
[?2004l[?2004h> if __name__ == '__main__':
[?2004l[?2004h>     main()
[?2004l[?2004h> EOFpython3 /tmp/archive_script.py
[?2004l[?2004h> python3 /tmp/archive_script.py
[?2004l[?2004h> 