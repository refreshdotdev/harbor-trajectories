```json
{
  "analysis": "The configuration file specifies that directories matching the regex '.*test.*' should be excluded and that the precision for the average should be 3 decimal places. We now need to inspect the log files in the remaining directories while respecting these settings.",
  "plan": "1. Create a Python script to handle the aggregation logic. 2. In the script, use os.walk() to traverse subdirectories. 3. Skip any directory matching the exclusion regex. 4. For each solver_run.log file, attempt to read it as UTF-8 first, then fall back to ISO-8859-1 if there's an encoding error. 5. Parse the CSV data, checking for NaN/Inf/-Inf values in the 'objective' column. 6. For valid files, find the minimum objective value and record the file's modification time. 7. After processing all files, calculate the average and generate the output files.",
  "commands": [
    {
      "keystrokes": "cat > aggregate_results.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport re\nimport csv\nimport json\nimport glob\nimport sys\nimport hashlib\nimport datetime\nfrom typing import List, Dict, Tuple, Optional\n\nclass ExperimentAggregator:\n    def __init__(self, config_path: str):\n        self.exclude_regex = \"\"\n        self.precision = 2\n        self.valid_runs = []\n        self.min_objectives = []\n        self.latest_mtime = 0.0\n        \n        # Read and parse configuration\n        if os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if line and not line.startswith('#'):\n                        key, value = line.split('=', 1)\n                        key = key.strip().lower()\n                        value = value.strip()\n                        if key == 'exclude_regex':\n                            self.exclude_regex = value\n                        elif key == 'precision':\n                            try:\n                                self.precision = int(value)\n                            except ValueError:\n                                pass  # Default to 2 if invalid\n        else:\n            print(f\"Warning: Config file {config_path} not found.\")\n        \n        self.exclude_pattern = re.compile(self.exclude_regex)\n    \n    def should_exclude_dir(self, dir_name: str) -> bool:\n        return bool(self.exclude_pattern.search(dir_name))\n    \n    def parse_log_file(self, log_path: str) -> Optional[float]:\n        min_objective = None\n        encoding = 'utf-8'\n        \n        # Try UTF-8 first\n        try:\n            with open(log_path, 'r', encoding=encoding) as f:\n                reader = csv.DictReader(f)\n                for row in reader:\n                    try:\n                        obj_val = float(row.get('objective', ''))\n                        # Check for NaN, Inf, -Inf\n                        if not row.get('objective') or obj_val in [float('nan'), float('inf'), float('-inf')]:\n                            return None  # Unstable run\n                        if min_objective is None or obj_val < min_objective:\n                            min_objective = obj_val\n                    except (ValueError, TypeError):\n                        continue\n        except UnicodeDecodeError:\n            # Fallback to ISO-8859-1 (Latin-1)\n            encoding = 'iso-8859-1'\n            try:\n                with open(log_path, 'r', encoding=encoding) as f:\n                    reader = csv.DictReader(f)\n                    for row in reader:\n                        try:\n                            obj_val = float(row.get('objective', ''))\n                            if not row.get('objective') or obj_val in [float('nan'), float('inf'), float('-inf')]:\n                                return None\n                            if min_objective is None or obj_val < min_objective:\n                                min_objective = obj_val\n                        except (ValueError, TypeError):\n                            continue\n            except Exception as e:\n                print(f\"Error reading {log_path} as {encoding}: {e}\")\n                return None\n        return min_objective\n    \n    def process_directory(self, root_dir: str):\n        for dirpath, dirnames, filenames in os.walk(root_dir):\n            # Exclude directories matching the regex pattern\n            dirnames[:] = [d for d in dirnames if not self.should_exclude_dir(d)]\n            \n            for filename in filenames:\n                if filename == 'solver_run.log':\n                    log_path = os.path.join(dirpath, filename)\n                    try:\n                        # Get file modification time\n                        mtime = os.path.getmtime(log_path)\n                        if mtime > self.latest_mtime:\n                            self.latest_mtime = mtime\n                        \n                        # Process log file\n                        min_obj = self.parse_log_file(log_path)\n                        if min_obj is not None:\n                            self.min_objectives.append(min_obj)\n                            self.valid_runs.append(log_path)\n                    except Exception as e:\n                        print(f\"Error processing {log_path}: {e}\")\n    \n    def generate_summary(self, output_dir: str):\n        if not self.min_objectives:\n            print(\"No valid runs found.\")\n            return\n        \n        avg_min = sum(self.min_objectives) / len(self.min_objectives)\n        rounded_avg = round(avg_min, self.precision)\n        \n        # Convert latest_mtime to ISO format\n        try:\n            latest_activity = datetime.datetime.fromtimestamp(self.latest_mtime).isoformat()\n        except Exception:\n            latest_activity = \"1970-01-01T00:00:00\"\n        \n        summary_data = {\n            \"valid_runs\": len(self.min_objectives),\n            \"average_min_objective\": rounded_avg,\n            \"latest_activity\": latest_activity\n        }\n        \n        # Write summary.json in ASCII\n        with open(os.path.join(output_dir, 'summary.json'), 'w', encoding='ascii') as f:\n            json.dump(summary_data, f, ensure_ascii=True)\n        \n        # Generate manifest\n        with open(os.path.join(output_dir, 'summary.json'), 'rb') as f:\n            md5 = hashlib.md5(f.read()).hexdigest()\n        manifest_str = f\"summary.json:{md5}\"\n        \n        with open(os.path.join(output_dir, 'manifest.txt'), 'w') as f:\n            f.write(manifest_str)\n            # Ensure no trailing newline\n            f.seek(0, os.SEEK_END)\n            f.truncate()\n\nif __name__ == '__main__':\n    aggregator = ExperimentAggregator('experiment_config.txt')\n    aggregator.process_directory('.')  # Current directory\n    aggregator.generate_summary('.')   # Output files in current directory\nEOF",
      "duration": 1.0
    },
    {
      "keystrokes": "python3 aggregate_results.py\n",
      "duration": 2.0
    }
  ]
}
```