{"version": 2, "width": 160, "height": 40, "timestamp": 1771009615, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.005626, "o", "\u001b[?2004hroot@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user# "]
[1.047593, "i", "clear\r"]
[1.047725, "o", "clear\r\n\u001b[?2004l\r"]
[1.048741, "o", "\u001b[H\u001b[J"]
[1.048909, "o", "\u001b[?2004hroot@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user# "]
[82.105833, "i", "mkdir -p /home/user/query_optimization\n"]
[82.105975, "o", "mkdir -p /home/user/query_optimization\r\n\u001b[?2004l\r"]
[82.108054, "o", "\u001b[?2004hroot@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user# "]
[82.234384, "i", "mkdir -p /home/user/db_tools\n"]
[82.234538, "o", "mkdir -p /home/user/db_tools\r\n\u001b[?2004l\r"]
[82.235773, "o", "\u001b[?2004hroot@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user# "]
[82.380313, "i", "cat > /home/user/db_tools/db_config.ini << 'EOF'\n[DEFAULT]\nlog_file_path = /path/to/your/log/file.log\nmin_duration_ms = 10\nEOF\n"]
[82.380477, "o", "cat > /home/user/db_tools/db_config.ini << 'EOF'\r\n\u001b[?2004l\r"]
[82.380516, "o", "\u001b[?2004h"]
[82.380533, "o", "> "]
[82.380553, "o", "[DEFAULT]\r\n\u001b[?2004l\r"]
[82.380574, "o", "\u001b[?2004h> "]
[82.380682, "o", "log_file_path = /path/to/your/log/file.log\r\n\u001b[?2004l\r"]
[82.380704, "o", "\u001b[?2004h> "]
[82.380759, "o", "min_duration_ms = 10\r\n\u001b[?2004l\r"]
[82.380776, "o", "\u001b[?2004h> "]
[82.380792, "o", "EOF\r\n\u001b[?2004l\r"]
[82.38211, "o", "\u001b[?2004h"]
[82.382145, "o", "root@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user# "]
[82.520613, "i", "cd /home/user/query_optimization\n"]
[82.520745, "o", "cd /home/user/query_optimization\r\n\u001b[?2004l\r"]
[82.520831, "o", "\u001b[?2004hroot@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
[82.608962, "i", "python3 -m venv env\n"]
[82.609078, "o", "python3 -m venv env\r\n\u001b[?2004l\r"]
[83.628941, "i", "source env/bin/activate\n"]
[83.628979, "o", "source env/bin/activate\r\n"]
[83.751564, "i", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.9\n"]
[83.751668, "o", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.9\r\n"]
[84.519556, "o", "\u001b[?2004hroot@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
[84.519706, "o", "source env/bin/activate\r\n\u001b[?2004l\r"]
[84.519866, "o", "\u001b[?2004h(env) root@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
[84.52001, "o", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.9\r\n\u001b[?2004l\r"]
[84.784903, "o", "Collecting sqlparse==0.4.4\r\n"]
[84.817174, "o", "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\r\n"]
[84.817425, "o", "\u001b[?25l"]
[84.818416, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[84.827391, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[84.93051, "o", "Collecting psycopg2-binary==2.9.9\r\n"]
[84.97406, "o", "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n"]
[84.974309, "o", "\u001b[?25l"]
[84.975267, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[85.009808, "o", "\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/3.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"]
[85.021677, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[85.042611, "o", "Installing collected packages: sqlparse, psycopg2-binary\r\n"]
[85.109918, "o", "Successfully installed psycopg2-binary-2.9.9 sqlparse-0.4.4\r\n"]
[85.152415, "o", "\u001b[?2004h(env) root@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
[88.761861, "i", "cat > analyze_and_optimize.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport sys\nimport csv\nimport hashlib\nimport configparser\nimport sqlparse\nfrom sqlparse.sql import IdentifierList, Identifier, Where, Comparison, Parenthesis\nfrom sqlparse.tokens import Keyword, DML\nimport re\nimport json\nimport argparse\nimport datetime\n\nclass QueryAnalyzer:\n    def __init__(self):\n        self.config_path = \"/home/user/db_tools/db_config.ini\"\n        self.log_file_path = None\n        self.min_duration_ms = None\n        self.staging_dir = \"staging\"\n        self.staging_file = os.path.join(self.staging_dir, \"raw_extract.csv\")\n        self.report_file = \"report.json\"\n        self.args = self.parse_arguments()\n        \n    def parse_arguments(self):\n        parser = argparse.ArgumentParser(description='SQL Query Log Analysis & Optimization')\n        parser.add_argument('--date', type=str, required=True, help='Date in YYYY-MM-DD format')\n        return parser.parse_args()\n        \n    def read_config(self):\n        config = con"]
[88.76196, "i", "figparser.ConfigParser()\n        config.read(self.config_path)\n        self.log_file_path = config.get('DEFAULT', 'log_file_path')\n        self.min_duration_ms = int(config.get('DEFAULT', 'min_duration_ms'))\n        \n    def create_staging_directory(self):\n        os.makedirs(self.staging_dir, exist_ok=True)\n        \n    def parse_log_line(self, line):\n        parts = list(filter(None, [p.strip() for p in line.split('|')]))\n        if len(parts) < 4:\n            return None\n        timestamp_str = parts[0]\n        duration_str = parts[1]\n        level_str = parts[2]\n        message = parts[3]\n        \n        try:\n            duration_ms = int(duration_str.rstrip('ms'))\n            timestamp = datetime.datetime.fromisoformat(timestamp_str.rstrip('Z'))\n            return timestamp, duration_ms, level_str, message\n        except (ValueError, IndexError):\n            return None\n        \n    def is_query(self, level_str):\n        return level_str.strip() == '[QUERY]'\n        \n    def filter_log_entries(self, log"]
[88.761989, "i", "_lines):\n        filtered_entries = []\n        target_date = self.args.date\n        \n        for line in log_lines:\n            parsed = self.parse_log_line(line)\n            if not parsed:\n                continue\n            timestamp, duration_ms, level_str, message = parsed\n            \n            if (timestamp.strftime('%Y-%m-%d') == target_date and\n                self.is_query(level_str) and\n                duration_ms >= self.min_duration_ms):\n                filtered_entries.append((timestamp, duration_ms, message))\n        return filtered_entries\n        \n    def write_staging_csv(self, entries):\n        self.create_staging_directory()\n        with open(self.staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as f:\n            writer = csv.writer(f)\n            writer.writerow(['timestamp', 'duration_ms', 'query_text'])\n            for entry in entries:\n                writer.writerow([entry[0].isoformat() + 'Z', entry[1], entry[2]])\n        \n    def compute_staging_file_sha256(self):\n       "]
[88.76201, "i", " hasher = hashlib.sha256()\n        with open(self.staging_file, 'rb') as f:\n            while True:\n                chunk = f.read(4096)\n                if not chunk:\n                    break\n                hasher.update(chunk)\n        return hasher.hexdigest()\n        \n    def normalize_sql(self, sql_query):\n        try:\n            parsed = sqlparse.parse(sql_query)[0]\n            parsed = parsed.token_next_match(0, Keyword)\n            formatted = parsed.value.strip()\n            if formatted.endswith(';'):\n                formatted = formatted[:-1].strip()\n            return formatted\n        except (IndexError, sqlparse.exceptions.SQLParseError):\n            return sql_query.strip()\n        \n    def extract_table_and_columns(self, normalized_sql):\n        try:\n            stmt = sqlparse.parse(normalized_sql)[0]\n            tokens = [t for t in stmt.tokens if t.is_whitespace == False]\n            \n            where_clause = None\n            for token in tokens:\n                if isinstance(token, Wher"]
[88.76203, "o", "cat > analyze_and_optimize.py << 'EOF'\r\n\u001b[?2004l\r"]
[88.762059, "i", "e):\n                    where_clause = token\n                    break\n            \n            if not where_clause:\n                return None, None\n                \n            conditions = []\n            current_condition = []\n            \n            for token in where_clause.tokens[1:]:  # Skip 'WHERE' keyword\n                if token.value.upper() == 'AND':\n                    if current_condition:\n                        conditions.append(current_condition)\n                        current_condition = []\n                else:\n                    current_condition.append(token.value.strip())\n            \n            if current_condition:\n                conditions.append(current_condition)\n                \n            table_name = None\n            columns = set()\n            \n            # Find the table name (simplified: first FROM clause)\n            from_seen = False\n            for token in stmt.tokens:\n                if token.value.upper() == 'FROM':\n                    from_seen = True\n          "]
[88.762077, "o", " hasher = hashlib.sha256()\r\n        with open(self.staging_file, 'rb') as f:\r\n            while True:\r\n                chunk = f.read(4096)\r\n                if not chunk:\r\n                    break\r\n                hasher.update(chunk)\r\n        re"]
[88.762106, "i", "          continue\n                if from_seen and isinstance(token, Identifier):\n                    table_name = token.get_real_name()\n                    break\n            \n            if not table_name:\n                return None, None\n                \n            # Parse conditions to extract columns\n            for cond in conditions:\n                if len(cond) >= 3 and cond[1].strip() == '=':\n                    col_name = cond[0].strip()\n                    columns.add(col_name)\n            \n            if not columns:\n                return None, None\n                \n            sorted_columns = sorted(columns)\n            return table_name, sorted_columns\n        except (IndexError, sqlparse.exceptions.SQLParseError):\n            return None, None\n        \n    def generate_index_suggestion(self, table_name, columns):\n        if not table_name or not columns:\n            return None\n        col_str = '_'.join(columns)\n        return f\"CREATE INDEX idx_{table_name}_{col_str} ON {table_name}({', '"]
[88.762124, "o", "turn hasher.hexdigest()\r\n        \r\n    def normalize_sql(self, sql_query):\r\n        try:\r\n            parsed = sqlparse.parse(sql_query)[0]\r\n            parsed = parsed.token_next_match(0, Keyword)\r\n            formatted = parsed.value.strip()\r\n"]
[88.762144, "i", ".join(columns)});\"\n        \n    def process_queries(self, entries):\n        normalized_queries = {}\n        \n        for entry in entries:\n            timestamp, duration_ms, query_text = entry\n            normalized_sql = self.normalize_sql(query_text)\n            \n            if normalized_sql not in normalized_queries:\n                normalized_queries[normalized_sql] = timestamp\n                \n        return normalized_queries\n        \n    def generate_report(self, staging_sha256, stats, suggestions):\n        report = {\n            \"meta\": {\n                \"date_analyzed\": self.args.date,\n                \"staging_file_sha256\": staging_sha256\n            },\n            \"stats\": {\n                \"total_queries_extracted\": stats['total_queries_extracted'],\n                \"unique_patterns\": stats['unique_patterns']\n            },\n            \"suggestions\": suggestions\n        }\n        \n        with open(self.report_file, 'w', encoding='utf-8', newline='\\n') as f:\n            json.dump(report, f, indent"]
[88.76216, "o", "            if formatted.endswith(';'):\r\n                formatted = formatted[:-1].strip()\r\n            return format"]
[88.762179, "i", "=2)\n        \n    def run(self):\n        try:\n            self.read_config()\n            \n            if not os.path.exists(self.log_file_path):\n                print(f\"Error: Log file not found at {self.log_file_path}\")\n                sys.exit(1)\n            \n            with open(self.log_file_path, 'r', encoding='ISO-8859-1') as f:\n                log_lines = f.readlines()\n            \n            filtered_entries = self.filter_log_entries(log_lines)\n            \n            if not filtered_entries:\n                print(\"No queries matched the criteria.\")\n                with open(self.staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as f:\n                    writer = csv.writer(f)\n                    writer.writerow(['timestamp', 'duration_ms', 'query_text'])\n                staging_sha256 = self.compute_staging_file_sha256()\n                self.generate_report(staging_sha256, {\n                    'total_queries_extracted': 0,\n                    'unique_patterns': 0\n                }, [])\n   "]
[88.762197, "o", "ted\r\n        except (IndexError, sqlparse.exceptions.SQLParseError):\r\n            return sql_q"]
[88.762216, "i", "             return\n            \n            self.write_staging_csv(filtered_entries)\n            staging_sha256 = self.compute_staging_file_sha256()\n            \n            normalized_queries_map = self.process_queries(filtered_entries)\n            suggestions = []\n            \n            for norm_sql, first_occurrence_timestamp in normalized_queries_map.items():\n                table_name, columns = self.extract_table_and_columns(norm_sql)\n                index_suggestion = self.generate_index_suggestion(table_name, columns)\n                if index_suggestion:\n                    suggestions.append({\n                        \"id\": first_occurrence_timestamp.isoformat() + 'Z',\n                        \"normalized_sql\": norm_sql,\n                        \"index_suggestion\": index_suggestion\n                    })\n            \n            suggestions.sort(key=lambda x: x['id'])\n            \n            self.generate_report(staging_sha256, {\n                'total_queries_extracted': len(filtered_entries),\n    "]
[88.762231, "o", "uery.strip()\r\n        \r\n    def extract_table_and_columns(self, normalized_sql):\r\n        try:\r\n            stmt = sqlparse.parse(norma"]
[88.76225, "i", "            'unique_patterns': len(normalized_queries_map)\n            }, suggestions)\n            \n        except Exception as e:\n            print(f\"Error: {e}\")\n            import traceback\n            traceback.print_exc()\n\ndef main():\n    analyzer = QueryAnalyzer()\n    analyzer.run()\n\nif __name__ == '__main__':\n    main()\nEOF\n"]
[88.762266, "o", "lized_sql)[0]\r\n            tokens = [t for t in stmt.tokens if t.is_whitespace == False]\r\n            \r\n            where_clause = None\r\n            for token in tokens:\r\n                if isinstance(token, Wher"]
[88.762285, "o", "e):\r\n                    where_clause ="]
[88.7623, "o", "\u001b[?2004h> "]
[88.762379, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[88.762403, "o", "\u001b[?2004h> "]
[88.762445, "o", "import os\r\n\u001b[?2004l\r"]
[88.762467, "o", "\u001b[?2004h> "]
[88.762511, "o", "import sys\r\n\u001b[?2004l\r"]
[88.762528, "o", "\u001b[?2004h> "]
[88.762561, "o", "import csv\r\n\u001b[?2004l\r"]
[88.762577, "o", "\u001b[?2004h> "]
[88.762632, "o", "import hashlib\r\n\u001b[?2004l\r"]
[88.762649, "o", "\u001b[?2004h> "]
[88.762711, "o", "import configparser\r\n\u001b[?2004l\r"]
[88.762731, "o", "\u001b[?2004h> "]
[88.762785, "o", "import sqlparse\r\n\u001b[?2004l\r"]
[88.762807, "o", "\u001b[?2004h> "]
[88.763062, "o", "from sqlparse.sql import IdentifierList, Identifier, Where, Comparison, Parenthesis\r\n\u001b[?2004l\r"]
[88.763083, "o", "\u001b[?2004h> "]
[88.763206, "o", "from sqlparse.tokens import Keyword, DML\r\n\u001b[?2004l\r"]
[88.763227, "o", "\u001b[?2004h> "]
[88.763262, "o", "import re\r\n\u001b[?2004l\r"]
[88.763283, "o", "\u001b[?2004h> "]
[88.763325, "o", "import json\r\n\u001b[?2004l\r"]
[88.763346, "o", "\u001b[?2004h> "]
[88.763398, "o", "import argparse\r\n\u001b[?2004l\r"]
[88.763415, "o", "\u001b[?2004h> "]
[88.763469, "o", "import datetime\r\n\u001b[?2004l\r"]
[88.763494, "o", "\u001b[?2004h> "]
[88.763514, "o", "\r\n\u001b[?2004l\r \u001b[?2004h"]
[88.763534, "o", "> "]
[88.763587, "o", "class QueryAnalyzer:\r\n\u001b[?2004l\r"]
[88.763602, "o", "\u001b[?2004h> "]
[88.763691, "o", "    def __init__(self):\r\n\u001b[?2004l\r"]
[88.763708, "o", "\u001b[?2004h> "]
[88.763915, "o", "        self.config_path = \"/home/user/db_tools/db_config.ini\"\r\n\u001b[?2004l\r"]
[88.76394, "o", "\u001b[?2004h> "]
[88.764033, "o", "        self.log_file_path = None\r\n\u001b[?2004l\r"]
[88.764072, "o", "\u001b[?2004h> "]
[88.764176, "o", "        self.min_duration_ms = None\r\n\u001b[?2004l\r"]
[88.764199, "o", "\u001b[?2004h> "]
[88.76431, "o", "        self.staging_dir = \"staging\"\r\n\u001b[?2004l\r"]
[88.764331, "o", "\u001b[?2004h> "]
[88.764562, "o", "        self.staging_file = os.path.join(self.staging_dir, \"raw_extract.csv\")\r\n\u001b[?2004l\r"]
[88.764585, "o", "\u001b[?2004h> "]
[88.764708, "o", "        self.report_file = \"report.json\"\r\n\u001b[?2004l\r"]
[88.76473, "o", "\u001b[?2004h> "]
[88.764852, "o", "        self.args = self.parse_arguments()\r\n\u001b[?2004l\r"]
[88.764873, "o", "\u001b[?2004h> "]
[88.764903, "o", "        \r\n\u001b[?2004l\r"]
[88.764923, "o", "\u001b[?2004h> "]
[88.765014, "o", "    def parse_arguments(self):\r\n\u001b[?2004l\r"]
[88.765033, "o", "\u001b[?2004h> "]
[88.765364, "o", "        parser = argparse.ArgumentParser(description='SQL Query Log Analysis & Optimization')\r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.765644, "o", "        parser.add_argument('--date', type=str, required=True, help='Date in YYYY-MM-DD format')\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.765685, "o", "> "]
[88.765766, "o", "        return parser.parse_args()\r\n\u001b[?2004l\r"]
[88.765804, "o", "\u001b[?2004h> "]
[88.765846, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.765918, "o", "    def read_config(self):\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.765954, "o", "> "]
[88.766085, "o", "        config = configparser.ConfigParser()\r\n\u001b[?2004l\r"]
[88.766122, "o", "\u001b[?2004h> "]
[88.766228, "o", "        config.read(self.config_path)\r\n\u001b[?2004l\r"]
[88.766264, "o", "\u001b[?2004h> "]
[88.766448, "o", "        self.log_file_path = config.get('DEFAULT', 'log_file_path')\r\n\u001b[?2004l\r"]
[88.766487, "o", "\u001b[?2004h> "]
[88.766694, "o", "        self.min_duration_ms = int(config.get('DEFAULT', 'min_duration_ms'))\r\n\u001b[?2004l\r"]
[88.76673, "o", "\u001b[?2004h>         \r\n"]
[88.766772, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[88.766895, "o", "    def create_staging_directory(self):\r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.767071, "o", "        os.makedirs(self.staging_dir, exist_ok=True)\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.767116, "o", ">         \r\n\u001b[?2004l\r"]
[88.767158, "o", "\u001b[?2004h> "]
[88.767244, "o", "    def parse_log_line(self, line):\r\n\u001b[?2004l\r"]
[88.76728, "o", "\u001b[?2004h> "]
[88.767479, "o", "        parts = list(filter(None, [p.strip() for p in line.split('|')]))\r\n\u001b[?2004l\r"]
[88.767523, "o", "\u001b[?2004h> "]
[88.767584, "o", "        if len(parts) < 4:\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.76762, "o", "> "]
[88.767677, "o", "            return None\r\n\u001b[?2004l\r"]
[88.767715, "o", "\u001b[?2004h> "]
[88.767794, "o", "        timestamp_str = parts[0]\r\n\u001b[?2004l\r"]
[88.76783, "o", "\u001b[?2004h> "]
[88.767909, "o", "        duration_str = parts[1]\r\n\u001b[?2004l\r"]
[88.767944, "o", "\u001b[?2004h> "]
[88.768014, "o", "        level_str = parts[2]\r\n\u001b[?2004l\r"]
[88.768067, "o", "\u001b[?2004h> "]
[88.768126, "o", "        message = parts[3]\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.768164, "o", ">         \r\n\u001b[?2004l\r"]
[88.768205, "o", "\u001b[?2004h> "]
[88.768245, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.768424, "o", "            duration_ms = int(duration_str.rstrip('ms'))\r\n\u001b[?2004l\r"]
[88.768469, "o", "\u001b[?2004h> "]
[88.768688, "o", "            timestamp = datetime.datetime.fromisoformat(timestamp_str.rstrip('Z'))\r\n\u001b[?2004l\r"]
[88.768722, "o", "\u001b[?2004h> "]
[88.768891, "o", "            return timestamp, duration_ms, level_str, message\r\n\u001b[?2004l\r"]
[88.768925, "o", "\u001b[?2004h> "]
[88.769032, "o", "        except (ValueError, IndexError):\r\n\u001b[?2004l\r"]
[88.769075, "o", "\u001b[?2004h> "]
[88.769131, "o", "            return None\r\n\u001b[?2004l\r"]
[88.769168, "o", "\u001b[?2004h>         \r\n"]
[88.76921, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[88.769313, "o", "    def is_query(self, level_str):\r\n\u001b[?2004l\r"]
[88.769349, "o", "\u001b[?2004h> "]
[88.769472, "o", "        return level_str.strip() == '[QUERY]'\r\n\u001b[?2004l\r"]
[88.769504, "o", "\u001b[?2004h> "]
[88.769546, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.769671, "o", "    def filter_log_entries(self, log_lines):\r\n\u001b[?2004l\r"]
[88.769704, "o", "\u001b[?2004h> "]
[88.769783, "o", "        filtered_entries = []\r\n\u001b[?2004l\r"]
[88.769815, "o", "\u001b[?2004h> "]
[88.769913, "o", "        target_date = self.args.date\r\n\u001b[?2004l\r"]
[88.769947, "o", "\u001b[?2004h> "]
[88.769989, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.770078, "o", "        for line in log_lines:\r\n\u001b[?2004l\r"]
[88.770112, "o", "\u001b[?2004h> "]
[88.770237, "o", "            parsed = self.parse_log_line(line)\r\n\u001b[?2004l\r"]
[88.77028, "o", "\u001b[?2004h> "]
[88.770342, "o", "            if not parsed:\r\n\u001b[?2004l\r"]
[88.77038, "o", "\u001b[?2004h> "]
[88.770442, "o", "                continue\r\n\u001b[?2004l\r"]
[88.770484, "o", "\u001b[?2004h> "]
[88.770657, "o", "            timestamp, duration_ms, level_str, message = parsed\r\n\u001b[?2004l\r"]
[88.770694, "o", "\u001b[?2004h> "]
[88.770735, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.770926, "o", "            if (timestamp.strftime('%Y-%m-%d') == target_date and\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.770963, "o", "> "]
[88.771084, "o", "                self.is_query(level_str) and\r\n\u001b[?2004l\r"]
[88.771127, "o", "\u001b[?2004h> "]
[88.771265, "o", "                duration_ms >= self.min_duration_ms):\r\n\u001b[?2004l\r"]
[88.771308, "o", "\u001b[?2004h> "]
[88.7715, "o", "                filtered_entries.append((timestamp, duration_ms, message))\r\n\u001b[?2004l\r"]
[88.77154, "o", "\u001b[?2004h> "]
[88.771614, "o", "        return filtered_entries\r\n\u001b[?2004l\r"]
[88.771656, "o", "\u001b[?2004h>         \r\n\u001b[?2004l\r"]
[88.771698, "o", "\u001b[?2004h> "]
[88.771804, "o", "    def write_staging_csv(self, entries):\r\n\u001b[?2004l\r"]
[88.77186, "o", "\u001b[?2004h> "]
[88.771954, "o", "        self.create_staging_directory()\r\n\u001b[?2004l\r"]
[88.771993, "o", "\u001b[?2004h> "]
[88.772233, "o", "        with open(self.staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as f:\r\n\u001b[?2004l\r"]
[88.772268, "o", "\u001b[?2004h> "]
[88.772365, "o", "            writer = csv.writer(f)\r\n\u001b[?2004l\r"]
[88.772397, "o", "\u001b[?2004h> "]
[88.772606, "o", "            writer.writerow(['timestamp', 'duration_ms', 'query_text'])\r\n\u001b[?2004l\r"]
[88.772638, "o", "\u001b[?2004h> "]
[88.772735, "o", "            for entry in entries:\r\n\u001b[?2004l\r"]
[88.772772, "o", "\u001b[?2004h> "]
[88.772991, "o", "                writer.writerow([entry[0].isoformat() + 'Z', entry[1], entry[2]])\r\n\u001b[?2004l\r"]
[88.77303, "o", "\u001b[?2004h> "]
[88.773078, "o", "        \r\n\u001b[?2004l\re=\u001b[?2004h"]
[88.77313, "o", "> "]
[88.773268, "o", "    def compute_staging_file_sha256(self):\r\n\u001b[?2004l\r"]
[88.773308, "o", "\u001b[?2004h> "]
[88.773404, "o", "        hasher = hashlib.sha256()\r\n\u001b[?2004l\r"]
[88.773445, "o", "\u001b[?2004h> "]
[88.773582, "o", "        with open(self.staging_file, 'rb') as f:\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.773623, "o", "> "]
[88.773669, "o", "            while True:\r\n\u001b[?2004l\r"]
[88.773709, "o", "\u001b[?2004h> "]
[88.773798, "o", "                chunk = f.read(4096)\r\n\u001b[?2004l\r"]
[88.773835, "o", "\u001b[?2004h> "]
[88.773926, "o", "                if not chunk:\r\n\u001b[?2004l\r"]
[88.773962, "o", "\u001b[?2004h> "]
[88.774033, "o", "                    break\r\n\u001b[?2004l\r"]
[88.774089, "o", "\u001b[?2004h> "]
[88.774171, "o", "                hasher.update(chunk)\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.774213, "o", "> "]
[88.774293, "o", "        return hasher.hexdigest()\r\n\u001b[?2004l\r"]
[88.77433, "o", "\u001b[?2004h> "]
[88.774369, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.774488, "o", "    def normalize_sql(self, sql_query):\r\n\u001b[?2004l\r"]
[88.77453, "o", "\u001b[?2004h> "]
[88.774582, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.774621, "o", "> "]
[88.774727, "o", "            parsed = sqlparse.parse(sql_query)[0]\r\n\u001b[?2004l\r"]
[88.774758, "o", "\u001b[?2004h> "]
[88.774908, "o", "            parsed = parsed.token_next_match(0, Keyword)\r\n\u001b[?2004l\r"]
[88.774947, "o", "\u001b[?2004h> "]
[88.775077, "o", "            formatted = parsed.value.strip()\r\n\u001b[?2004l\r"]
[88.77512, "o", "\u001b[?2004h> "]
[88.775227, "o", "            if formatted.endswith(';'):\r\n\u001b[?2004l\r"]
[88.775269, "o", "\u001b[?2004h> "]
[88.775406, "o", "                formatted = formatted[:-1].strip()\r\n\u001b[?2004l\r"]
[88.775448, "o", "\u001b[?2004h> "]
[88.775522, "o", "            return formatted\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.775572, "o", "> "]
[88.775724, "o", "        except (IndexError, sqlparse.exceptions.SQLParseError):\r\n\u001b[?2004l\r"]
[88.775761, "o", "\u001b[?2004h> "]
[88.775853, "o", "            return sql_query.strip()\r\n\u001b[?2004l\r"]
[88.775895, "o", "\u001b[?2004h> "]
[88.77594, "o", "        \r\n\u001b[?2004l\r \u001b[?2004h> "]
[88.776116, "o", "    def extract_table_and_columns(self, normalized_sql):\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.776155, "o", "> "]
[88.776198, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.776369, "o", "            stmt = sqlparse.parse(normalized_sql)[0]\r\n\u001b[?2004l\r"]
[88.776411, "o", "\u001b[?2004h> "]
[88.776607, "o", "            tokens = [t for t in stmt.tokens if t.is_whitespace == False]\r\n\u001b[?2004l\r"]
[88.776646, "o", "\u001b[?2004h> "]
[88.776689, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.776784, "o", "            where_clause = None\r\n\u001b[?2004l\r"]
[88.776822, "o", "\u001b[?2004h> "]
[88.776911, "o", "            for token in tokens:\r\n\u001b[?2004l\r"]
[88.77695, "o", "\u001b[?2004h> "]
[88.777068, "o", "                if isinstance(token, Where):\r\n\u001b[?2004l\r"]
[88.777101, "o", "\u001b[?2004h> "]
[88.77721, "o", "                    where_clause = token\r\n\u001b[?2004l\r"]
[88.777251, "o", "\u001b[?2004h> "]
[88.777312, "o", "                    break\r\n\u001b[?2004l\r"]
[88.777351, "o", "\u001b[?2004h> "]
[88.777388, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h"]
[88.777398, "o", "> "]
[88.777498, "o", "            if not where_clause:\r\n\u001b[?2004l\r"]
[88.777539, "o", "\u001b[?2004h> "]
[88.777633, "o", "                return None, None\r\n\u001b[?2004l\r"]
[88.777673, "o", "\u001b[?2004h> "]
[88.777713, "o", "                \r\n\u001b[?2004l\r"]
[88.777748, "o", "\u001b[?2004h> "]
[88.777812, "o", "            conditions = []\r\n\u001b[?2004l\r"]
[88.777831, "o", "\u001b[?2004h> "]
[88.777937, "o", "            current_condition = []\r\n\u001b[?2004l\r"]
[88.777969, "o", "\u001b[?2004h> "]
[88.777999, "o", "            \r\n\u001b[?2004l\r"]
[88.778034, "o", "\u001b[?2004h> "]
[88.778246, "o", "            for token in where_clause.tokens[1:]:  # Skip 'WHERE' keyword\r\n\u001b[?2004l\r"]
[88.778283, "o", "\u001b[?2004h> "]
[88.778408, "o", "                if token.value.upper() == 'AND':\r\n\u001b[?2004l\r"]
[88.778445, "o", "\u001b[?2004h> "]
[88.778543, "o", "                    if current_condition:\r\n\u001b[?2004l\r"]
[88.778575, "o", "\u001b[?2004h> "]
[88.778755, "o", "                        conditions.append(current_condition)\r\n\u001b[?2004l\r"]
[88.778787, "o", "\u001b[?2004h> "]
[88.778919, "o", "                        current_condition = []\r\n\u001b[?2004l\r"]
[88.77895, "o", "\u001b[?2004h> "]
[88.779012, "o", "                else:\r\n\u001b[?2004l\r  "]
[88.77905, "o", "\u001b[?2004h"]
[88.779086, "o", "> "]
[88.779223, "o", "                    current_condition.append(token.value.strip())\r\n\u001b[?2004l\r"]
[88.779254, "o", "\u001b[?2004h> "]
[88.779289, "o", "            \r\n\u001b[?2004l\r"]
[88.779324, "o", "\u001b[?2004h> "]
[88.779416, "o", "            if current_condition:\r\n\u001b[?2004l\r"]
[88.779455, "o", "\u001b[?2004h> "]
[88.779597, "o", "                conditions.append(current_condition)\r\n\u001b[?2004l\r"]
[88.779635, "o", "\u001b[?2004h> "]
[88.779681, "o", "                \r\n\u001b[?2004l\r"]
[88.779717, "o", "\u001b[?2004h> "]
[88.779793, "o", "            table_name = None\r\n\u001b[?2004l\r"]
[88.779824, "o", "\u001b[?2004h> "]
[88.779889, "o", "            columns = set()\r\n\u001b[?2004l\r"]
[88.779921, "o", "\u001b[?2004h> "]
[88.779953, "o", "            \r\n\u001b[?2004l\r"]
[88.779963, "o", "\u001b[?2004h"]
[88.780014, "o", "> "]
[88.780164, "o", "            # Find the table name (simplified: first FROM clause)\r\n\u001b[?2004l\r"]
[88.7802, "o", "\u001b[?2004h> "]
[88.780272, "o", "            from_seen = False\r\n\u001b[?2004l\r"]
[88.780315, "o", "\u001b[?2004h> "]
[88.780405, "o", "            for token in stmt.tokens:\r\n\u001b[?2004l\r"]
[88.780437, "o", "\u001b[?2004h> "]
[88.780577, "o", "                if token.value.upper() == 'FROM':\r\n\u001b[?2004l\r"]
[88.780642, "o", "\u001b[?2004h> "]
[88.780708, "o", "                    from_seen = True\r\n\u001b[?2004l\r"]
[88.780746, "o", "\u001b[?2004h> "]
[88.780821, "o", "                    continue\r\n\u001b[?2004l\r"]
[88.780849, "o", "\u001b[?2004h> "]
[88.781029, "o", "                if from_seen and isinstance(token, Identifier):\r\n\u001b[?2004l\r"]
[88.781117, "o", "\u001b[?2004h> "]
[88.781228, "o", "                    table_name = token.get_real_name()\r\n\u001b[?2004l\r"]
[88.781265, "o", "\u001b[?2004h> "]
[88.781323, "o", "                    break\r\n\u001b[?2004l\r"]
[88.781354, "o", "\u001b[?2004h"]
[88.781385, "o", "> "]
[88.781417, "o", "            \r\n\u001b[?2004l\r"]
[88.781452, "o", "\u001b[?2004h> "]
[88.781523, "o", "            if not table_name:\r\n\u001b[?2004l\r"]
[88.781555, "o", "\u001b[?2004h> "]
[88.781642, "o", "                return None, None\r\n\u001b[?2004l\r"]
[88.781674, "o", "\u001b[?2004h> "]
[88.781711, "o", "                \r\n\u001b[?2004l\r"]
[88.781739, "o", "\u001b[?2004h> "]
[88.781882, "o", "            # Parse conditions to extract columns\r\n\u001b[?2004l\r"]
[88.781912, "o", "\u001b[?2004h> "]
[88.782006, "o", "            for cond in conditions:\r\n\u001b[?2004l\r"]
[88.782037, "o", "\u001b[?2004h> "]
[88.782178, "o", "                if len(cond) >= 3 and cond[1].strip() == '=':\r\n\u001b[?2004l\r"]
[88.782186, "o", "\u001b[?2004h"]
[88.782229, "o", "> "]
[88.782306, "o", "                    col_name = cond[0].strip()\r\n\u001b[?2004l\r"]
[88.782338, "o", "\u001b[?2004h> "]
[88.782424, "o", "                    columns.add(col_name)\r\n\u001b[?2004l\r"]
[88.782456, "o", "\u001b[?2004h> "]
[88.782486, "o", "            \r\n\u001b[?2004l\r"]
[88.782515, "o", "\u001b[?2004h> "]
[88.782563, "o", "            if not columns:\r\n\u001b[?2004l\r"]
[88.782595, "o", "\u001b[?2004h> "]
[88.782663, "o", "                return None, None\r\n\u001b[?2004l\r"]
[88.782696, "o", "\u001b[?2004h> "]
[88.782727, "o", "                \r\n\u001b[?2004l\r"]
[88.782757, "o", "\u001b[?2004h> "]
[88.782849, "o", "            sorted_columns = sorted(columns)\r\n\u001b[?2004l\r"]
[88.782882, "o", "\u001b[?2004h> "]
[88.782978, "o", "            return table_name, sorted_columns\r\n\u001b[?2004l\r"]
[88.78301, "o", "\u001b[?2004h> "]
[88.783171, "o", "        except (IndexError, sqlparse.exceptions.SQLParseError):\r\n\u001b[?2004l\r"]
[88.783205, "o", "\u001b[?2004h> "]
[88.783263, "o", "            return None, None\r\n\u001b[?2004l\r"]
[88.783271, "o", "\u001b[?2004h"]
[88.783285, "o", "> "]
[88.783337, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.78347, "o", "    def generate_index_suggestion(self, table_name, columns):\r\n\u001b[?2004l\r"]
[88.783502, "o", "\u001b[?2004h> "]
[88.783589, "o", "        if not table_name or not columns:\r\n\u001b[?2004l\r"]
[88.783621, "o", "\u001b[?2004h> "]
[88.783667, "o", "            return None\r\n\u001b[?2004l\r"]
[88.783698, "o", "\u001b[?2004h> "]
[88.783774, "o", "        col_str = '_'.join(columns)\r\n\u001b[?2004l\r"]
[88.783806, "o", "\u001b[?2004h> "]
[88.784022, "o", "        return f\"CREATE INDEX idx_{table_name}_{col_str} ON {table_name}({', '.join(columns)});\"\r\n\u001b[?2004l\r"]
[88.784124, "o", "\u001b[?2004h>         \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.784199, "o", "    def process_queries(self, entries):\r\n\u001b[?2004l\r"]
[88.78424, "o", "\u001b[?2004h> "]
[88.784296, "o", "        normalized_queries = {}\r\n\u001b[?2004l\r"]
[88.784333, "o", "\u001b[?2004h>         \r\n\u001b[?2004l\r"]
[88.784352, "o", "\u001b[?2004h> "]
[88.784428, "o", "        for entry in entries:\r\n\u001b[?2004l\r"]
[88.784461, "o", "\u001b[?2004h> "]
[88.784582, "o", "            timestamp, duration_ms, query_text = entry\r\n\u001b[?2004l\r"]
[88.784615, "o", "\u001b[?2004h> "]
[88.784748, "o", "            normalized_sql = self.normalize_sql(query_text)\r\n\u001b[?2004l\r"]
[88.78478, "o", "\u001b[?2004h> "]
[88.784805, "o", "            \r\n\u001b[?2004l\r"]
[88.784837, "o", "\u001b[?2004h"]
[88.784868, "o", "> "]
[88.784961, "o", "            if normalized_sql not in normalized_queries:\r\n\u001b[?2004l\r"]
[88.784992, "o", "\u001b[?2004h> "]
[88.785154, "o", "                normalized_queries[normalized_sql] = timestamp\r\n\u001b[?2004l\r"]
[88.785162, "o", "\u001b[?2004h"]
[88.785201, "o", "> "]
[88.785232, "o", "                \r\n\u001b[?2004l\r"]
[88.785261, "o", "\u001b[?2004h> "]
[88.785317, "o", "        return normalized_queries\r\n\u001b[?2004l\r"]
[88.785349, "o", "\u001b[?2004h> "]
[88.785364, "o", "        \r\n\u001b[?2004l\r"]
[88.785413, "o", "\u001b[?2004h> "]
[88.785596, "o", "    def generate_report(self, staging_sha256, stats, suggestions):\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.785684, "o", ">         report = {\r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.785728, "o", "            \"meta\": {\r\n\u001b[?2004l\r"]
[88.785766, "o", "\u001b[?2004h> "]
[88.785877, "o", "                \"date_analyzed\": self.args.date,\r\n\u001b[?2004l\r"]
[88.785909, "o", "\u001b[?2004h> "]
[88.786032, "o", "                \"staging_file_sha256\": staging_sha256\r\n\u001b[?2004l\r"]
[88.786056, "o", "\u001b[?2004h> "]
[88.786107, "o", "            },\r\n\u001b[?2004l\r"]
[88.786117, "o", "\u001b[?2004h"]
[88.78617, "o", "> "]
[88.786209, "o", "            \"stats\": {\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.786219, "o", "> "]
[88.786402, "o", "                \"total_queries_extracted\": stats['total_queries_extracted'],\r\n\u001b[?2004l\r"]
[88.78641, "o", "\u001b[?2004h"]
[88.786453, "o", "> "]
[88.78657, "o", "                \"unique_patterns\": stats['unique_patterns']\r\n\u001b[?2004l\r"]
[88.786606, "o", "\u001b[?2004h> "]
[88.786635, "o", "            },\r\n\u001b[?2004l\r"]
[88.786666, "o", "\u001b[?2004h> "]
[88.78676, "o", "            \"suggestions\": suggestions\r\n\u001b[?2004l\r"]
[88.786791, "o", "\u001b[?2004h> "]
[88.786823, "o", "        }\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.786853, "o", "> "]
[88.786883, "o", "        \r\n\u001b[?2004l\r"]
[88.786913, "o", "\u001b[?2004h> "]
[88.787086, "o", "        with open(self.report_file, 'w', encoding='utf-8', newline='\\n') as f:\r\n\u001b[?2004l\r"]
[88.787122, "o", "\u001b[?2004h> "]
[88.78722, "o", "            json.dump(report, f, indent=2)\r\n\u001b[?2004l\r"]
[88.787254, "o", "\u001b[?2004h> "]
[88.787285, "o", "        \r\n\u001b[?2004l\r"]
[88.787315, "o", "\u001b[?2004h> "]
[88.787343, "o", "    def run(self):\r\n\u001b[?2004l\r"]
[88.787382, "o", "\u001b[?2004h> "]
[88.787422, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.787431, "o", "> "]
[88.787507, "o", "            self.read_config()\r\n\u001b[?2004l\r"]
[88.787535, "o", "\u001b[?2004h> "]
[88.787564, "o", "            \r\n\u001b[?2004l\r"]
[88.787608, "o", "\u001b[?2004h> "]
[88.787725, "o", "            if not os.path.exists(self.log_file_path):\r\n\u001b[?2004l\r"]
[88.787763, "o", "\u001b[?2004h> "]
[88.787934, "o", "                print(f\"Error: Log file not found at {self.log_file_path}\")\r\n\u001b[?2004l\r"]
[88.787967, "o", "\u001b[?2004h> "]
[88.78803, "o", "                sys.exit(1)\r\n\u001b[?2004l\r"]
[88.788051, "o", "\u001b[?2004h> "]
[88.788121, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.788305, "o", "            with open(self.log_file_path, 'r', encoding='ISO-8859-1') as f:\r\n\u001b[?2004l\r"]
[88.788317, "o", "\u001b[?2004h> "]
[88.788429, "o", "                log_lines = f.readlines()\r\n\u001b[?2004l\r"]
[88.788439, "o", "\u001b[?2004h"]
[88.788464, "o", "> "]
[88.788506, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.788672, "o", "            filtered_entries = self.filter_log_entries(log_lines)\r\n\u001b[?2004l\r"]
[88.788681, "o", "\u001b[?2004h"]
[88.788694, "o", "> "]
[88.788755, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.788857, "o", "            if not filtered_entries:\r\n\u001b[?2004l\r"]
[88.78887, "o", "\u001b[?2004h> "]
[88.789022, "o", "                print(\"No queries matched the criteria.\")\r\n\u001b[?2004l\r"]
[88.789032, "o", "\u001b[?2004h"]
[88.789092, "o", "> "]
[88.789281, "o", "                with open(self.staging_file, 'w', encoding='ISO-8859-1', newline='\\r\\n') as f:\r\n\u001b[?2004l\r"]
[88.789317, "o", "\u001b[?2004h> "]
[88.789411, "o", "                    writer = csv.writer(f)\r\n\u001b[?2004l\r"]
[88.789624, "o", "\u001b[?2004h>                     writer.writerow(['timestamp', 'duration_ms', 'query_text'])\r\n\u001b[?2004l\r"]
[88.78966, "o", "\u001b[?2004h> "]
[88.789816, "o", "                staging_sha256 = self.compute_staging_file_sha256()\r\n\u001b[?2004l\r"]
[88.789826, "o", "\u001b[?2004h"]
[88.789869, "o", "> "]
[88.789984, "o", "                self.generate_report(staging_sha256, {\r\n\u001b[?2004l\r"]
[88.789993, "o", "\u001b[?2004h"]
[88.790062, "o", "> "]
[88.790133, "o", "                    'total_queries_extracted': 0,\r\n\u001b[?2004l\r"]
[88.790164, "o", "\u001b[?2004h> "]
[88.790262, "o", "                    'unique_patterns': 0\r\n\u001b[?2004l\r"]
[88.790271, "o", "\u001b[?2004h"]
[88.790282, "o", "> "]
[88.790345, "o", "                }, [])\r\n\u001b[?2004l\r"]
[88.790377, "o", "\u001b[?2004h> "]
[88.790433, "o", "                return"]
[88.790462, "o", "\r\n\u001b[?2004l\r"]
[88.790471, "o", "\u001b[?2004h"]
[88.790483, "o", "> "]
[88.79053, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h"]
[88.79054, "o", "> "]
[88.790673, "o", "            self.write_staging_csv(filtered_entries)\r\n\u001b[?2004l\r"]
[88.790683, "o", "\u001b[?2004h"]
[88.790726, "o", "> "]
[88.79087, "o", "            staging_sha256 = self.compute_staging_file_sha256()\r\n\u001b[?2004l\r"]
[88.790901, "o", "\u001b[?2004h> "]
[88.790932, "o", "            \r\n\u001b[?2004l\r"]
[88.790968, "o", "\u001b[?2004h> "]
[88.79115, "o", "            normalized_queries_map = self.process_queries(filtered_entries)\r\n\u001b[?2004l\r"]
[88.791161, "o", "\u001b[?2004h"]
[88.791212, "o", "> "]
[88.791254, "o", "            suggestions = []\r\n\u001b[?2004l\r"]
[88.791272, "o", "\u001b[?2004h> "]
[88.791313, "o", "            \r\n\u001b[?2004l\r"]
[88.791322, "o", "\u001b[?2004h"]
[88.791361, "o", "> "]
[88.791548, "o", "            for norm_sql, first_occurrence_timestamp in normalized_queries_map.items():\r\n\u001b[?2004l\r"]
[88.791583, "o", "\u001b[?2004h> "]
[88.791768, "o", "                table_name, columns = self.extract_table_and_columns(norm_sql)\r\n\u001b[?2004l\r"]
[88.791808, "o", "\u001b[?2004h> "]
[88.792001, "o", "                index_suggestion = self.generate_index_suggestion(table_name, columns)\r\n\u001b[?2004l\r"]
[88.792037, "o", "\u001b[?2004h> "]
[88.792128, "o", "                if index_suggestion:\r\n\u001b[?2004l\r"]
[88.792163, "o", "\u001b[?2004h> "]
[88.792254, "o", "                    suggestions.append({\r\n\u001b[?2004l\r"]
[88.792289, "o", "\u001b[?2004h> "]
[88.792462, "o", "                        \"id\": first_occurrence_timestamp.isoformat() + 'Z',\r\n\u001b[?2004l\r"]
[88.792496, "o", "\u001b[?2004h> "]
[88.792614, "o", "                        \"normalized_sql\": norm_sql,\r\n\u001b[?2004l\r"]
[88.79265, "o", "\u001b[?2004h> "]
[88.792787, "o", "                        \"index_suggestion\": index_suggestion\r\n\u001b[?2004l\r"]
[88.792825, "o", "\u001b[?2004h> "]
[88.792872, "o", "                    })\r\n\u001b[?2004l\r"]
[88.792882, "o", "\u001b[?2004h> "]
[88.792932, "o", "            \r\n\u001b[?2004l\r"]
[88.792941, "o", "\u001b[?2004h"]
[88.792978, "o", "> "]
[88.793089, "o", "            suggestions.sort(key=lambda x: x['id'])\r\n\u001b[?2004l\r"]
[88.793098, "o", "\u001b[?2004h"]
[88.793139, "o", "> "]
[88.793148, "o", "            \r\n\u001b[?2004l\r"]
[88.793189, "o", "\u001b[?2004h> "]
[88.793299, "o", "            self.generate_report(staging_sha256, {\r\n\u001b[?2004l\r"]
[88.793335, "o", "\u001b[?2004h> "]
[88.793485, "o", "                'total_queries_extracted': len(filtered_entries),\r\n\u001b[?2004l\r"]
[88.79352, "o", "\u001b[?2004h> "]
[88.79367, "o", "                'unique_patterns': len(normalized_queries_map)\r\n\u001b[?2004l\r"]
[88.793679, "o", "\u001b[?2004h> "]
[88.793762, "o", "            }, suggestions)\r\n\u001b[?2004l\r"]
[88.793797, "o", "\u001b[?2004h> "]
[88.793838, "o", "            \r\n\u001b[?2004l\r"]
[88.793847, "o", "\u001b[?2004h> "]
[88.793927, "o", "        except Exception as e:\r\n\u001b[?2004l\r"]
[88.793962, "o", "\u001b[?2004h> "]
[88.794036, "o", "            print(f\"Error: {e}\")\r\n\u001b[?2004l\r"]
[88.794052, "o", "\u001b[?2004h> "]
[88.794135, "o", "            import traceback\r\n\u001b[?2004l\r"]
[88.794171, "o", "\u001b[?2004h> "]
[88.794249, "o", "            traceback.print_exc()\r\n\u001b[?2004l\r"]
[88.794258, "o", "\u001b[?2004h"]
[88.794296, "o", "> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[88.79433, "o", "def main():\r\n\u001b[?2004l\r"]
[88.794366, "o", "\u001b[?2004h> "]
[88.79444, "o", "    analyzer = QueryAnalyzer()\r\n\u001b[?2004l\r"]
[88.794449, "o", "\u001b[?2004h"]
[88.794489, "o", "> "]
[88.794525, "o", "    analyzer.run()\r\n\u001b[?2004l\r"]
[88.794534, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[88.794577, "o", "\u001b[?2004h> "]
[88.794636, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r"]
[88.794671, "o", "\u001b[?2004h> "]
[88.794706, "o", "    main()\r\n\u001b[?2004l\r\u001b[?2004h"]
[88.79473, "o", "> "]
[88.794739, "o", "EOF\r\n\u001b[?2004l\r"]
[88.79633, "o", "\u001b[?2004h(env) root@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
[89.772062, "i", "chmod +x analyze_and_optimize.py\n"]
[89.77228, "o", "chmod +x analyze_and_optimize.py\r\n"]
[89.772291, "o", "\u001b[?2004l\r"]
[89.773664, "o", "\u001b[?2004h"]
[89.773683, "o", "(env) root@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
[89.900964, "i", "env/bin/python analyze_and_optimize.py --date 2023-03-15\n"]
[89.901162, "o", "env/bin/python analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[89.924683, "o", "Error: Log file not found at /path/to/your/log/file.log\r\n"]
[89.92783, "o", "\u001b[?2004h"]
[89.927877, "o", "(env) root@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
[90.897955, "i", "cat report.json\n"]
[90.898035, "o", "cat report.json\r\n"]
[90.898139, "o", "\u001b[?2004l\r"]
[90.898819, "o", "cat: report.json"]
[90.898882, "o", ": No such file or directory\r\n"]
[90.899068, "o", "\u001b[?2004h"]
[90.899086, "o", "(env) root@fcc061f5-83af-463b-b71b-727db5465fa5:/home/user/query_optimization# "]
