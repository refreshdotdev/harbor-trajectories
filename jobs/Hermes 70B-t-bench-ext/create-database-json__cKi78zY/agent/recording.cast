{"version": 2, "width": 160, "height": 40, "timestamp": 1771009542, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.005148, "o", "\u001b[?2004hroot@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user# "]
[0.90728, "i", "clear\r"]
[0.907332, "o", "clear\r\n\u001b[?2004l\r"]
[0.90836, "o", "\u001b[H\u001b[J"]
[0.908497, "o", "\u001b[?2004h"]
[0.908519, "o", "root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user# "]
[15.421927, "i", "mkdir -p /home/user/query_optimization/staging\n"]
[15.422101, "o", "mkdir -p /home/user/query_optimization/staging\r\n\u001b[?2004l\r"]
[15.423832, "o", "\u001b[?2004h"]
[15.42386, "o", "root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user# "]
[15.549084, "i", "cd /home/user/query_optimization\n"]
[15.549237, "o", "cd /home/user/query_optimization\r\n\u001b[?2004l\r"]
[15.549314, "o", "\u001b[?2004h"]
[15.549324, "o", "root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[15.750585, "i", "python3 -m venv env\n"]
[15.750707, "o", "python3 -m venv env\r\n\u001b[?2004l\r"]
[16.691312, "i", "source env/bin/activate\n"]
[16.691347, "o", "source env/bin/activate\r\n"]
[16.897183, "i", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\n"]
[16.897309, "o", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\r\n"]
[17.698163, "o", "\u001b[?2004hroot@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[17.698293, "o", "source env/bin/activate\r\n\u001b[?2004l\r"]
[17.698502, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[17.698642, "o", "pip install sqlparse==0.4.4 psycopg2-binary==2.9.7\r\n\u001b[?2004l\r"]
[17.965524, "o", "Collecting sqlparse==0.4.4\r\n"]
[17.995602, "o", "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\r\n"]
[17.995841, "o", "\u001b[?25l"]
[17.996811, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.2 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[18.001237, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[18.107757, "o", "Collecting psycopg2-binary==2.9.7\r\n"]
[18.114685, "o", "  Downloading psycopg2_binary-2.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\r\n"]
[18.114864, "o", "\u001b[?25l"]
[18.1156, "o", "     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"]
[18.150364, "o", "\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/3.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"]
[18.162615, "o", "\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n\u001b[?25h"]
[18.183624, "o", "Installing collected packages: sqlparse, psycopg2-binary\r\n"]
[18.252894, "o", "Successfully installed psycopg2-binary-2.9.7 sqlparse-0.4.4\r\n"]
[18.298568, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[102.94587, "i", "cat > analyze_and_optimize.py << 'EOF'\n#!/usr/bin/env python3\n\"\"\"\nSQL Query Log Analysis & Optimization Script\n\"\"\"\n\nimport os\nimport sys\nimport csv\nimport re\nimport json\nimport hashlib\nimport argparse\nfrom configparser import ConfigParser\nfrom sqlparse import format as sql_format, tokens as sql_tokens\nfrom sqlparse.sql import Identifier, Comparison, Parenthesis, Where\nfrom sqlparse.sql import Token, Function, Keyword, DML\n\n# --- Configuration & Argument Parsing ---\nCONFIG_FILE = \"/home/user/db_tools/db_config.ini\"\nSTAGING_FILE = \"/home/user/query_optimization/staging/raw_extract.csv\"\nREPORT_FILE = \"/home/user/query_optimization/report.json\"\n\ndef read_config():\n    \"\"\"Read configuration from INI file.\"\"\"\n    parser = ConfigParser()\n    parser.read(CONFIG_FILE)\n    config = {\n        'log_file_path': parser.get('DEFAULT', 'log_file_path', fallback=''),\n        'min_duration_ms': parser.getint('DEFAULT', 'min_duration_ms', fallback=0)\n    }\n    return config\n\ndef parse_duration(duration_str):\n    \"\"\"Parse durati"]
[102.946, "i", "on string (e.g., '15ms') to milliseconds.\"\"\"\n    match = re.search(r'^([0-9.]+)ms$', duration_str.strip())\n    if match:\n        return float(match.group(1))\n    return 0.0\n\ndef normalize_sql(sql_query):\n    \"\"\"Normalize SQL query using sqlparse.\"\"\"\n    try:\n        # sqlparse.format can raise exceptions, handle them\n        formatted = sql_format(\n            sql_query,\n            keyword_case='upper',\n            strip_comments=True,\n            reindent=False,\n            truncate_strings=1000  # Truncate very long strings for safety\n        )\n        # Remove trailing semicolon if present\n        if formatted.endswith(';'):\n            formatted = formatted[:-1].strip()\n        return formatted\n    except Exception as e:\n        # In case of parsing errors, return the original query\n        print(f\"Warning: Could not format SQL: {sql_query}. Error: {e}\", file=sys.stderr)\n        return sql_query\n\ndef extract_table_and_columns(query_text):\n    \"\"\"Extract table and columns from WHERE clause for index sugge"]
[102.946021, "o", "cat > analyze_and_optimize.py << 'EOF'\r\n\u001b[?2004l\r"]
[102.946045, "i", "stion.\"\"\"\n    table_name = None\n    columns = []\n    try:\n        parsed = sqlparse.parse(query_text)[0]\n        # Find FROM clause to get table name (simple approach)\n        from_seen = False\n        for token in parsed.tokens:\n            if from_seen and isinstance(token, Identifier):\n                table_name = token.get_real_name()\n                break\n            if isinstance(token, Keyword) and token.normalized == 'FROM':\n                from_seen = True\n        \n        # Find WHERE clause\n        where_clause = None\n        for token in parsed.tokens:\n            if isinstance(token, Where):\n                where_clause = token\n                break\n        \n        if where_clause:\n            # Extract conditions from WHERE clause\n            conditions = []\n            # Simple recursive walk to find comparisons\n            def walk_tokens(tokens):\n                for token in tokens:\n                    if isinstance(token, Comparison) and token.tokens:\n                        # Check if it's"]
[102.946083, "i", " an equality comparison\n                        if len(token.tokens) >= 3 and \\\n                           isinstance(token.tokens[1], sql_tokens.Operator) and \\\n                           token.tokens[1].normalized == '=':\n                            # token.tokens[0] is the left side (column), token.tokens[2] is the right side\n                            # Try to get the column name from the left side\n                            col_token = token.tokens[0]\n                            if isinstance(col_token, Identifier):\n                                col_name = col_token.get_real_name()\n                                conditions.append(col_name)\n                            elif isinstance(col_token, Function) and col_token.get_type() == 'name':\n                                # Try to extract name from function-like identifiers\n                                col_name = col_token.get_name()\n                                conditions.append(col_name)\n                            elif isinstance(col_token, T"]
[102.946107, "o", "stion.\"\"\"\r\n    table_name = None\r\n    columns = []\r\n    try:\r\n        parsed = sqlparse.parse(query_text)[0]\r\n        # Find FROM clause to get table name (simple approach)\r\n        from_seen = False\r\n        for token in parsed.tokens:\r\n            if from_seen and isinstance(token, Identifier)"]
[102.946153, "i", "oken) and col_token.ttype is not None:\n                                # Fallback to text representation\n                                col_name = col_token.value.strip()\n                                if col_name:\n                                    conditions.append(col_name)\n                    elif isinstance(token, Parenthesis):\n                        walk_tokens(token.tokens[1:-1])  # Skip parentheses tokens themselves\n                    elif hasattr(token, 'tokens'):\n                        walk_tokens(token.tokens)\n            \n            walk_tokens(where_clause.tokens)\n            \n            # Filter out non-column like tokens and duplicates\n            seen_columns = set()\n            for cond in conditions:\n                # Basic check if it looks like a column name (alphanumeric plus underscores)\n                if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', cond):\n                    if cond not in seen_columns:\n                        seen_columns.add(cond)\n                        columns.app"]
[102.94617, "o", ":\r\n                table_name = token.get_real_name()\r\n                break\r\n            if isinstance(token, Keyword) and token.normalized == 'FROM':\r\n                from_seen = True\r\n        \r\n        # Find WHERE clause\r\n        where_clause = None\r\n        for token in parsed.tokens:\r\n            if isinstance(token, Where):\r\n                where_clause = token\r\n     "]
[102.946187, "i", "end(cond)\n    except Exception as e:\n        print(f\"Warning: Could not extract table/columns from: {query_text}. Error: {e}\", file=sys.stderr)\n    \n    return table_name, columns\n\ndef suggest_index(table_name, columns):\n    \"\"\"Suggest an index creation SQL statement.\"\"\"\n    if not table_name or not columns:\n        return None\n    \n    # Sanitize column names for index name\n    safe_cols_for_name = [re.sub(r'[^a-zA-Z0-9_]', '', col) for col in columns]\n    index_name = f\"idx_{table_name}_{'_'.join(safe_cols_for_name)}\"\n    \n    # Create a safe index name, truncate if too long\n    index_name = index_name[:63]  # Max length for Postgres index names\n    index_name = index_name[:63].strip('_')  # Ensure it doesn't end with an underscore\n    \n    return f\"CREATE INDEX {index_name} ON {table_name}({', '.join(columns)});\"\n\ndef calculate_sha256(filepath):\n    \"\"\"Calculate SHA-256 checksum of a file.\"\"\"\n    sha256 = hashlib.sha256()\n    with open(filepath, 'rb') as f:\n        while True:\n            data = f.read(655"]
[102.946201, "o", "           break\r\n        \r\n        if where_clause:\r\n            # Extract conditions "]
[102.94622, "i", "36)  # Read in 64k chunks\n            if not data:\n                break\n            sha256.update(data)\n    return sha256.hexdigest()\n\ndef main():\n    parser = argparse.ArgumentParser(description='SQL Query Log Analysis & Optimization')\n    parser.add_argument('--date', required=True, help='Date to analyze (YYYY-MM-DD)')\n    args = parser.parse_args()\n    \n    target_date = args.date\n    \n    print(f\"Starting analysis for date: {target_date}\")\n    \n    config = read_config()\n    log_file_path = config['log_file_path']\n    min_duration_ms = config['min_duration_ms']\n    \n    print(f\"Config: log_file='{log_file_path}', min_duration_ms={min_duration_ms}\")\n    \n    # Create staging directory if it doesn't exist\n    staging_dir = os.path.dirname(STAGING_FILE)\n    if not os.path.exists(staging_dir):\n        os.makedirs(staging_dir, exist_ok=True)\n        print(f\"Created directory: {staging_dir}\")\n    \n    extracted_queries = []\n    \n    if not os.path.exists(log_file_path):\n        print(f\"Error: Log file not foun"]
[102.946235, "o", "from WHERE clause\r\n            conditions = []\r\n            # Simple recursive walk to find comparisons\r\n            def walk_tokens(tokens):\r\n                for token in tokens:\r\n                    if isinstance(token, Comparison) and token.tokens"]
[102.946263, "i", "d at '{log_file_path}'. Cannot continue.\", file=sys.stderr)\n        sys.exit(1)\n    \n    print(f\"Processing log file: {log_file_path}\")\n    \n    with open(log_file_path, 'r', encoding='utf-8') as log_f:\n        # Prepare CSV writer\n        with open(STAGING_FILE, 'w', encoding='ISO-8859-1', newline='') as csv_f:\n            csv_writer = csv.writer(csv_f, lineterminator='\\r\\n') # CRLF line endings\n            csv_writer.writerow(['timestamp', 'duration_ms', 'query_text']) # Header\n            \n            for line_num, line in enumerate(log_f, 1):\n                line = line.strip()\n                if not line:\n                    continue\n                \n                parts = [p.strip() for p in line.split('|')]\n                if len(parts) != 4:\n                    print(f\"Warning: Skipping malformed line {line_num}: '{line}'\")\n                    continue\n                \n                timestamp, duration_str, level, message = parts\n                \n                # 1. Date Match\n                try:"]
[102.946275, "o", ":\r\n                        # Check if it's an equality comparison\r\n                        if len(token.tokens) >= 3 and \\\r\n                           isinstance(token.tokens[1], sql_tokens.Operator) and \\\r\n                           token.tokens[1].normalized == '='"]
[102.94631, "i", "\n                    # Use the first 10 chars for YYYY-MM-DD\n                    log_date = timestamp[:10]\n                    if log_date != target_date:\n                        continue\n                except Exception:\n                    print(f\"Warning: Skipping line {line_num} due to date format error: '{timestamp}'\")\n                    continue\n                \n                # 2. Type Match\n                if level != '[QUERY]':\n                    continue\n                \n                # 3. Performance Threshold\n                duration_val = parse_duration(duration_str)\n                if duration_val < min_duration_ms:\n                    continue\n                \n                # If all conditions met, write to CSV\n                # Duration for CSV is just the numeric part\n                csv_writer.writerow([timestamp, duration_val, message])\n                extracted_queries.append({'timestamp': timestamp, 'query_text': message})\n            \n    print(f\"Extracted {len(extracted_queries)}"]
[102.946325, "o", ":\r\n                            # token.tokens[0] is the left side (column), token.tokens[2] is the right side\r\n                            # Try to get the column name from the left side\r\n                            col_token = token.tokens[0]\r\n      "]
[102.946341, "i", " queries to {STAGING_FILE}\")\n    \n    # Phase 2: Analysis & Normalization\n    staging_file_sha256 = calculate_sha256(STAGING_FILE)\n    print(f\"SHA-256 of staging file: {staging_file_sha256}\")\n    \n    # Process queries for normalization, deduplication, and index suggestions\n    normalized_queries = {}\n    \n    for item in extracted_queries:\n        original_query = item['query_text']\n        normalized_query_text = normalize_sql(original_query)\n        \n        # Use normalized text as key for deduplication\n        if normalized_query_text not in normalized_queries:\n            table_name, columns = extract_table_and_columns(normalized_query_text)\n            index_suggestion = suggest_index(table_name, columns)\n            \n            normalized_queries[normalized_query_text] = {\n                'first_timestamp': item['timestamp'], # Store first occurrence timestamp\n                'count': 1,\n                'normalized_sql': normalized_query_text,\n                'index_suggestion': index_suggestion\n    "]
[102.946356, "o", "                      if isinstance(col_token, Identifier):\r\n                                col_name = col_token.get_real_name()\r\n          "]
[102.946379, "i", "        }\n        else:\n            # Increment count for already seen query\n            normalized_queries[normalized_query_text]['count'] += 1\n    \n    # Prepare suggestions for report, sorted by first_timestamp\n    suggestions_list = sorted(\n        normalized_queries.values(), \n        key=lambda x: x['first_timestamp']\n    )\n    \n    total_queries_extracted = len(extracted_queries)\n    unique_patterns = len(suggestions_list)\n    \n    print(f\"Analysis complete. Total extracted: {total_queries_extracted}, Unique patterns: {unique_patterns}\")\n    \n    # Phase 3: Reporting\n    report_data = {\n        \"meta\": {\n            \"date_analyzed\": target_date,\n            \"staging_file_sha256\": staging_file_sha256\n        },\n        \"stats\": {\n            \"total_queries_extracted\": total_queries_extracted,\n            \"unique_patterns\": unique_patterns\n        },\n        \"suggestions\": []\n    }\n    \n    for suggestion in suggestions_list:\n        report_data[\"suggestions\"].append({\n           \"id\": suggestion['first_"]
[102.946392, "o", "                      conditions.append(col_name)\r\n                            elif isinstance(col_token, Function) and col_token.get_type() == 'name':\r\n                                # Try to extract name from function-"]
[102.946407, "i", "timestamp'],\n           \"normalized_sql\": suggestion['normalized_sql'],\n           \"index_suggestion\": suggestion['index_suggestion']\n        })\n    \n    with open(REPORT_FILE, 'w', encoding='utf-8', newline='\\n') as report_f: # Unix line endings\n        json.dump(report_data, report_f, indent=2)\n    \n    print(f\"Report generated at: {REPORT_FILE}\")\n\nif __name__ == \"__main__\":\n    main()\nEOF\n"]
[102.946422, "o", "like identifiers\r\n                                col_name = col_token.get_name()\r\n                                conditions.append(col_name)\r\n                            elif isinstance(col_token, Token) and col_token.ttype is not"]
[102.946441, "o", " None:\u001b[?2004h"]
[102.94646, "o", "> "]
[102.946526, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[102.946543, "o", "\u001b[?2004h> "]
[102.946562, "o", "\"\"\"\r\n\u001b[?2004l\r"]
[102.946578, "o", "\u001b[?2004h> "]
[102.946719, "o", "SQL Query Log Analysis & Optimization Script\r\n\u001b[?2004l\r"]
[102.946736, "o", "\u001b[?2004h> "]
[102.946757, "o", "\"\"\"\r\n\u001b[?2004l\r"]
[102.946773, "o", "\u001b[?2004h> "]
[102.946788, "o", "\r\n\u001b[?2004l\r"]
[102.946815, "o", "\u001b[?2004h> "]
[102.946833, "o", "import os\r\n\u001b[?2004l\r"]
[102.946861, "o", "\u001b[?2004h> "]
[102.946891, "o", "import sys\r\n\u001b[?2004l\r"]
[102.946908, "o", "\u001b[?2004h> "]
[102.946949, "o", "import csv\r\n\u001b[?2004l\r"]
[102.946971, "o", "\u001b[?2004h> "]
[102.947007, "o", "import re\r\n\u001b[?2004l\r"]
[102.947024, "o", "\u001b[?2004h> "]
[102.947065, "o", "import json\r\n\u001b[?2004l\r"]
[102.94708, "o", "\u001b[?2004h> "]
[102.947158, "o", "import hashlib\r\n\u001b[?2004l\r"]
[102.947177, "o", "\u001b[?2004h> "]
[102.947235, "o", "import argparse\r\n\u001b[?2004l\r"]
[102.947249, "o", "\u001b[?2004h> "]
[102.947372, "o", "from configparser import ConfigParser\r\n\u001b[?2004l\r"]
[102.947389, "o", "\u001b[?2004h> "]
[102.947578, "o", "from sqlparse import format as sql_format, tokens as sql_tokens\r\n\u001b[?2004l\r"]
[102.947594, "o", "\u001b[?2004h> "]
[102.947802, "o", "from sqlparse.sql import Identifier, Comparison, Parenthesis, Where\r\n\u001b[?2004l\r"]
[102.947824, "o", "\u001b[?2004h> "]
[102.947987, "o", "from sqlparse.sql import Token, Function, Keyword, DML\r\n\u001b[?2004l\r"]
[102.948003, "o", "\u001b[?2004h> "]
[102.94802, "o", "\r\n\u001b[?2004l\r"]
[102.948035, "o", "\u001b[?2004h> "]
[102.948183, "o", "# --- Configuration & Argument Parsing ---\r\n\u001b[?2004l\r\u001b[?2004h"]
[102.94821, "o", "> "]
[102.948347, "o", "CONFIG_FILE = \"/home/user/db_tools/db_config.ini\"\r\n\u001b[?2004l\r"]
[102.948381, "o", "\u001b[?2004h"]
[102.948402, "o", "> "]
[102.94858, "o", "STAGING_FILE = \"/home/user/query_optimization/staging/raw_extract.csv\"\r\n\u001b[?2004l\r"]
[102.948598, "o", "\u001b[?2004h> "]
[102.948774, "o", "REPORT_FILE = \"/home/user/query_optimization/report.json\"\r\n\u001b[?2004l\r"]
[102.948794, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[102.948813, "o", "\u001b[?2004h> "]
[102.948878, "o", "def read_config():\r\n\u001b[?2004l\r"]
[102.948899, "o", "\u001b[?2004h> "]
[102.949067, "o", "    \"\"\"Read configuration from INI file.\"\"\"\r\n\u001b[?2004l\r\u001b[?2004h> "]
[102.949142, "o", "    parser = ConfigParser()\r\n\u001b[?2004l\r"]
[102.94917, "o", "\u001b[?2004h> "]
[102.949248, "o", "    parser.read(CONFIG_FILE)\r\n\u001b[?2004l\r"]
[102.949274, "o", "\u001b[?2004h> "]
[102.949318, "o", "    config = {\r\n\u001b[?2004l\r"]
[102.949341, "o", "\u001b[?2004h> "]
[102.949568, "o", "        'log_file_path': parser.get('DEFAULT', 'log_file_path', fallback=''),\r\n\u001b[?2004l\r"]
[102.949597, "o", "\u001b[?2004h> "]
[102.949842, "o", "        'min_duration_ms': parser.getint('DEFAULT', 'min_duration_ms', fallback=0)\r\n\u001b[?2004l\r"]
[102.949854, "o", "\u001b[?2004h"]
[102.949899, "o", "> "]
[102.949926, "o", "    }\r\n\u001b[?2004l\r\u001b[?2004h> "]
[102.949959, "o", "    return config\r\n\u001b[?2004l\r"]
[102.949982, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[102.950009, "o", "\u001b[?2004h> "]
[102.95011, "o", "def parse_duration(duration_str):\r\n\u001b[?2004l\r"]
[102.950144, "o", "\u001b[?2004h> "]
[102.950333, "o", "    \"\"\"Parse duration string (e.g., '15ms') to milliseconds.\"\"\"\r\n\u001b[?2004l\r"]
[102.950358, "o", "\u001b[?2004h> "]
[102.950543, "o", "    match = re.search(r'^([0-9.]+)ms$', duration_str.strip())\r\n\u001b[?2004l\r"]
[102.950567, "o", "\u001b[?2004h> "]
[102.950607, "o", "    if match:\r\n\u001b[?2004l\r"]
[102.950632, "o", "\u001b[?2004h> "]
[102.95074, "o", "        return float(match.group(1))\r\n\u001b[?2004l\r"]
[102.950769, "o", "\u001b[?2004h"]
[102.950796, "o", "> "]
[102.950824, "o", "    return 0.0\r\n\u001b[?2004l\r\u001b[?2004h> "]
[102.950847, "o", "\r\n\u001b[?2004l\r\u001b[?2004h> "]
[102.950939, "o", "def normalize_sql(sql_query):\r\n\u001b[?2004l\r"]
[102.950962, "o", "\u001b[?2004h> "]
[102.951097, "o", "    \"\"\"Normalize SQL query using sqlparse.\"\"\"\r\n\u001b[?2004l\r"]
[102.951127, "o", "\u001b[?2004h> "]
[102.951153, "o", "    try:\r\n\u001b[?2004l\r"]
[102.951173, "o", "\u001b[?2004h> "]
[102.951353, "o", "        # sqlparse.format can raise exceptions, handle them\r\n\u001b[?2004l\r"]
[102.951365, "o", "\u001b[?2004h"]
[102.951391, "o", "> "]
[102.95147, "o", "        formatted = sql_format(\r\n\u001b[?2004l\r"]
[102.951488, "o", "\u001b[?2004h> "]
[102.951561, "o", "            sql_query,\r\n\u001b[?2004l\r"]
[102.951585, "o", "\u001b[?2004h> "]
[102.951684, "o", "            keyword_case='upper',\r\n\u001b[?2004l\r"]
[102.951716, "o", "\u001b[?2004h> "]
[102.951806, "o", "            strip_comments=True,\r\n\u001b[?2004l\r"]
[102.951829, "o", "\u001b[?2004h> "]
[102.951917, "o", "            reindent=False,\r\n\u001b[?2004l\r"]
[102.951943, "o", "\u001b[?2004h> "]
[102.952171, "o", "            truncate_strings=1000  # Truncate very long strings for safety\r\n\u001b[?2004l\r"]
[102.952197, "o", "\u001b[?2004h"]
[102.95222, "o", ">         )\r\n"]
[102.952243, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[102.952386, "o", "        # Remove trailing semicolon if present\r\n\u001b[?2004l\r"]
[102.952409, "o", "\u001b[?2004h> "]
[102.952512, "o", "        if formatted.endswith(';'):\r\n\u001b[?2004l\r"]
[102.952535, "o", "\u001b[?2004h> "]
[102.952671, "o", "            formatted = formatted[:-1].strip()\r\n\u001b[?2004l\r"]
[102.952702, "o", "\u001b[?2004h"]
[102.95273, "o", "> "]
[102.952772, "o", "        return formatted\r\n\u001b[?2004l\r"]
[102.952801, "o", "\u001b[?2004h> "]
[102.95287, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[102.952893, "o", "\u001b[?2004h> "]
[102.953074, "o", "        # In case of parsing errors, return the original query\r\n\u001b[?2004l\r"]
[102.953097, "o", "\u001b[?2004h> "]
[102.953366, "o", "        print(f\"Warning: Could not format SQL: {sql_query}. Error: {e}\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[102.95339, "o", "\u001b[?2004h> "]
[102.95346, "o", "        return sql_query\r\n\u001b[?2004l\r"]
[102.953483, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[102.953506, "o", "\u001b[?2004h> "]
[102.953629, "o", "def extract_table_and_columns(query_text):\r\n\u001b[?2004l\r"]
[102.953655, "o", "\u001b[?2004h> "]
[102.953873, "o", "    \"\"\"Extract table and columns from WHERE clause for index suggestion.\"\"\"\r\n\u001b[?2004l\r"]
[102.953899, "o", "\u001b[?2004h> "]
[102.953959, "o", "    table_name = None\r\n\u001b[?2004l\r"]
[102.953981, "o", "\u001b[?2004h> "]
[102.95403, "o", "    columns = []\r\n\u001b[?2004l\r"]
[102.954053, "o", "\u001b[?2004h> "]
[102.954078, "o", "    try:\r\n\u001b[?2004l\r"]
[102.9541, "o", "\u001b[?2004h> "]
[102.954246, "o", "        parsed = sqlparse.parse(query_text)[0]\r\n\u001b[?2004l\r"]
[102.954269, "o", "\u001b[?2004h> "]
[102.954451, "o", "        # Find FROM clause to get table name (simple approach)\r\n\u001b[?2004l\r"]
[102.954475, "o", "\u001b[?2004h> "]
[102.954565, "o", "        from_seen = False\r\n\u001b[?2004l\r\u001b[?2004h"]
[102.954605, "o", "> "]
[102.954678, "o", "        for token in parsed.tokens:\r\n\u001b[?2004l\r"]
[102.954715, "o", "\u001b[?2004h> "]
[102.954873, "o", "            if from_seen and isinstance(token, Identifier):\r\n\u001b[?2004l\r"]
[102.9549, "o", "\u001b[?2004h> "]
[102.955046, "o", "                table_name = token.get_real_name()\r\n\u001b[?2004l\r"]
[102.955071, "o", "\u001b[?2004h> "]
[102.955137, "o", "                break\r\n\u001b[?2004l\r"]
[102.955161, "o", "\u001b[?2004h> "]
[102.955381, "o", "            if isinstance(token, Keyword) and token.normalized == 'FROM':\r\n\u001b[?2004l\r"]
[102.955408, "o", "\u001b[?2004h> "]
[102.955497, "o", "                from_seen = True\r\n\u001b[?2004l\r"]
[102.955533, "o", "\u001b[?2004h"]
[102.955561, "o", ">         \r\n\u001b[?2004l\r\u001b[?2004h> "]
[102.95565, "o", "        # Find WHERE clause\r\n\u001b[?2004l\r"]
[102.955678, "o", "\u001b[?2004h> "]
[102.955765, "o", "        where_clause = None\r\n\u001b[?2004l\r"]
[102.9558, "o", "\u001b[?2004h> "]
[102.955889, "o", "        for token in parsed.tokens:\r\n\u001b[?2004l\r"]
[102.955915, "o", "\u001b[?2004h> "]
[102.956031, "o", "            if isinstance(token, Where):\r\n\u001b[?2004l\r"]
[102.956055, "o", "\u001b[?2004h> "]
[102.956212, "o", "                where_clause = token\r\n\u001b[?2004l\r\u001b[?2004h> "]
[102.956244, "o", "                break\r\n\u001b[?2004l\r"]
[102.956268, "o", "\u001b[?2004h> "]
[102.956295, "o", "        \r\n\u001b[?2004l\r"]
[102.956318, "o", "\u001b[?2004h> "]
[102.95639, "o", "        if where_clause:\r\n\u001b[?2004l\r"]
[102.956413, "o", "\u001b[?2004h> "]
[102.956563, "o", "            # Extract conditions from WHERE clause\r\n\u001b[?2004l\r"]
[102.956588, "o", "\u001b[?2004h> "]
[102.95667, "o", "            conditions = []\r\n\u001b[?2004l\r"]
[102.956694, "o", "\u001b[?2004h> "]
[102.956858, "o", "            # Simple recursive walk to find comparisons\r\n\u001b[?2004l\r"]
[102.95687, "o", "\u001b[?2004h"]
[102.956937, "o", "> "]
[102.956986, "o", "            def walk_tokens(tokens):\r\n\u001b[?2004l\r"]
[102.957005, "o", "\u001b[?2004h"]
[102.957023, "o", "> "]
[102.957119, "o", "                for token in tokens:\r\n\u001b[?2004l\r"]
[102.95714, "o", "\u001b[?2004h"]
[102.957157, "o", "> "]
[102.957348, "o", "                    if isinstance(token, Comparison) and token.tokens:\r\n\u001b[?2004l\r"]
[102.957368, "o", "\u001b[?2004h"]
[102.957384, "o", "> "]
[102.957553, "o", "                        # Check if it's an equality comparison\r\n\u001b[?2004l\r"]
[102.95757, "o", "\u001b[?2004h> "]
[102.95774, "o", "                        if len(token.tokens) >= 3 and \\\r\n\u001b[?2004l\r"]
[102.957753, "o", "\u001b[?2004h"]
[102.957764, "o", "> "]
[102.958001, "o", "                           isinstance(token.tokens[1], sql_tokens.Operator) and \\\r\n\u001b[?2004l\r"]
[102.958021, "o", "\u001b[?2004h> "]
[102.958215, "o", "                           token.tokens[1].normalized == '=':\r\n\u001b[?2004l\r"]
[102.958239, "o", "\u001b[?2004h> "]
[102.958543, "o", "                            # token.tokens[0] is the left side (column), token.tokens[2] is the right side\r\n\u001b[?2004l\r"]
[102.958567, "o", "\u001b[?2004h> "]
[102.958786, "o", "                            # Try to get the column name from the left side\r\n\u001b[?2004l\r"]
[102.958802, "o", "\u001b[?2004h"]
[102.958819, "o", "> "]
[102.958973, "o", "                            col_token = token.tokens[0]\r\n\u001b[?2004l\r"]
[102.958989, "o", "\u001b[?2004h"]
[102.959005, "o", "> "]
[102.959194, "o", "                            if isinstance(col_token, Identifier):\r\n\u001b[?2004l\r"]
[102.959213, "o", "\u001b[?2004h"]
[102.959229, "o", "> "]
[102.959416, "o", "                                col_name = col_token.get_real_name()\r\n\u001b[?2004l\r"]
[102.959434, "o", "\u001b[?2004h"]
[102.95945, "o", "> "]
[102.959612, "o", "                                conditions.append(col_name)\r\n\u001b[?2004l\r"]
[102.959632, "o", "\u001b[?2004h"]
[102.959649, "o", "> "]
[102.959928, "o", "                            elif isinstance(col_token, Function) and col_token.get_type() == 'name':\r\n\u001b[?2004l\r"]
[102.959945, "o", "\u001b[?2004h"]
[102.959962, "o", "> "]
[102.960202, "o", "                                # Try to extract name from function-like identifiers\r\n\u001b[?2004l\r"]
[102.960222, "o", "\u001b[?2004h"]
[102.960239, "o", "> "]
[102.96041, "o", "                                col_name = col_token.get_name()\r\n\u001b[?2004l\r"]
[102.960422, "o", "\u001b[?2004h"]
[102.960439, "o", "> "]
[102.960605, "o", "                                conditions.append(col_name)\r\n\u001b[?2004l\r"]
[102.960623, "o", "\u001b[?2004h"]
[102.960644, "o", "> "]
[102.960902, "o", "                            elif isinstance(col_token, Token) and col_token.ttype is not None:\r\n\u001b[?2004l\r"]
[102.960925, "o", "\u001b[?2004h> "]
[102.96112, "o", "                                # Fallback to text representation\r\n\u001b[?2004l\r"]
[102.961138, "o", "\u001b[?2004h"]
[102.961154, "o", "> "]
[102.961335, "o", "                                col_name = col_token.value.strip()\r\n\u001b[?2004l\r"]
[102.961356, "o", "\u001b[?2004h> "]
[102.961491, "o", "                                if col_name:\r\n\u001b[?2004l\r"]
[102.961514, "o", "\u001b[?2004h> "]
[102.961699, "o", "                                    conditions.append(col_name)\r\n\u001b[?2004l\r"]
[102.961718, "o", "\u001b[?2004h> "]
[102.961889, "o", "                    elif isinstance(token, Parenthesis):\r\n\u001b[?2004l\r"]
[102.961905, "o", "\u001b[?2004h> "]
[102.962192, "o", "                        walk_tokens(token.tokens[1:-1])  # Skip parentheses tokens themselves\r\n\u001b[?2004l\r"]
[102.962211, "o", "\u001b[?2004h"]
[102.962227, "o", "> "]
[102.962362, "o", "                    elif hasattr(token, 'tokens'):\r\n\u001b[?2004l\r"]
[102.96238, "o", "\u001b[?2004h"]
[102.962401, "o", "> "]
[102.962531, "o", "                        walk_tokens(token.tokens)\r\n\u001b[?2004l\r"]
[102.962547, "o", "\u001b[?2004h"]
[102.962565, "o", "> "]
[102.962595, "o", "            \r\n\u001b[?2004l\r"]
[102.96261, "o", "\u001b[?2004h"]
[102.962627, "o", "> "]
[102.96275, "o", "            walk_tokens(where_clause.tokens)\r\n\u001b[?2004l\r"]
[102.96277, "o", "\u001b[?2004h> "]
[102.962814, "o", "            \r\n\u001b[?2004l\r"]
[102.96283, "o", "\u001b[?2004h"]
[102.962847, "o", "> "]
[102.963021, "o", "            # Filter out non-column like tokens and duplicates\r\n\u001b[?2004l\r"]
[102.963039, "o", "\u001b[?2004h"]
[102.963055, "o", "> "]
[102.963147, "o", "            seen_columns = set()\r\n\u001b[?2004l\r"]
[102.963166, "o", "\u001b[?2004h"]
[102.963182, "o", "> "]
[102.963278, "o", "            for cond in conditions:\r\n\u001b[?2004l\r"]
[102.963296, "o", "\u001b[?2004h"]
[102.963313, "o", "> "]
[102.963573, "o", "                # Basic check if it looks like a column name (alphanumeric plus underscores)\r\n\u001b[?2004l\r"]
[102.963593, "o", "\u001b[?2004h> "]
[102.963781, "o", "                if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', cond):\r\n\u001b[?2004l\r"]
[102.963796, "o", "\u001b[?2004h"]
[102.963814, "o", "> "]
[102.963948, "o", "                    if cond not in seen_columns:\r\n\u001b[?2004l\r"]
[102.963965, "o", "\u001b[?2004h"]
[102.963983, "o", "> "]
[102.964112, "o", "                        seen_columns.add(cond)\r\n\u001b[?2004l\r"]
[102.964132, "o", "\u001b[?2004h"]
[102.964138, "o", "> "]
[102.964281, "o", "                        columns.append(cond)\r\n\u001b[?2004l\r"]
[102.964323, "o", "\u001b[?2004h"]
[102.964348, "o", "> "]
[102.964387, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[102.96441, "o", "\u001b[?2004h"]
[102.96443, "o", "> "]
[102.964729, "o", "        print(f\"Warning: Could not extract table/columns from: {query_text}. Error: {e}\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[102.964749, "o", "\u001b[?2004h"]
[102.964768, "o", ">     \r\n"]
[102.964791, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[102.964885, "o", "    return table_name, columns\r\n\u001b[?2004l\r"]
[102.964901, "o", "\u001b[?2004h"]
[102.964919, "o", "> \r\n\u001b[?2004l\r"]
[102.964936, "o", "\u001b[?2004h> "]
[102.965052, "o", "def suggest_index(table_name, columns):\r\n\u001b[?2004l\r"]
[102.965069, "o", "\u001b[?2004h> "]
[102.965221, "o", "    \"\"\"Suggest an index creation SQL statement.\"\"\"\r\n\u001b[?2004l\r"]
[102.965242, "o", "\u001b[?2004h> "]
[102.965356, "o", "    if not table_name or not columns:\r\n\u001b[?2004l\r"]
[102.965371, "o", "\u001b[?2004h> "]
[102.965436, "o", "        return None\r\n\u001b[?2004l\r"]
[102.965451, "o", "\u001b[?2004h"]
[102.965475, "o", ">     \r\n\u001b[?2004l\r"]
[102.965494, "o", "\u001b[?2004h"]
[102.965511, "o", "> "]
[102.965624, "o", "    # Sanitize column names for index name\r\n\u001b[?2004l\r"]
[102.965644, "o", "\u001b[?2004h> "]
[102.96588, "o", "    safe_cols_for_name = [re.sub(r'[^a-zA-Z0-9_]', '', col) for col in columns]\r\n\u001b[?2004l\r"]
[102.965909, "o", "\u001b[?2004h> "]
[102.966099, "o", "    index_name = f\"idx_{table_name}_{'_'.join(safe_cols_for_name)}\"\r\n\u001b[?2004l\r"]
[102.966121, "o", "\u001b[?2004h> "]
[102.966142, "o", "    \r\n\u001b[?2004l\r"]
[102.966162, "o", "\u001b[?2004h"]
[102.966181, "o", "> "]
[102.966321, "o", "    # Create a safe index name, truncate if too long\r\n\u001b[?2004l\r"]
[102.96634, "o", "\u001b[?2004h> "]
[102.966557, "o", "    index_name = index_name[:63]  # Max length for Postgres index names\r\n\u001b[?2004l\r"]
[102.966576, "o", "\u001b[?2004h"]
[102.966595, "o", "> "]
[102.966841, "o", "    index_name = index_name[:63].strip('_')  # Ensure it doesn't end with an underscore\r\n\u001b[?2004l\r"]
[102.966857, "o", "\u001b[?2004h"]
[102.966875, "o", ">     \r\n\u001b[?2004l\r"]
[102.966893, "o", "\u001b[?2004h"]
[102.966912, "o", "> "]
[102.967138, "o", "    return f\"CREATE INDEX {index_name} ON {table_name}({', '.join(columns)});\"\r\n\u001b[?2004l\r"]
[102.967158, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[102.967176, "o", "\u001b[?2004h"]
[102.967193, "o", "> "]
[102.96728, "o", "def calculate_sha256(filepath):\r\n\u001b[?2004l\r"]
[102.967294, "o", "\u001b[?2004h"]
[102.96732, "o", "> "]
[102.967444, "o", "    \"\"\"Calculate SHA-256 checksum of a file.\"\"\"\r\n\u001b[?2004l\r"]
[102.967458, "o", "\u001b[?2004h"]
[102.967476, "o", "> "]
[102.967553, "o", "    sha256 = hashlib.sha256()\r\n\u001b[?2004l\r"]
[102.967567, "o", "\u001b[?2004h"]
[102.967593, "o", "> "]
[102.967689, "o", "    with open(filepath, 'rb') as f:\r\n\u001b[?2004l\r"]
[102.967705, "o", "\u001b[?2004h"]
[102.967726, "o", "> "]
[102.967785, "o", "        while True:\r\n\u001b[?2004l\r"]
[102.9678, "o", "\u001b[?2004h"]
[102.967817, "o", "> "]
[102.967972, "o", "            data = f.read(65536)  # Read in 64k chunks\r\n\u001b[?2004l\r"]
[102.967991, "o", "\u001b[?2004h> "]
[102.968069, "o", "            if not data:\r\n\u001b[?2004l\r"]
[102.968083, "o", "\u001b[?2004h"]
[102.968101, "o", "> "]
[102.968162, "o", "                break\r\n\u001b[?2004l\r"]
[102.968182, "o", "\u001b[?2004h> "]
[102.96828, "o", "            sha256.update(data)\r\n\u001b[?2004l\r"]
[102.968294, "o", "\u001b[?2004h"]
[102.968312, "o", "> "]
[102.968389, "o", "    return sha256.hexdigest()\r\n\u001b[?2004l\r"]
[102.968419, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[102.968437, "o", "\u001b[?2004h> "]
[102.96847, "o", "def main():\r\n\u001b[?2004l\r"]
[102.968484, "o", "\u001b[?2004h"]
[102.968501, "o", "> "]
[102.96876, "o", "    parser = argparse.ArgumentParser(description='SQL Query Log Analysis & Optimization')\r\n\u001b[?2004l\r"]
[102.968784, "o", "\u001b[?2004h> "]
[102.969022, "o", "    parser.add_argument('--date', required=True, help='Date to analyze (YYYY-MM-DD)')\r\n\u001b[?2004l\r"]
[102.969038, "o", "\u001b[?2004h> "]
[102.969138, "o", "    args = parser.parse_args()\r\n\u001b[?2004l\r"]
[102.969157, "o", "\u001b[?2004h"]
[102.969174, "o", ">     \r\n\u001b[?2004l\r"]
[102.969193, "o", "\u001b[?2004h> "]
[102.969276, "o", "    target_date = args.date\r\n\u001b[?2004l\r"]
[102.969292, "o", "\u001b[?2004h"]
[102.969309, "o", "> "]
[102.969326, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[102.969343, "o", "> "]
[102.969501, "o", "    print(f\"Starting analysis for date: {target_date}\")\r\n\u001b[?2004l\r"]
[102.969516, "o", "\u001b[?2004h"]
[102.969533, "o", "> "]
[102.96955, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[102.969568, "o", "> "]
[102.969643, "o", "    config = read_config()\r\n\u001b[?2004l\r"]
[102.969661, "o", "\u001b[?2004h> "]
[102.9698, "o", "    log_file_path = config['log_file_path']\r\n\u001b[?2004l\r"]
[102.969817, "o", "\u001b[?2004h> "]
[102.96997, "o", "    min_duration_ms = config['min_duration_ms']\r\n\u001b[?2004l\r"]
[102.969988, "o", "\u001b[?2004h> "]
[102.970007, "o", "    \r\n\u001b[?2004l\r"]
[102.970025, "o", "\u001b[?2004h"]
[102.970043, "o", "> "]
[102.970281, "o", "    print(f\"Config: log_file='{log_file_path}', min_duration_ms={min_duration_ms}\")\r\n\u001b[?2004l\r"]
[102.970299, "o", "\u001b[?2004h"]
[102.970316, "o", ">     \r\n\u001b[?2004l\r"]
[102.970332, "o", "\u001b[?2004h"]
[102.970349, "o", "> "]
[102.970491, "o", "    # Create staging directory if it doesn't exist\r\n\u001b[?2004l\r"]
[102.97051, "o", "\u001b[?2004h> "]
[102.970656, "o", "    staging_dir = os.path.dirname(STAGING_FILE)\r\n\u001b[?2004l\r"]
[102.970671, "o", "\u001b[?2004h"]
[102.970688, "o", "> "]
[102.970801, "o", "    if not os.path.exists(staging_dir):\r\n\u001b[?2004l\r"]
[102.970816, "o", "\u001b[?2004h"]
[102.970832, "o", "> "]
[102.970963, "o", "        os.makedirs(staging_dir, exist_ok=True)\r\n\u001b[?2004l\r"]
[102.970978, "o", "\u001b[?2004h"]
[102.970994, "o", "> "]
[102.971138, "o", "        print(f\"Created directory: {staging_dir}\")\r\n\u001b[?2004l\r"]
[102.971157, "o", "\u001b[?2004h> "]
[102.971173, "o", "    \r\n\u001b[?2004l\r"]
[102.971192, "o", "\u001b[?2004h"]
[102.971208, "o", "> "]
[102.97128, "o", "    extracted_queries = []\r\n\u001b[?2004l\r"]
[102.971294, "o", "\u001b[?2004h"]
[102.971311, "o", ">     "]
[102.971329, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[102.971351, "o", "> "]
[102.971464, "o", "    if not os.path.exists(log_file_path):\r\n\u001b[?2004l\r"]
[102.971479, "o", "\u001b[?2004h"]
[102.971495, "o", "> "]
[102.971777, "o", "        print(f\"Error: Log file not found at '{log_file_path}'. Cannot continue.\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[102.971793, "o", "\u001b[?2004h"]
[102.97181, "o", "> "]
[102.97186, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[102.971874, "o", "\u001b[?2004h"]
[102.971891, "o", "> "]
[102.971913, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[102.971931, "o", "> "]
[102.972068, "o", "    print(f\"Processing log file: {log_file_path}\")\r\n\u001b[?2004l\r"]
[102.972082, "o", "\u001b[?2004h"]
[102.972099, "o", "> "]
[102.972119, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[102.972133, "o", "> "]
[102.972318, "o", "    with open(log_file_path, 'r', encoding='utf-8') as log_f:\r\n\u001b[?2004l\r"]
[102.972337, "o", "\u001b[?2004h"]
[102.972353, "o", "> "]
[102.972424, "o", "        # Prepare CSV writer\r\n\u001b[?2004l\r"]
[102.972439, "o", "\u001b[?2004h"]
[102.972457, "o", "> "]
[102.972688, "o", "        with open(STAGING_FILE, 'w', encoding='ISO-8859-1', newline='') as csv_f:\r\n\u001b[?2004l\r"]
[102.972703, "o", "\u001b[?2004h"]
[102.97272, "o", "> "]
[102.972965, "o", "            csv_writer = csv.writer(csv_f, lineterminator='\\r\\n') # CRLF line endings\r\n\u001b[?2004l\r"]
[102.972985, "o", "\u001b[?2004h> "]
[102.973191, "o", "            csv_writer.writerow(['timestamp', 'duration_ms', 'query_text']) # Header\r\n\u001b[?2004l\r"]
[102.973207, "o", "\u001b[?2004h> "]
[102.973243, "o", "            \r\n\u001b[?2004l\r"]
[102.973257, "o", "\u001b[?2004h"]
[102.973272, "o", "> "]
[102.973396, "o", "            for line_num, line in enumerate(log_f, 1):\r\n\u001b[?2004l\r"]
[102.973409, "o", "\u001b[?2004h"]
[102.973423, "o", "> "]
[102.973503, "o", "                line = line.strip()\r\n\u001b[?2004l\r"]
[102.973516, "o", "\u001b[?2004h"]
[102.973531, "o", "> "]
[102.973592, "o", "                if not line:\r\n\u001b[?2004l\r"]
[102.973607, "o", "\u001b[?2004h"]
[102.973624, "o", "> "]
[102.973683, "o", "                    continue\r\n\u001b[?2004l\r"]
[102.973696, "o", "\u001b[?2004h"]
[102.973712, "o", "> "]
[102.973745, "o", "                \r\n\u001b[?2004l\r"]
[102.973758, "o", "\u001b[?2004h"]
[102.973773, "o", "> "]
[102.973912, "o", "                parts = [p.strip() for p in line.split('|')]\r\n\u001b[?2004l\r"]
[102.973926, "o", "\u001b[?2004h"]
[102.973942, "o", "> "]
[102.974019, "o", "                if len(parts) != 4:\r\n\u001b[?2004l\r"]
[102.974033, "o", "\u001b[?2004h> "]
[102.974247, "o", "                    print(f\"Warning: Skipping malformed line {line_num}: '{line}'\")\r\n\u001b[?2004l\r"]
[102.974264, "o", "\u001b[?2004h> "]
[102.974335, "o", "                    continue\r\n\u001b[?2004l\r"]
[102.974348, "o", "\u001b[?2004h"]
[102.974363, "o", "> "]
[102.974397, "o", "                \r\n\u001b[?2004l\r"]
[102.974411, "o", "\u001b[?2004h"]
[102.974426, "o", "> "]
[102.974571, "o", "                timestamp, duration_str, level, message = parts\r\n\u001b[?2004l\r"]
[102.974583, "o", "\u001b[?2004h"]
[102.974598, "o", "> "]
[102.974632, "o", "                \r\n\u001b[?2004l\r"]
[102.974647, "o", "\u001b[?2004h"]
[102.974663, "o", "> "]
[102.974731, "o", "                # 1. Date Match\r\n\u001b[?2004l\r"]
[102.974743, "o", "\u001b[?2004h"]
[102.97476, "o", "> "]
[102.974802, "o", "                try:\r\n\u001b[?2004l\r"]
[102.974816, "o", "\u001b[?2004h"]
[102.974833, "o", "> "]
[102.974967, "o", "                    # Use the first 10 chars for YYYY-MM-DD\r\n\u001b[?2004l\r"]
[102.974979, "o", "\u001b[?2004h"]
[102.974995, "o", "> "]
[102.975095, "o", "                    log_date = timestamp[:10]\r\n\u001b[?2004l\r"]
[102.97511, "o", "\u001b[?2004h"]
[102.975132, "o", "> "]
[102.975237, "o", "                    if log_date != target_date:\r\n\u001b[?2004l\r"]
[102.975256, "o", "\u001b[?2004h> "]
[102.975335, "o", "                        continue\r\n\u001b[?2004l\r"]
[102.975354, "o", "\u001b[?2004h> "]
[102.975437, "o", "                except Exception:\r\n\u001b[?2004l\r"]
[102.975453, "o", "\u001b[?2004h"]
[102.97547, "o", "> "]
[102.975705, "o", "                    print(f\"Warning: Skipping line {line_num} due to date format error: '{timestamp}'\")\r\n\u001b[?2004l\r"]
[102.975722, "o", "\u001b[?2004h> "]
[102.975796, "o", "                    continue\r\n\u001b[?2004l\r"]
[102.975814, "o", "\u001b[?2004h> "]
[102.975858, "o", "                \r\n\u001b[?2004l\r"]
[102.975872, "o", "\u001b[?2004h"]
[102.975889, "o", "> "]
[102.975955, "o", "                # 2. Type Match\r\n\u001b[?2004l\r"]
[102.975973, "o", "\u001b[?2004h> "]
[102.976069, "o", "                if level != '[QUERY]':\r\n\u001b[?2004l\r"]
[102.976084, "o", "\u001b[?2004h"]
[102.976106, "o", "> "]
[102.976165, "o", "                    continue\r\n\u001b[?2004l\r"]
[102.976182, "o", "\u001b[?2004h"]
[102.976199, "o", "> "]
[102.976229, "o", "                \r\n\u001b[?2004l\r"]
[102.976246, "o", "\u001b[?2004h> "]
[102.976353, "o", "                # 3. Performance Threshold\r\n\u001b[?2004l\r"]
[102.976372, "o", "\u001b[?2004h> "]
[102.976517, "o", "                duration_val = parse_duration(duration_str)\r\n\u001b[?2004l\r"]
[102.976534, "o", "\u001b[?2004h> "]
[102.976658, "o", "                if duration_val < min_duration_ms:\r\n\u001b[?2004l\r"]
[102.976676, "o", "\u001b[?2004h> "]
[102.976748, "o", "                    continue\r\n\u001b[?2004l\r"]
[102.976763, "o", "\u001b[?2004h"]
[102.976782, "o", "> "]
[102.976812, "o", "                \r\n\u001b[?2004l\r"]
[102.97683, "o", "\u001b[?2004h> "]
[102.976961, "o", "                # If all conditions met, write to CSV\r\n\u001b[?2004l\r"]
[102.976976, "o", "\u001b[?2004h"]
[102.976992, "o", "> "]
[102.977132, "o", "                # Duration for CSV is just the numeric part\r\n\u001b[?2004l\r"]
[102.977156, "o", "\u001b[?2004h> "]
[102.97732, "o", "                csv_writer.writerow([timestamp, duration_val, message])\r\n\u001b[?2004l\r"]
[102.977347, "o", "\u001b[?2004h> "]
[102.977554, "o", "                extracted_queries.append({'timestamp': timestamp, 'query_text': message})\r\n\u001b[?2004l\r"]
[102.977585, "o", "\u001b[?2004h> "]
[102.977606, "o", "            \r\n\u001b[?2004l\r"]
[102.977625, "o", "\u001b[?2004h"]
[102.977642, "o", "> "]
[102.977805, "o", "    print(f\"Extracted {len(extracted_queries)} queries to {STAGING_FILE}\")\r\n\u001b[?2004l\r"]
[102.977835, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[102.977853, "o", "\u001b[?2004h"]
[102.97787, "o", "> "]
[102.977957, "o", "    # Phase 2: Analysis & Normalization\r\n\u001b[?2004l\r"]
[102.977983, "o", "\u001b[?2004h> "]
[102.978114, "o", "    staging_file_sha256 = calculate_sha256(STAGING_FILE)\r\n\u001b[?2004l\r"]
[102.978138, "o", "\u001b[?2004h> "]
[102.978286, "o", "    print(f\"SHA-256 of staging file: {staging_file_sha256}\")\r\n\u001b[?2004l\r"]
[102.978301, "o", "\u001b[?2004h"]
[102.978318, "o", ">     \r\n\u001b[?2004l\r"]
[102.978336, "o", "\u001b[?2004h> "]
[102.978524, "o", "    # Process queries for normalization, deduplication, and index suggestions\r\n\u001b[?2004l\r"]
[102.978538, "o", "\u001b[?2004h"]
[102.978555, "o", "> "]
[102.978611, "o", "    normalized_queries = {}\r\n\u001b[?2004l\r"]
[102.978626, "o", "\u001b[?2004h"]
[102.978647, "o", ">     \r\n\u001b[?2004l\r"]
[102.978661, "o", "\u001b[?2004h"]
[102.978679, "o", "> "]
[102.978751, "o", "    for item in extracted_queries:\r\n\u001b[?2004l\r"]
[102.978766, "o", "\u001b[?2004h"]
[102.978785, "o", "> "]
[102.978878, "o", "        original_query = item['query_text']\r\n\u001b[?2004l\r"]
[102.9789, "o", "\u001b[?2004h> "]
[102.979046, "o", "        normalized_query_text = normalize_sql(original_query)\r\n\u001b[?2004l\r"]
[102.979071, "o", "\u001b[?2004h> "]
[102.97909, "o", "        \r\n\u001b[?2004l\r"]
[102.979111, "o", "\u001b[?2004h> "]
[102.979247, "o", "        # Use normalized text as key for deduplication\r\n\u001b[?2004l\r"]
[102.979272, "o", "\u001b[?2004h"]
[102.979292, "o", "> "]
[102.979408, "o", "        if normalized_query_text not in normalized_queries:\r\n\u001b[?2004l\r"]
[102.979429, "o", "\u001b[?2004h"]
[102.979447, "o", "> "]
[102.979624, "o", "            table_name, columns = extract_table_and_columns(normalized_query_text)\r\n\u001b[?2004l\r"]
[102.979654, "o", "\u001b[?2004h> "]
[102.979802, "o", "            index_suggestion = suggest_index(table_name, columns)\r\n\u001b[?2004l\r"]
[102.979817, "o", "\u001b[?2004h"]
[102.979836, "o", "> "]
[102.979856, "o", "            \r\n\u001b[?2004l\r"]
[102.979871, "o", "\u001b[?2004h"]
[102.979889, "o", "> "]
[102.980015, "o", "            normalized_queries[normalized_query_text] = {\r\n\u001b[?2004l\r"]
[102.98003, "o", "\u001b[?2004h> "]
[102.980253, "o", "                'first_timestamp': item['timestamp'], # Store first occurrence timestamp\r\n\u001b[?2004l\r"]
[102.980273, "o", "\u001b[?2004h> "]
[102.980341, "o", "                'count': 1,\r\n\u001b[?2004l\r"]
[102.980359, "o", "\u001b[?2004h"]
[102.980377, "o", "> "]
[102.980505, "o", "                'normalized_sql': normalized_query_text,\r\n\u001b[?2004l\r"]
[102.980522, "o", "\u001b[?2004h"]
[102.98054, "o", "> "]
[102.980659, "o", "                'index_suggestion': index_suggestion\r\n\u001b[?2004l\r"]
[102.980677, "o", "\u001b[?2004h"]
[102.980695, "o", "> "]
[102.980721, "o", "            }\r\n\u001b[?2004l\r"]
[102.980737, "o", "\u001b[?2004h"]
[102.980755, "o", "> "]
[102.980782, "o", "        else:\r\n\u001b[?2004l\r"]
[102.980799, "o", "\u001b[?2004h"]
[102.980819, "o", "> "]
[102.980937, "o", "            # Increment count for already seen query\r\n\u001b[?2004l\r"]
[102.980956, "o", "\u001b[?2004h"]
[102.980974, "o", "> "]
[102.981132, "o", "            normalized_queries[normalized_query_text]['count'] += 1\r\n\u001b[?2004l\r"]
[102.981148, "o", "\u001b[?2004h> "]
[102.981173, "o", "    \r\n\u001b[?2004l\r"]
[102.981192, "o", "\u001b[?2004h> "]
[102.981343, "o", "    # Prepare suggestions for report, sorted by first_timestamp\r\n\u001b[?2004l\r"]
[102.981359, "o", "\u001b[?2004h"]
[102.981374, "o", "> "]
[102.981439, "o", "    suggestions_list = sorted(\r\n\u001b[?2004l\r"]
[102.981454, "o", "\u001b[?2004h"]
[102.981472, "o", "> "]
[102.981551, "o", "        normalized_queries.values(), \r\n\u001b[?2004l\r"]
[102.981566, "o", "\u001b[?2004h"]
[102.981584, "o", "> "]
[102.981675, "o", "        key=lambda x: x['first_timestamp']\r\n\u001b[?2004l\r"]
[102.981693, "o", "\u001b[?2004h"]
[102.981711, "o", ">     )\r\n"]
[102.981731, "o", "\u001b[?2004l\r\u001b[?2004h"]
[102.981747, "o", ">     \r\n\u001b[?2004l\r"]
[102.981765, "o", "\u001b[?2004h> "]
[102.981899, "o", "    total_queries_extracted = len(extracted_queries)\r\n\u001b[?2004l\r"]
[102.981919, "o", "\u001b[?2004h> "]
[102.982024, "o", "    unique_patterns = len(suggestions_list)\r\n\u001b[?2004l\r"]
[102.982043, "o", "\u001b[?2004h> "]
[102.982058, "o", "    \r\n\u001b[?2004l\r"]
[102.982099, "o", "\u001b[?2004h> "]
[102.982352, "o", "    print(f\"Analysis complete. Total extracted: {total_queries_extracted}, Unique patterns: {unique_patterns}\")\r\n\u001b[?2004l\r"]
[102.982375, "o", "\u001b[?2004h> "]
[102.982387, "o", "    \r\n\u001b[?2004l\r"]
[102.982421, "o", "\u001b[?2004h> "]
[102.982462, "o", "    # Phase 3: Reporting\r\n\u001b[?2004l\r"]
[102.982478, "o", "\u001b[?2004h"]
[102.982495, "o", "> "]
[102.982537, "o", "    report_data = {\r\n\u001b[?2004l\r"]
[102.982555, "o", "\u001b[?2004h> "]
[102.982602, "o", "        \"meta\": {\r\n\u001b[?2004l\r"]
[102.982619, "o", "\u001b[?2004h"]
[102.982639, "o", "> "]
[102.982724, "o", "            \"date_analyzed\": target_date,\r\n\u001b[?2004l\r"]
[102.982749, "o", "\u001b[?2004h> "]
[102.982879, "o", "            \"staging_file_sha256\": staging_file_sha256\r\n\u001b[?2004l\r"]
[102.982901, "o", "\u001b[?2004h> "]
[102.982928, "o", "        },\r\n\u001b[?2004l\r"]
[102.982943, "o", "\u001b[?2004h> "]
[102.982996, "o", "        \"stats\": {\r\n\u001b[?2004l\r"]
[102.98301, "o", "\u001b[?2004h> "]
[102.983176, "o", "            \"total_queries_extracted\": total_queries_extracted,\r\n\u001b[?2004l\r"]
[102.983203, "o", "\u001b[?2004h> "]
[102.983311, "o", "            \"unique_patterns\": unique_patterns\r\n\u001b[?2004l\r"]
[102.983329, "o", "\u001b[?2004h> "]
[102.98336, "o", "        },\r\n\u001b[?2004l\r"]
[102.983379, "o", "\u001b[?2004h> "]
[102.983443, "o", "        \"suggestions\": []\r\n\u001b[?2004l\r"]
[102.983464, "o", "\u001b[?2004h> "]
[102.983484, "o", "    }\r\n\u001b[?2004l\r"]
[102.983503, "o", "\u001b[?2004h> "]
[102.983529, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[102.983548, "o", "> "]
[102.983633, "o", "    for suggestion in suggestions_list:\r\n\u001b[?2004l\r"]
[102.983654, "o", "\u001b[?2004h> "]
[102.983759, "o", "        report_data[\"suggestions\"].append({\r\n\u001b[?2004l\r"]
[102.983777, "o", "\u001b[?2004h> "]
[102.983903, "o", "           \"id\": suggestion['first_timestamp'],\r\n\u001b[?2004l\r"]
[102.98393, "o", "\u001b[?2004h> "]
[102.984075, "o", "           \"normalized_sql\": suggestion['normalized_sql'],\r\n\u001b[?2004l\r"]
[102.984094, "o", "\u001b[?2004h> "]
[102.984257, "o", "           \"index_suggestion\": suggestion['index_suggestion']\r\n\u001b[?2004l\r"]
[102.984275, "o", "\u001b[?2004h> "]
[102.984308, "o", "        })\r\n\u001b[?2004l\r"]
[102.984326, "o", "\u001b[?2004h> "]
[102.984347, "o", "    \r\n\u001b[?2004l\r"]
[102.984365, "o", "\u001b[?2004h"]
[102.984383, "o", "> "]
[102.984613, "o", "    with open(REPORT_FILE, 'w', encoding='utf-8', newline='\\n') as report_f: # Unix line endings\r\n\u001b[?2004l\r"]
[102.984635, "o", "\u001b[?2004h> "]
[102.98478, "o", "        json.dump(report_data, report_f, indent=2)\r\n\u001b[?2004l\r"]
[102.984808, "o", "\u001b[?2004h"]
[102.984828, "o", "> "]
[102.984861, "o", "    \r\n"]
[102.984879, "o", "\u001b[?2004l\r"]
[102.984898, "o", "\u001b[?2004h"]
[102.984916, "o", "> "]
[102.98518, "o", "    print(f\"Report generated at: {REPORT_FILE}\")\r\n\u001b[?2004l\r"]
[102.985208, "o", "\u001b[?2004h"]
[102.985239, "o", "> \r\n\u001b[?2004l\r"]
[102.985271, "o", "\u001b[?2004h"]
[102.98529, "o", "> "]
[102.985439, "o", "if __name__ == \"__main__\":\r\n"]
[102.985459, "o", "\u001b[?2004l\r"]
[102.985476, "o", "\u001b[?2004h"]
[102.985498, "o", "> "]
[102.985561, "o", "    main()\r\n\u001b[?2004l\r"]
[102.985589, "o", "\u001b[?2004h"]
[102.985605, "o", "> "]
[102.985635, "o", "EOF\r\n"]
[102.985653, "o", "\u001b[?2004l\r"]
[102.986962, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[103.782172, "i", "chmod +x analyze_and_optimize.py\n"]
[103.782317, "o", "chmod +x analyze_and_optimize.py\r\n\u001b[?2004l\r"]
[103.783255, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[115.186211, "i", "./analyze_and_optimize.py --date 2023-03-15\n"]
[115.186383, "o", "./analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[115.217019, "o", "Traceback (most recent call last):\r\n  File \"/home/user/query_optimization/./analyze_and_optimize.py\", line 16, in <module>\r\n"]
[115.217053, "o", "    from sqlparse.sql import Token, Function, Keyword, DML\r\nImportError: cannot import name 'Keyword' from 'sqlparse.sql' (/home/user/query_optimization/env/lib/python3.10/site-packages/sqlparse/sql.py)\r\n"]
[115.21985, "o", "\u001b[?2004h"]
[115.219878, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[120.088518, "i", "cat report.json\n"]
[120.088614, "o", "cat report.json\r\n"]
[120.088639, "o", "\u001b[?2004l\r"]
[120.089282, "o", "cat: report.json"]
[120.089337, "o", ": No such file or directory\r\n"]
[120.089504, "o", "\u001b[?2004h"]
[120.089527, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[135.972878, "i", "sed -i 's/from sqlparse.sql import Token, Function, Keyword, DML/from sqlparse.tokens import Keyword, DML, Token; from sqlparse.sql import Function/' analyze_and_optimize.py\n"]
[135.973361, "o", "sed -i 's/from sqlparse.sql import Token, Function, Keyword, DML/from sqlparse.tokens import Keyword, DML, Token; from sqlparse.sql import Function/' analyze_and_optimize.py\r\n\u001b[?2004l\r"]
[135.974977, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[136.404713, "i", "python3 analyze_and_optimize.py --date 2023-03-15\n"]
[136.404879, "o", "python3 analyze_and_optimize.py --date 2023-03-15\r\n"]
[136.40491, "o", "\u001b[?2004l\r"]
[136.428046, "o", "Starting analysis for date: 2023-03-15\r\n"]
[136.42821, "o", "Config: log_file='', min_duration_ms=0\r\n"]
[136.42825, "o", "Error: Log file not found at ''. Cannot continue.\r\n"]
[136.431511, "o", "\u001b[?2004h"]
[136.431557, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[141.398746, "i", "cat report.json\n"]
[141.398847, "o", "cat report.json\r\n\u001b[?2004l\r"]
[141.399822, "o", "cat: report.json"]
[141.399889, "o", ": No such file or directory\r\n"]
[141.400095, "o", "\u001b[?2004h"]
[141.40013, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[169.671966, "i", "mkdir -p /home/user/db_tools\n"]
[169.672239, "o", "mkdir -p /home/user/db_tools\r\n\u001b[?2004l\r"]
[169.674169, "o", "\u001b[?2004h"]
[169.674216, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[169.916462, "i", "cat > /home/user/db_tools/db_config.ini << 'EOF'\n[DEFAULT]\nlog_file_path = /home/user/query_logs/sample.log\nmin_duration_ms = 10\nEOF\n"]
[169.916632, "o", "cat > /home/user/db_tools/db_config.ini << 'EOF'\r\n\u001b[?2004l\r"]
[169.916682, "o", "\u001b[?2004h"]
[169.916713, "o", "> "]
[169.916731, "o", "[DEFAULT]\r\n\u001b[?2004l\r"]
[169.916764, "o", "\u001b[?2004h> "]
[169.916894, "o", "log_file_path = /home/user/query_logs/sample.log\r\n\u001b[?2004l\r"]
[169.916919, "o", "\u001b[?2004h> "]
[169.916983, "o", "min_duration_ms = 10\r\n\u001b[?2004l\r"]
[169.91701, "o", "\u001b[?2004h> "]
[169.917035, "o", "EOF\r\n\u001b[?2004l\r"]
[169.918576, "o", "\u001b[?2004h"]
[169.918584, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[170.303254, "i", "mkdir -p /home/user/query_logs\n"]
[170.30338, "o", "mkdir -p /home/user/query_logs\r\n"]
[170.303411, "o", "\u001b[?2004l\r"]
[170.304552, "o", "\u001b[?2004h"]
[170.304561, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[170.432178, "i", "cat > /home/user/query_logs/sample.log << 'EOF'\n2023-03-15T10:00:00Z | 15ms | [QUERY] | SELECT * FROM users WHERE id = 1;\n2023-03-15T10:01:00Z | 5ms | [QUERY] | SELECT name FROM products WHERE category_id = 10;\n2023-03-15T10:02:00Z | 20ms | [QUERY] | SELECT * FROM users WHERE id = 1;\n2023-03-15T10:03:00Z | 2ms | [INFO] | System startup complete.\n2023-03-15T10:04:00Z | 12ms | [QUERY] | SELECT name, email FROM customers WHERE status = 'active';\n2023-03-15T10:05:00Z | 22ms | [QUERY] | SELECT * FROM orders WHERE customer_id = 123 AND order_date > '2023-01-01';\n2023-03-15T10:06:00Z | 8ms | [QUERY] | SELECT name FROM products WHERE category_id = 10; -- get product names\n2023-03-15T11:00:00Z | 50ms | [QUERY] | UPDATE users SET last_login = NOW() WHERE id = 100;\nEOF\n"]
[170.43237, "o", "cat > /home/user/query_logs/sample.log << 'EOF'\r\n\u001b[?2004l\r"]
[170.432416, "o", "\u001b[?2004h> "]
[170.432612, "o", "2023-03-15T10:00:00Z | 15ms | [QUERY] | SELECT * FROM users WHERE id = 1;\r\n\u001b[?2004l\r"]
[170.432629, "o", "\u001b[?2004h"]
[170.432647, "o", "> "]
[170.432846, "o", "2023-03-15T10:01:00Z | 5ms | [QUERY] | SELECT name FROM products WHERE category_id = 10;\r\n\u001b[?2004l\r"]
[170.432861, "o", "\u001b[?2004h"]
[170.432876, "o", "> "]
[170.433046, "o", "2023-03-15T10:02:00Z | 20ms | [QUERY] | SELECT * FROM users WHERE id = 1;\r\n\u001b[?2004l\r"]
[170.433066, "o", "\u001b[?2004h> "]
[170.433234, "o", "2023-03-15T10:03:00Z | 2ms | [INFO] | System startup complete.\r\n\u001b[?2004l\r"]
[170.43325, "o", "\u001b[?2004h> "]
[170.433499, "o", "2023-03-15T10:04:00Z | 12ms | [QUERY] | SELECT name, email FROM customers WHERE status = 'active';\r\n\u001b[?2004l\r"]
[170.433518, "o", "\u001b[?2004h"]
[170.433532, "o", "> "]
[170.43381, "o", "2023-03-15T10:05:00Z | 22ms | [QUERY] | SELECT * FROM orders WHERE customer_id = 123 AND order_date > '2023-01-01';\r\n\u001b[?2004l\r"]
[170.433825, "o", "\u001b[?2004h"]
[170.43384, "o", "> "]
[170.434106, "o", "2023-03-15T10:06:00Z | 8ms | [QUERY] | SELECT name FROM products WHERE category_id = 10; -- get product names\r\n\u001b[?2004l\r"]
[170.434129, "o", "\u001b[?2004h"]
[170.434143, "o", "> "]
[170.434376, "o", "2023-03-15T11:00:00Z | 50ms | [QUERY] | UPDATE users SET last_login = NOW() WHERE id = 100;\r\n\u001b[?2004l\r"]
[170.434391, "o", "\u001b[?2004h"]
[170.434406, "o", "> "]
[170.43442, "o", "EOF\r\n\u001b[?2004l\r"]
[170.435404, "o", "\u001b[?2004h"]
[170.435417, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[171.436276, "i", "python3 analyze_and_optimize.py --date 2023-03-15\n"]
[171.436445, "o", "python3 analyze_and_optimize.py --date 2023-03-15\r\n"]
[171.436476, "o", "\u001b[?2004l\r"]
[171.459508, "o", "Starting analysis for date: 2023-03-15\r\n"]
[171.459633, "o", "Config: log_file='/home/user/query_logs/sample.log', min_duration_ms=10\r\n"]
[171.459665, "o", "Processing log file: /home/user/query_logs/sample.log\r\n"]
[171.460045, "o", "Extracted 5 queries to /home/user/query_optimization/staging/raw_extract.csv\r\n"]
[171.460083, "o", "SHA-256 of staging file: 901803ac828f19b83a041a599d4255def673f6d0fd1c5cc516e654209ceec8fd\r\n"]
[171.463353, "o", "Warning: Could not extract table/columns from: SELECT * FROM users WHERE id = 1. Error: name 'sqlparse' is not defined\r\n"]
[171.464109, "o", "Warning: Could not extract table/columns from: SELECT name, email FROM customers WHERE status = 'active'. Error: name 'sqlparse' is not defined\r\n"]
[171.464599, "o", "Warning: Could not extract table/columns from: SELECT * FROM orders WHERE customer_id = 123 AND order_date > '2023-01-01'. Error: name 'sqlparse' is not defined\r\n"]
[171.465127, "o", "Warning: Could not extract table/columns from: UPDATE users SET last_login = NOW() WHERE id = 100. Error: name 'sqlparse' is not defined\r\nAnalysis complete. Total extracted: 5, Unique patterns: 4\r\n"]
[171.465239, "o", "Report generated at: /home/user/query_optimization/report.json\r\n"]
[171.468795, "o", "\u001b[?2004h"]
[171.468872, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[176.442233, "i", "cat report.json\n"]
[176.44238, "o", "cat report.json\r\n\u001b[?2004l\r"]
[176.443392, "o", "{\r\n  \"meta\": {\r\n    \"date_analyzed\": \"2023-03-15\",\r\n    \"staging_file_sha256\": \"901803ac828f19b83a041a599d4255def673f6d0fd1c5cc516e654209ceec8fd\"\r\n  },\r\n  \"stats\": {\r\n    \"total_queries_extracted\": 5,\r\n    \"unique_patterns\": 4\r\n  },\r\n  \"suggestions\": [\r\n    {\r\n      \"id\": \"2023-03-15T10:00:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM users WHERE id = 1\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:04:00Z\",\r\n      \"normalized_sql\": \"SELECT name, email FROM customers WHERE status = 'active'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:05:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM orders WHERE customer_id = 123 AND order_date > '2023-01-01'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T11:00:00Z\",\r\n      \"normalized_sql\": \"UPDATE users SET last_login = NOW() WHERE id = 100\",\r\n      \"index_suggestion\": null\r\n    }\r\n  ]\r\n}"]
[176.443647, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[195.399548, "i", "sed -i '/def extract_table_and_columns(query_text):/a\\\\timport sqlparse' analyze_and_optimize.py\n"]
[195.399859, "o", "sed -i '/def extract_table_and_columns(query_text):/a\\\\timport sqlparse' analyze_and_optimize.py\r\n\u001b[?2004l\r"]
[195.40155, "o", "\u001b[?2004h"]
[195.401586, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[195.779924, "i", "python3 analyze_and_optimize.py --date 2023-03-15\n"]
[195.780104, "o", "python3 analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[195.787315, "o", "  File \"/home/user/query_optimization/analyze_and_optimize.py\", line 62\r\n    \"\"\"Extract table and columns from WHERE clause for index suggestion.\"\"\"\r\n"]
[195.787329, "o", "                                                                           ^\r\nIndentationError: unindent does not match any outer indentation level\r\n"]
[195.788713, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[200.814578, "i", "cat report.json\n"]
[200.814717, "o", "cat report.json\r\n\u001b[?2004l\r"]
[200.815513, "o", "{\r\n  \"meta\": {\r\n    \"date_analyzed\": \"2023-03-15\",\r\n    \"staging_file_sha256\": \"901803ac828f19b83a041a599d4255def673f6d0fd1c5cc516e654209ceec8fd\"\r\n  },\r\n  \"stats\": {\r\n    \"total_queries_extracted\": 5,\r\n    \"unique_patterns\": 4\r\n  },\r\n  \"suggestions\": [\r\n    {\r\n      \"id\": \"2023-03-15T10:00:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM users WHERE id = 1\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:04:00Z\",\r\n      \"normalized_sql\": \"SELECT name, email FROM customers WHERE status = 'active'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:05:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM orders WHERE customer_id = 123 AND order_date > '2023-01-01'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T11:00:00Z\",\r\n      \"normalized_sql\": \"UPDATE users SET last_login = NOW() WHERE id = 100\",\r\n      \"index_suggestion\": null\r\n    }\r\n  ]\r\n}"]
[200.815693, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[218.460258, "i", "sed -i '0,/^def extract_table_and_columns(query_text):$/ s//&\\\\n\\timport sqlparse/' analyze_and_optimize.py\n"]
[218.46067, "o", "sed -i '0,/^def extract_table_and_columns(query_text):$/ s//&\\\\n\\timport sqlparse/' analyze_and_optimize.py\r\n\u001b[?2004l\r"]
[218.462336, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[218.837821, "i", "python3 analyze_and_optimize.py --date 2023-03-15\n"]
[218.837984, "o", "python3 analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[218.844945, "o", "  File \"/home/user/query_optimization/analyze_and_optimize.py\", line 60\r\n    def extract_table_and_columns(query_text):\\n\timport sqlparse\r\n                                               ^\r\nSyntaxError: unexpected character after line continuation character\r\n"]
[218.846281, "o", "\u001b[?2004h"]
[218.846293, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[223.81499, "i", "cat report.json\n"]
[223.815123, "o", "cat report.json\r\n\u001b[?2004l\r"]
[223.815821, "o", "{\r\n  \"meta\": {\r\n    \"date_analyzed\": \"2023-03-15\",\r\n    \"staging_file_sha256\": \"901803ac828f19b83a041a599d4255def673f6d0fd1c5cc516e654209ceec8fd\"\r\n  },\r\n  \"stats\": {\r\n    \"total_queries_extracted\": 5,\r\n    \"unique_patterns\": 4\r\n  },\r\n  \"suggestions\": [\r\n    {\r\n      \"id\": \"2023-03-15T10:00:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM users WHERE id = 1\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:04:00Z\",\r\n      \"normalized_sql\": \"SELECT name, email FROM customers WHERE status = 'active'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:05:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM orders WHERE customer_id = 123 AND order_date > '2023-01-01'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T11:00:00Z\",\r\n      \"normalized_sql\": \"UPDATE users SET last_login = NOW() WHERE id = 100\",\r\n      \"index_suggestion\": null\r\n    }\r\n  ]\r\n}"]
[223.815985, "o", "\u001b[?2004h"]
[223.816028, "o", "(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[243.238753, "i", "python3 analyze_and_optimize.py --date 2023-03-15\n"]
[243.238927, "o", "python3 analyze_and_optimize.py --date 2023-03-15\r\n\u001b[?2004l\r"]
[243.246345, "o", "  File \"/home/user/query_optimization/analyze_and_optimize.py\", line 60\r\n    def extract_table_and_columns(query_text):\\n\timport sqlparse\r\n                                               ^\r\nSyntaxError: unexpected character after line continuation character\r\n"]
[243.247768, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
[248.153443, "i", "cat report.json\n"]
[248.153562, "o", "cat report.json\r\n\u001b[?2004l\r"]
[248.154221, "o", "{\r\n  \"meta\": {\r\n    \"date_analyzed\": \"2023-03-15\",\r\n    \"staging_file_sha256\": \"901803ac828f19b83a041a599d4255def673f6d0fd1c5cc516e654209ceec8fd\"\r\n  },\r\n  \"stats\": {\r\n    \"total_queries_extracted\": 5,\r\n    \"unique_patterns\": 4\r\n  },\r\n  \"suggestions\": [\r\n    {\r\n      \"id\": \"2023-03-15T10:00:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM users WHERE id = 1\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:04:00Z\",\r\n      \"normalized_sql\": \"SELECT name, email FROM customers WHERE status = 'active'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T10:05:00Z\",\r\n      \"normalized_sql\": \"SELECT * FROM orders WHERE customer_id = 123 AND order_date > '2023-01-01'\",\r\n      \"index_suggestion\": null\r\n    },\r\n    {\r\n      \"id\": \"2023-03-15T11:00:00Z\",\r\n      \"normalized_sql\": \"UPDATE users SET last_login = NOW() WHERE id = 100\",\r\n      \"index_suggestion\": null\r\n    }\r\n  ]\r\n}"]
[248.154392, "o", "\u001b[?2004h(env) root@46cd37b5-c3fc-40bd-8daa-ac893002f79a:/home/user/query_optimization# "]
