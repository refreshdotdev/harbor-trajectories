```json
{
  "analysis": "The initial setup commands have executed successfully. The output directories (`png_large`, `jpg_old`, `misc_small`) have been created under `/home/user/datasets/organized`. The `organize_log.tsv` file has been created (empty), and the `summary.json` file has been initialized with counts set to zero. The current working directory is now `/home/user`, as required for the next steps.",
  "plan": "The next step is to process the files from `/home/user/datasets/raw`. This will be done using a `find` command to locate all regular files, sorted alphabetically by their absolute paths. This list of files will then be piped into a `while` loop. Inside the loop, each file will be evaluated against the exclusion criteria (broken links, 000 permissions). If not excluded, it will be categorized based on the priority rules: 1. png_large (PNG > 1MB), 2. jpg_old (JPG < July 1, 2021), 3. misc_small (other files <= 512KB, not .txt). For each file moved or copied, collision handling will be applied if necessary, and an entry will be logged to `organize_log.tsv` with the required details (original path, new absolute path, size, mtime, MD5 hash). The summary counts will be updated.",
  "commands": [
    {
      "keystrokes": "find /home/user/datasets/raw -type f -print0 | sort -z > /tmp/sorted_files.txt\n",
      "duration": 1.0
    },
    {
      "keystrokes": "while IFS= read -r -d '' source_file; do\n  # Skip broken symbolic links (ls -L dereferences, -H prevents infinite recursion)\n  if [ ! -L \"${source_file}\" ] || [ ! -e \"$(ls -L \"${source_file}\" 2>/dev/null)\" ]; then\n    echo \"Skipping broken link: ${source_file}\"\n    continue\n  fi\n\n  # Skip files with permission mode 000\n  if [ \"$(stat -c '%a' \"${source_file}\")\" = \"000\" ]; then\n    echo \"Skipping file with 000 permissions: ${source_file}\"\n    continue\n  fi\n\n  ABS_DEST_PATH=\"\"\n  SIZE_BYTES=\"\"\n  MTIME_EPOCH=\"\"\n  MD5_HASH=\"\"\n  LOG_LINE=\"\"\n  FILE_MOVED=0\n  FILE_COPIED=0\n\n  # Rule 1: png_large (Move every *.png file strictly larger than 1,000,000 bytes)\n  if [ \"$(basename \"${source_file}\" | grep -E '\\.png$')\" ]; then\n    fsize=$(stat -c%s \"${source_file}\")\n    if [ ${fsize} -gt 1000000 ]; then\n      BASENAME=$(basename \"${source_file}\")\n      DEST_DIR=\"/home/user/datasets/organized/png_large\"\n      TARGET_PATH=\"${DEST_DIR}/${BASENAME}\"\n      COUNTER=1\n      while [ -e \"${TARGET_PATH}\" ]; do\n        PARENT_DIR_NAME=$(basename \"$(dirname \"${source_file}\")\")\n        TARGET_PATH=\"${DEST_DIR}/${BASENAME%.png}_${PARENT_DIR_NAME}.png\"\n        COUNTER=$((COUNTER+1))\n        if [ ${COUNTER} -gt 1 ]; then TARGET_PATH=\"${TARGET_PATH%.png}_${COUNTER}.png\"; fi\n      done\n      ABS_DEST_PATH=\"${TARGET_PATH}\"\n      mv \"${source_file}\" \"${ABS_DEST_PATH}\"\n      SIZE_BYTES=$(stat -c%s \"${ABS_DEST_PATH}\")\n      MTIME_EPOCH=$(stat -c%Y \"${ABS_DEST_PATH}\")\n      MD5_HASH=$(md5sum \"${ABS_DEST_PATH}\" | awk '{print $1}')\n      LOG_LINE=\"${source_file}\\t${ABS_DEST_PATH}\\t${SIZE_BYTES}\\t${MTIME_EPOCH}\\t${MD5_HASH}\"\n      FILE_MOVED=1\n    fi\n  fi\n\n  # Rule 2: jpg_old (Copy every *.jpg file with mtime strictly before Unix Epoch 1625097600)\n  if [ ${FILE_MOVED} -eq 0 ]; then\n    if [ \"$(basename \"${source_file}\" | grep -E '\\.jpg$')\" ]; then\n      MTIME_SRC=$(stat -c%Y \"${source_file}\")\n      if [ ${MTIME_SRC} -lt 1625097600 ]; then\n        BASENAME=$(basename \"${source_file}\")\n        DEST_DIR=\"/home/user/datasets/organized/jpg_old\"\n        TARGET_PATH=\"${DEST_DIR}/${BASENAME}\"\n        COUNTER=1\n        while [ -e \"${TARGET_PATH}\" ]; do\n          PARENT_DIR_NAME=$(basename \"$(dirname \"${source_file}\")\")\n          TARGET_PATH=\"${DEST_DIR}/${BASENAME%.jpg}_${PARENT_DIR_NAME}.jpg\"\n          COUNTER=$((COUNTER+1))\n          if [ ${COUNTER} -gt 1 ]; then TARGET_PATH=\"${TARGET_PATH%.jpg}_${COUNTER}.jpg\"; fi\n        done\n        ABS_DEST_PATH=\"${TARGET_PATH}\"\n        cp -p \"${source_file}\" \"${ABS_DEST_PATH}\"\n        SIZE_BYTES=$(stat -c%s \"${ABS_DEST_PATH}\")\n        MTIME_EPOCH=$(stat -c%Y \"${ABS_DEST_PATH}\")\n        MD5_HASH=$(md5sum \"${ABS_DEST_PATH}\" | awk '{print $1}')\n        LOG_LINE=\"${source_file}\\t${ABS_DEST_PATH}\\t${SIZE_BYTES}\\t${MTIME_EPOCH}\\t${MD5_HASH}\"\n        FILE_COPIED=1\n      fi\n    fi\n  fi\n\n  # Rule 3: misc_small (Copy every remaining regular file <= 512,000 bytes AND not .txt)\n  if [ ${FILE_MOVED} -eq 0 ] && [ ${FILE_COPIED} -eq 0 ]; then\n    if [ ! \"$(basename \"${source_file}\" | grep -E '\\.txt$')\" ] && [ $(stat -c%s \"${source_file}\") -le 512000 ]; then\n      BASENAME=$(basename \"${source_file}\")\n      DEST_DIR=\"/home/user/datasets/organized/misc_small\"\n      TARGET_PATH=\"${DEST_DIR}/${BASENAME}\"\n      COUNTER=1\n      while [ -e \"${TARGET_PATH}\" ]; do\n        PARENT_DIR_NAME=$(basename \"$(dirname \"${source_file}\")\")\n        TARGET_PATH=\"${DEST_DIR}/${BASENAME%.*}_${PARENT_DIR_NAME}.${BASENAME##*.}\"\n        COUNTER=$((COUNTER+1))\n        if [ ${COUNTER} -gt 1 ]; then TARGET_PATH=\"${TARGET_PATH%.*}_${COUNTER}.${BASENAME##*.}\"; fi\n      done\n      ABS_DEST_PATH=\"${TARGET_PATH}\"\n      cp -p \"${source_file}\" \"${ABS_DEST_PATH}\"\n      SIZE_BYTES=$(stat -c%s \"${ABS_DEST_PATH}\")\n      MTIME_EPOCH=$(stat -c%Y \"${ABS_DEST_PATH}\")\n      MD5_HASH=$(md5sum \"${ABS_DEST_PATH}\" | awk '{print $1}')\n      LOG_LINE=\"${source_file}\\t${ABS_DEST_PATH}\\t${SIZE_BYTES}\\t${MTIME_EPOCH}\\t${MD5_HASH}\"\n      FILE_COPIED=1\n    fi\n  fi\n\n  if [ -n \"${LOG_LINE}\" ]; then\n    echo \"${LOG_LINE}\" >> /home/user/datasets/organized/organize_log.tsv\n    if [ ${FILE_MOVED} -eq 1 ]; then\n      MOVED_COUNT=$(jq '.moved_count += 1' /home/user/datasets/organized/summary.json)\n      jq '.moved_count += 1' /home/user/datasets/organized/summary.json | sponge /home/user/datasets/organized/summary.json\n    fi\n    if [ ${FILE_COPIED} -eq 1 ]; then\n      COPIED_COUNT=$(jq '.copied_count += 1' /home/user/datasets/organized/summary.json)\n      jq '.copied_count += 1' /home/user/datasets/organized/summary.json | sponge /home/user/datasets/organized/summary.json\n    fi\n  fi\ndone < /tmp/sorted_files.txt\n",
      "duration": 20.0
    },
    {
      "keystrokes": "TOTAL_BYTES_SUM=0\n",
      "duration": 0.1
    },
    {
      "keystrokes": "find /home/user/datasets/organized/png_large /home/user/datasets/organized/jpg_old /home/user/datasets/organized/misc_small -type f -printf '\\0' | xargs -0 -I {} sh -c 'SUM=$(stat -c%s \"{}\" 2>/dev/null); if [ -n \"${SUM}\" ]; then TOTAL_BYTES_SUM=$((TOTAL_BYTES_SUM + SUM)); fi' _\n",
      "duration": 5.0
    },
    {
      "keystrokes": "jq \".total_bytes = ${TOTAL_BYTES_SUM}\" /home/user/datasets/organized/summary.json | sponge /home/user/datasets/organized/summary.json\n",
      "duration": 0.5
    },
    {
      "keystrokes": "sort -o /home/user/datasets/organized/organize_log.tsv /home/user/datasets/organized/organize_log.tsv\n",
      "duration": 1.0
    },
    {
      "keystrokes": "rm /tmp/sorted_files.txt\n",
      "duration": 0.1
    },
    {
      "keystrokes": "echo \"Task complete. Final summary:\"\n",
      "duration": 0.1
    },
    {
      "keystrokes": "cat /home/user/datasets/organized/summary.json\n",
      "duration": 0.1
    }
  ],
  "task_complete": true
}
```