{"version": 2, "width": 160, "height": 40, "timestamp": 1771009761, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.006417, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:~# "]
[0.983683, "i", "clear\r"]
[0.983815, "o", "clear\r\n\u001b[?2004l\r"]
[0.984792, "o", "\u001b[H\u001b[J"]
[0.984962, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:~# "]
[67.002412, "i", "mkdir -p /home/user/mlops/summary\n"]
[67.00257, "o", "mkdir -p /home/user/mlops/summary\r\n\u001b[?2004l\r"]
[67.011239, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:~# "]
[67.087069, "i", "cd /home/user\n"]
[67.087178, "o", "cd /home/user\r\n\u001b[?2004l\r"]
[67.087236, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[67.258236, "i", "cat > mlops_processor.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport sys\nimport hashlib\nimport shutil\nimport glob\nimport re\nimport yaml\nfrom datetime import datetime\nfrom collections import defaultdict\n\nclass MLOpsProcessor:\n    def __init__(self):\n        self.experiments_dir = '/home/user/mlops/experiments'\n        self.deprecated_dir = '/home/user/mlops/deprecated'\n        self.summary_dir = '/home/user/mlops/summary'\n        self.corruption_report = os.path.join(self.summary_dir, 'corruption_report.txt')\n        self.inventory_file = os.path.join(self.summary_dir, 'inventory.csv')\n        self.manifest_file = os.path.join(self.summary_dir, 'manifest.yaml')\n        \n        # Create summary directory if it doesn't exist\n        os.makedirs(self.summary_dir, exist_ok=True)\n        \n        # Initialize data structures\n        self.corrupted_lines = []\n        self.inventory_data = []\n        self.artifact_locations = defaultdict(set)\n        self.exclusion_set = set()\n        \n    def load_deprecated_a"]
[67.25835, "i", "ssets(self):\n        \"\"\"Load deprecated asset filenames for exclusion\"\"\"\n        if os.path.exists(self.deprecated_dir):\n            for filename in os.listdir(self.deprecated_dir):\n                # Get root name (filename without extension)\n                root_name = os.path.splitext(filename)[0].lower()\n                self.exclusion_set.add(root_name)\n    \n    def detect_encoding(self, file_path):\n        \"\"\"Detect file encoding\"\"\"\n        encodings_to_try = ['utf-8', 'latin-1', 'utf-16-le']\n        \n        for encoding in encodings_to_try:\n            try:\n                with open(file_path, 'r', encoding=encoding) as f:\n                    f.read(1)  # Test read\n                    return encoding\n            except UnicodeDecodeError:\n                continue\n        return None\n    \n    def is_line_corrupted(self, line):\n        \"\"\"Check if line has odd number of trailing spaces\"\"\"\n        # Remove newline character and check trailing spaces\n        line_without_newline = line.rstrip('\n')\n        t"]
[67.258397, "i", "railing_spaces = len(line_without_newline) - len(line_without_newline.rstrip())\n        return trailing_spaces % 2 != 0\n    \n    def parse_artifact_line(self, line, experiment_dir):\n        \"\"\"Parse valid artifact line\"\"\"\n        # Remove newline and strip whitespace\n        line = line.rstrip('\n').strip()\n        if not line or '|' not in line:\n            return None\n            \n        parts = [part.strip() for part in line.split('|')]\n        if len(parts) < 3:\n            return None\n            \n        artifact_name = parts[0]\n        status = parts[1]\n        metadata = parts[2] if len(parts) > 2 else ''\n        \n        return artifact_name, status, metadata\n    \n    def get_status_override(self, experiment_dir, artifact_name):\n        \"\"\"Check for status override file\"\"\"\n        override_file = os.path.join(experiment_dir, 'status_override.map')\n        if os.path.exists(override_file):\n            try:\n                with open(override_file, 'r', encoding='utf-8') as f:\n                    for li"]
[67.258417, "o", "cat > mlops_processor.py << 'EOF'\r\n"]
[67.25844, "i", "ne in f:\n                        line = line.strip()\n                        if line and '=' in line:\n                            key, value = line.split('=', 1)\n                            key = key.strip()\n                            if key == artifact_name:\n                                return value.strip()\n            except Exception as e:\n                print(f\"Warning: Error reading status override file: {e}\")\n        return None\n    \n    def process_experiments(self):\n        \"\"\"Main processing function\"\"\"\n        print(\"Loading deprecated assets...\")\n        self.load_deprecated_assets()\n        print(f\"Loaded {len(self.exclusion_set)} deprecated assets\")\n        \n        print(\"Processing experiments...\")\n        \n        # Find all artifact files\n        artifact_files = []\n        for root, dirs, files in os.walk(self.experiments_dir):\n            for file in files:\n                if file in ['artifacts.txt', 'artifacts.log']:\n                    artifact_files.append(os.path.join(root, file))"]
[67.258458, "o", "\u001b[?2004l\r"]
[67.258483, "i", "\n        \n        print(f\"Found {len(artifact_files)} artifact files\")\n        \n        for file_path in artifact_files:\n            print(f\"Processing: {file_path}\")\n            experiment_dir = os.path.dirname(file_path)\n            \n            # Detect and convert encoding\n            encoding = self.detect_encoding(file_path)\n            if not encoding:\n                print(f\"Warning: Unknown encoding for {file_path}, skipping\")\n                continue\n            \n            try:\n                with open(file_path, 'r', encoding=encoding) as f:\n                    for line_number, line in enumerate(f, start=1):\n                        # Check for corruption (odd trailing spaces)\n                        if self.is_line_corrupted(line):\n                            self.corrupted_lines.append(f\"{file_path}:{line_number}:{line}\")\n                            continue\n                            \n                        # Parse valid line\n                        parsed = self.parse_artifact_line(line, ex"]
[67.258496, "o", "ne in f:\r\n                        line = line.strip()\r\n                        if line and '=' in line:\r\n                            key, value = line.split('=', 1)\r\n                            key = key.strip()\r\n           "]
[67.25853, "i", "periment_dir)\n                        if not parsed:\n                            continue\n                            \n                        artifact_name, log_status, metadata = parsed\n                        \n                        # Apply status override\n                        final_status = self.get_status_override(experiment_dir, artifact_name) or log_status\n                        \n                        # Filter for READY status\n                        if final_status != 'READY':\n                            continue\n                            \n                        # Check against deprecated assets\n                        root_name = os.path.splitext(artifact_name)[0].lower()\n                        if root_name in self.exclusion_set:\n                            continue\n                            \n                        # Add to inventory\n                        artifact_key = artifact_name.strip()\n                        \n                        # Calculate MD5 hash of artifact name\n       "]
[67.258537, "o", "                 if key == artifact_name:\r\n                                return value.strip()\r\n            except Exception as e:\r\n                print(f\"Warning: Error readin"]
[67.258569, "i", "                 md5_hash = hashlib.md5(artifact_key.encode('utf-8')).hexdigest()\n                        \n                        # Record location (relative path from /home/user/mlops/)\n                        relative_path = os.path.relpath(experiment_dir, os.path.dirname(self.experiments_dir))\n                        self.artifact_locations[artifact_key].add(relative_path)\n                        \n                        # Add to inventory data\n                        self.inventory_data.append(artifact_key)\n                        \n            except Exception as e:\n                print(f\"Error processing {file_path}: {e}\")\n        \n        print(\"Processing complete\")\n    \n    def generate_output(self):\n        \"\"\"Generate output files\"\"\"\n        print(\"Generating output files...\")\n        \n        # Write corrupted lines report\n        if self.corrupted_lines:\n            with open(self.corruption_report, 'w', encoding='utf-8') as f:\n                for line in self.corrupted_lines:\n                  "]
[67.258588, "o", "g status override file: {e}\")\r\n        return None\r\n    \r\n    def process_experiments(self):\r\n        \"\"\"Main proce"]
[67.258599, "i", "  f.write(line)\n        \n        # Generate inventory CSV\n        if self.inventory_data:\n            # Create unique artifacts with sorted locations\n            inventory_entries = []\n            for artifact in sorted(set(self.inventory_data)):\n                locations = ';'.join(sorted(self.artifact_locations[artifact]))\n                inventory_entries.append(f\"{artifact}|{hashlib.md5(artifact.encode('utf-8')).hexdigest()}|{locations}\")\n            \n            # Add header\n            inventory_entries.insert(0, \"Artifact|NameHash|Locations\")\n            \n            with open(self.inventory_file, 'w', encoding='utf-8') as f:\n                f.write('\\n'.join(inventory_entries))\n        \n        # Generate manifest YAML\n        manifest_data = {\n            'total_artifacts': len(set(self.inventory_data)),\n            'inventory_hash': hashlib.sha256(open(self.inventory_file, 'rb').read()).hexdigest(),\n            'generated_at': datetime.utcnow().strftime('%Y-%m-%d')\n        }\n        \n        with op"]
[67.258625, "o", "ssing function\"\"\"\r\n        print(\"Loading deprecated assets...\")\r\n        self.load_deprecated_assets()\r\n        print(f\"Loaded {len(self.exclusion_set)} deprecated assets\")\r\n        \r\n        print(\"Processing experiments...\")\r\n  "]
[67.258646, "i", "en(self.manifest_file, 'w', encoding='utf-8') as f:\n            yaml.dump(manifest_data, f, default_flow_style=False)\n        \n        print(\"Output files generated successfully\")\n\nif __name__ == '__main__':\n    processor = MLOpsProcessor()\n    processor.process_experiments()\n    processor.generate_output()\nEOF\n"]
[67.258668, "o", "      \r\n        # Find all artifact files\r\n        artifact_files = []\r\n        for root, dirs, files in os.walk(self.experiments_dir):\r\n            for file in files:\r\n             "]
[67.258686, "o", "   if file in ['artifacts.txt', 'artifacts.log']:\r\n                    artifact_files.append(os.path.join(root, file))"]
[67.258711, "o", "\r\n        \r\n        print(f\"Found {\u001b[?2004h"]
[67.258749, "o", "> "]
[67.258816, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[67.258841, "o", "\u001b[?2004h"]
[67.258865, "o", "> "]
[67.258889, "o", "import os\r\n\u001b[?2004l\r"]
[67.258914, "o", "\u001b[?2004h> "]
[67.258961, "o", "import sys\r\n\u001b[?2004l\r"]
[67.258994, "o", "\u001b[?2004h> "]
[67.259055, "o", "import hashlib\r\n"]
[67.259087, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[67.259146, "o", "import shutil\r\n\u001b[?2004l\r"]
[67.259179, "o", "\u001b[?2004h> "]
[67.259224, "o", "import glob\r\n\u001b[?2004l\r"]
[67.259255, "o", "\u001b[?2004h> "]
[67.259291, "o", "import re\r\n\u001b[?2004l\r"]
[67.25933, "o", "\u001b[?2004h> "]
[67.259374, "o", "import yaml\r\n\u001b[?2004l\r"]
[67.259403, "o", "\u001b[?2004h"]
[67.259444, "o", "> "]
[67.259547, "o", "from datetime import datetime\r\n\u001b[?2004l\r\u001b[?2004h> "]
[67.2597, "o", "from collections import defaultdict\r\n\u001b[?2004l\r"]
[67.259736, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[67.259753, "o", "\u001b[?2004h"]
[67.259802, "o", "> "]
[67.259866, "o", "class MLOpsProcessor:\r\n\u001b[?2004l\r"]
[67.259896, "o", "\u001b[?2004h> "]
[67.259997, "o", "    def __init__(self):\r\n\u001b[?2004l\r"]
[67.260025, "o", "\u001b[?2004h> "]
[67.260312, "o", "        self.experiments_dir = '/home/user/mlops/experiments'\r\n\u001b[?2004l\r"]
[67.260357, "o", "\u001b[?2004h"]
[67.260384, "o", "> "]
[67.260663, "o", "        self.deprecated_dir = '/home/user/mlops/deprecated'\r\n\u001b[?2004l\r"]
[67.260687, "o", "\u001b[?2004h> "]
[67.260908, "o", "        self.summary_dir = '/home/user/mlops/summary'\r\n\u001b[?2004l\r\u001b[?2004h> "]
[67.261251, "o", "        self.corruption_report = os.path.join(self.summary_dir, 'corruption_report.txt')\r\n\u001b[?2004l\r"]
[67.261322, "o", "\u001b[?2004h> "]
[67.26157, "o", "        self.inventory_file = os.path.join(self.summary_dir, 'inventory.csv')\r\n\u001b[?2004l\r"]
[67.26162, "o", "\u001b[?2004h> "]
[67.261891, "o", "        self.manifest_file = os.path.join(self.summary_dir, 'manifest.yaml')\r\n\u001b[?2004l\r"]
[67.261929, "o", "\u001b[?2004h> "]
[67.261967, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[67.26219, "o", "        # Create summary directory if it doesn't exist\r\n\u001b[?2004l\r"]
[67.262228, "o", "\u001b[?2004h> "]
[67.26243, "o", "        os.makedirs(self.summary_dir, exist_ok=True)\r\n\u001b[?2004l\r"]
[67.262479, "o", "\u001b[?2004h> "]
[67.262525, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[67.262659, "o", "        # Initialize data structures\r\n\u001b[?2004l\r"]
[67.262696, "o", "\u001b[?2004h> "]
[67.262819, "o", "        self.corrupted_lines = []\r\n\u001b[?2004l\r\u001b[?2004h"]
[67.26286, "o", "> "]
[67.26297, "o", "        self.inventory_data = []\r\n\u001b[?2004l\r"]
[67.263006, "o", "\u001b[?2004h> "]
[67.263193, "o", "        self.artifact_locations = defaultdict(set)\r\n\u001b[?2004l\r"]
[67.26323, "o", "\u001b[?2004h> "]
[67.26336, "o", "        self.exclusion_set = set()\r\n\u001b[?2004l\r"]
[67.263401, "o", "\u001b[?2004h> "]
[67.26344, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[67.263587, "o", "    def load_deprecated_assets(self):\r\n\u001b[?2004l\r"]
[67.263625, "o", "\u001b[?2004h> "]
[67.263843, "o", "        \"\"\"Load deprecated asset filenames for exclusion\"\"\"\r\n\u001b[?2004l\r"]
[67.263893, "o", "\u001b[?2004h> "]
[67.26406, "o", "        if os.path.exists(self.deprecated_dir):\r\n\u001b[?2004l\r\u001b[?2004h"]
[67.264098, "o", "> "]
[67.264316, "o", "            for filename in os.listdir(self.deprecated_dir):\r\n\u001b[?2004l\r"]
[67.264356, "o", "\u001b[?2004h> "]
[67.264572, "o", "                # Get root name (filename without extension)\r\n\u001b[?2004l\r"]
[67.264612, "o", "\u001b[?2004h> "]
[67.264853, "o", "                root_name = os.path.splitext(filename)[0].lower()\r\n\u001b[?2004l\r\u001b[?2004h"]
[67.264893, "o", "> "]
[67.265064, "o", "                self.exclusion_set.add(root_name)\r\n\u001b[?2004l\r"]
[67.265108, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[67.265145, "o", "\u001b[?2004h> "]
[67.265296, "o", "    def detect_encoding(self, file_path):\r\n\u001b[?2004l\r\u001b[?2004h"]
[67.265356, "o", "> "]
[67.265468, "o", "        \"\"\"Detect file encoding\"\"\"\r\n\u001b[?2004l\r\u001b[?2004h"]
[67.265535, "o", "> "]
[67.265723, "o", "        encodings_to_try = ['utf-8', 'latin-1', 'utf-16-le']\r\n\u001b[?2004l\r"]
[67.265764, "o", "\u001b[?2004h>         "]
[67.265793, "o", "\r\n\u001b[?2004l\r\u001b[?2004h> "]
[67.265948, "o", "        for encoding in encodings_to_try:\r\n\u001b[?2004l\r"]
[67.26597, "o", "\u001b[?2004h"]
[67.265993, "o", "> "]
[67.266055, "o", "            try:\r\n\u001b[?2004l\r"]
[67.266077, "o", "\u001b[?2004h"]
[67.266097, "o", "> "]
[67.266364, "o", "                with open(file_path, 'r', encoding=encoding) as f:\r\n\u001b[?2004l\r"]
[67.26639, "o", "\u001b[?2004h"]
[67.266412, "o", "> "]
[67.266583, "o", "                    f.read(1)  # Test read\r\n\u001b[?2004l\r"]
[67.266619, "o", "\u001b[?2004h> "]
[67.266763, "o", "                    return encoding\r\n\u001b[?2004l\r"]
[67.266791, "o", "\u001b[?2004h> "]
[67.266965, "o", "            except UnicodeDecodeError:\r\n\u001b[?2004l\r"]
[67.266991, "o", "\u001b[?2004h"]
[67.267009, "o", "> "]
[67.267098, "o", "                continue\r\n\u001b[?2004l\r"]
[67.26712, "o", "\u001b[?2004h"]
[67.267138, "o", "> "]
[67.26721, "o", "        return None\r\n\u001b[?2004l\r"]
[67.267233, "o", "\u001b[?2004h"]
[67.267251, "o", "> "]
[67.267271, "o", "    \r\n\u001b[?2004l\r"]
[67.26729, "o", "\u001b[?2004h> "]
[67.267474, "o", "    def is_line_corrupted(self, line):\r\n\u001b[?2004l\r"]
[67.267515, "o", "\u001b[?2004h> "]
[67.267809, "o", "        \"\"\"Check if line has odd number of trailing spaces\"\"\"\r\n\u001b[?2004l\r"]
[67.267857, "o", "\u001b[?2004h> "]
[67.268091, "o", "        # Remove newline character and check trailing spaces\r\n\u001b[?2004l\r"]
[67.268131, "o", "\u001b[?2004h> "]
[67.268334, "o", "        line_without_newline = line.rstrip('\r\n\u001b[?2004l\r\u001b[?2004h"]
[67.268389, "o", "> ')\r\n\u001b[?2004l\r\u001b[?2004h"]
[67.268424, "o", "> "]
[67.268766, "o", "        trailing_spaces = len(line_without_newline) - len(line_without_newline.rstrip())\r\n\u001b[?2004l\r"]
[67.268813, "o", "\u001b[?2004h> "]
[67.268969, "o", "        return trailing_spaces % 2 != 0\r\n\u001b[?2004l\r"]
[67.269008, "o", "\u001b[?2004h> "]
[67.269038, "o", "    \r\n\u001b[?2004l\r\u001b[?2004h"]
[67.269071, "o", "> "]
[67.269285, "o", "    def parse_artifact_line(self, line, experiment_dir):\r\n\u001b[?2004l\r"]
[67.269331, "o", "\u001b[?2004h> "]
[67.269499, "o", "        \"\"\"Parse valid artifact line\"\"\"\r\n\u001b[?2004l\r"]
[67.269542, "o", "\u001b[?2004h> "]
[67.269721, "o", "        # Remove newline and strip whitespace\r\n\u001b[?2004l\r"]
[67.269761, "o", "\u001b[?2004h> "]
[67.269876, "o", "        line = line.rstrip('\r\n\u001b[?2004l\r"]
[67.269916, "o", "\u001b[?2004h> "]
[67.269947, "o", "').strip()\r\n\u001b[?2004l\r"]
[67.269976, "o", "\u001b[?2004h> "]
[67.27015, "o", "        if not line or '|' not in line:\r\n\u001b[?2004l\r"]
[67.270195, "o", "\u001b[?2004h> "]
[67.270282, "o", "            return None\r\n\u001b[?2004l\r"]
[67.270335, "o", "\u001b[?2004h> "]
[67.270375, "o", "            \r\n\u001b[?2004l\r"]
[67.270412, "o", "\u001b[?2004h> "]
[67.270627, "o", "        parts = [part.strip() for part in line.split('|')]\r\n\u001b[?2004l\r"]
[67.270664, "o", "\u001b[?2004h> "]
[67.270759, "o", "        if len(parts) < 3:\r\n\u001b[?2004l\r"]
[67.270797, "o", "\u001b[?2004h> "]
[67.270875, "o", "            return None\r\n\u001b[?2004l\r"]
[67.270915, "o", "\u001b[?2004h> "]
[67.270955, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h"]
[67.270995, "o", "> "]
[67.271089, "o", "        artifact_name = parts[0]\r\n\u001b[?2004l\r"]
[67.271126, "o", "\u001b[?2004h> "]
[67.27121, "o", "        status = parts[1]\r\n\u001b[?2004l\r"]
[67.271249, "o", "\u001b[?2004h> "]
[67.271452, "o", "        metadata = parts[2] if len(parts) > 2 else ''\r\n\u001b[?2004l\r"]
[67.271492, "o", "\u001b[?2004h>         "]
[67.27153, "o", "\r\n\u001b[?2004l\r\u001b[?2004h> "]
[67.271695, "o", "        return artifact_name, status, metadata\r\n\u001b[?2004l\r"]
[67.271726, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[67.271763, "o", "\u001b[?2004h> "]
[67.272008, "o", "    def get_status_override(self, experiment_dir, artifact_name):\r\n\u001b[?2004l\r"]
[67.27206, "o", "\u001b[?2004h> "]
[67.272231, "o", "        \"\"\"Check for status override file\"\"\"\r\n\u001b[?2004l\r"]
[67.272268, "o", "\u001b[?2004h"]
[67.272284, "o", "> "]
[67.272536, "o", "        override_file = os.path.join(experiment_dir, 'status_override.map')\r\n\u001b[?2004l\r"]
[67.27257, "o", "\u001b[?2004h> "]
[67.27272, "o", "        if os.path.exists(override_file):\r\n\u001b[?2004l\r"]
[67.272746, "o", "\u001b[?2004h> "]
[67.272812, "o", "            try:\r\n\u001b[?2004l\r"]
[67.272836, "o", "\u001b[?2004h> "]
[67.273082, "o", "                with open(override_file, 'r', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[67.273121, "o", "\u001b[?2004h> "]
[67.273232, "o", "                    for line in f:\r\n\u001b[?2004l\r"]
[67.273265, "o", "\u001b[?2004h> "]
[67.273425, "o", "                        line = line.strip()\r\n\u001b[?2004l\r"]
[67.273468, "o", "\u001b[?2004h> "]
[67.273612, "o", "                        if line and '=' in line:\r\n\u001b[?2004l\r"]
[67.273646, "o", "\u001b[?2004h> "]
[67.27384, "o", "                            key, value = line.split('=', 1)\r\n\u001b[?2004l\r"]
[67.273864, "o", "\u001b[?2004h> "]
[67.274023, "o", "                            key = key.strip()\r\n\u001b[?2004l\r"]
[67.274043, "o", "\u001b[?2004h> "]
[67.27423, "o", "                            if key == artifact_name:\r\n\u001b[?2004l\r"]
[67.274258, "o", "\u001b[?2004h> "]
[67.274433, "o", "                                return value.strip()\r\n\u001b[?2004l\r"]
[67.274458, "o", "\u001b[?2004h> "]
[67.274575, "o", "            except Exception as e:\r\n\u001b[?2004l\r"]
[67.274597, "o", "\u001b[?2004h> "]
[67.274857, "o", "                print(f\"Warning: Error reading status override file: {e}\")\r\n\u001b[?2004l\r"]
[67.274875, "o", "\u001b[?2004h> "]
[67.274954, "o", "        return None\r\n\u001b[?2004l\r"]
[67.274972, "o", "\u001b[?2004h"]
[67.274992, "o", "> "]
[67.275009, "o", "    \r\n\u001b[?2004l\r"]
[67.275025, "o", "\u001b[?2004h> "]
[67.275153, "o", "    def process_experiments(self):\r\n\u001b[?2004l\r"]
[67.275173, "o", "\u001b[?2004h> "]
[67.275317, "o", "        \"\"\"Main processing function\"\"\"\r\n\u001b[?2004l\r"]
[67.275338, "o", "\u001b[?2004h> "]
[67.275498, "o", "        print(\"Loading deprecated assets...\")\r\n\u001b[?2004l\r"]
[67.275524, "o", "\u001b[?2004h> "]
[67.275649, "o", "        self.load_deprecated_assets()\r\n\u001b[?2004l\r"]
[67.275668, "o", "\u001b[?2004h> "]
[67.275902, "o", "        print(f\"Loaded {len(self.exclusion_set)} deprecated assets\")\r\n\u001b[?2004l\r"]
[67.275918, "o", "\u001b[?2004h> "]
[67.275963, "o", "        \r\n\u001b[?2004l\r"]
[67.275984, "o", "\u001b[?2004h> "]
[67.276133, "o", "        print(\"Processing experiments...\")\r\n\u001b[?2004l\r"]
[67.276156, "o", "\u001b[?2004h> "]
[67.276196, "o", "        \r\n\u001b[?2004l\r"]
[67.276211, "o", "\u001b[?2004h"]
[67.276232, "o", "> "]
[67.276349, "o", "        # Find all artifact files\r\n\u001b[?2004l\r"]
[67.276377, "o", "\u001b[?2004h"]
[67.276397, "o", "> "]
[67.276483, "o", "        artifact_files = []\r\n\u001b[?2004l\r"]
[67.276507, "o", "\u001b[?2004h> "]
[67.276716, "o", "        for root, dirs, files in os.walk(self.experiments_dir):\r\n\u001b[?2004l\r"]
[67.276757, "o", "\u001b[?2004h> "]
[67.276864, "o", "            for file in files:\r\n\u001b[?2004l\r"]
[67.276884, "o", "\u001b[?2004h> "]
[67.277135, "o", "                if file in ['artifacts.txt', 'artifacts.log']:\r\n\u001b[?2004l\r"]
[67.277172, "o", "\u001b[?2004h> "]
[67.277431, "o", "                    artifact_files.append(os.path.join(root, file))\r\n\u001b[?2004l\r"]
[67.277472, "o", "\u001b[?2004h> "]
[67.277498, "o", "        \r\n\u001b[?2004l\r"]
[67.277521, "o", "\u001b[?2004h> "]
[67.277746, "o", "        print(f\"Found {len(artifact_files)} artifact files\")\r\n\u001b[?2004l\r"]
[67.277785, "o", "\u001b[?2004h> "]
[67.277815, "o", "        \r\n\u001b[?2004l\r"]
[67.277838, "o", "\u001b[?2004h> "]
[67.277988, "o", "        for file_path in artifact_files:\r\n\u001b[?2004l\r"]
[67.278013, "o", "\u001b[?2004h> "]
[67.27819, "o", "            print(f\"Processing: {file_path}\")\r\n\u001b[?2004l\r"]
[67.278216, "o", "\u001b[?2004h> "]
[67.278439, "o", "            experiment_dir = os.path.dirname(file_path)\r\n\u001b[?2004l\r"]
[67.278479, "o", "\u001b[?2004h> "]
[67.278515, "o", "            \r\n\u001b[?2004l\r"]
[67.27854, "o", "\u001b[?2004h> "]
[67.278704, "o", "            # Detect and convert encoding\r\n\u001b[?2004l\r"]
[67.278746, "o", "\u001b[?2004h> "]
[67.278915, "o", "            encoding = self.detect_encoding(file_path)\r\n\u001b[?2004l\r"]
[67.278955, "o", "\u001b[?2004h> "]
[67.279013, "o", "            if not encoding:\r\n\u001b[?2004l\r"]
[67.279035, "o", "\u001b[?2004h"]
[67.279057, "o", "> "]
[67.27924, "o", "                print(f\"Warning: Unknown encoding for {file_path}, skipping\")\r\n\u001b[?2004l\r"]
[67.279276, "o", "\u001b[?2004h> "]
[67.279336, "o", "                continue\r\n\u001b[?2004l\r"]
[67.279359, "o", "\u001b[?2004h> "]
[67.279389, "o", "            \r\n\u001b[?2004l\r"]
[67.279415, "o", "\u001b[?2004h> "]
[67.279459, "o", "            try:\r\n\u001b[?2004l\r"]
[67.279482, "o", "\u001b[?2004h> "]
[67.279653, "o", "                with open(file_path, 'r', encoding=encoding) as f:\r\n\u001b[?2004l\r"]
[67.279688, "o", "\u001b[?2004h> "]
[67.279847, "o", "                    for line_number, line in enumerate(f, start=1):\r\n\u001b[?2004l\r"]
[67.27987, "o", "\u001b[?2004h> "]
[67.280045, "o", "                        # Check for corruption (odd trailing spaces)\r\n\u001b[?2004l\r"]
[67.280066, "o", "\u001b[?2004h> "]
[67.280219, "o", "                        if self.is_line_corrupted(line):\r\n\u001b[?2004l\r"]
[67.280261, "o", "\u001b[?2004h> "]
[67.280488, "o", "                            self.corrupted_lines.append(f\"{file_path}:{line_number}:{line}\")\r\n\u001b[?2004l\r"]
[67.280535, "o", "\u001b[?2004h> "]
[67.280617, "o", "                            continue\r\n\u001b[?2004l\r"]
[67.280643, "o", "\u001b[?2004h> "]
[67.280735, "o", "                            \r\n\u001b[?2004l\r"]
[67.280764, "o", "\u001b[?2004h> "]
[67.280869, "o", "                        # Parse valid line\r\n\u001b[?2004l\r"]
[67.280895, "o", "\u001b[?2004h> "]
[67.281106, "o", "                        parsed = self.parse_artifact_line(line, experiment_dir)\r\n\u001b[?2004l\r"]
[67.281133, "o", "\u001b[?2004h> "]
[67.281233, "o", "                        if not parsed:\r\n\u001b[?2004l\r"]
[67.281259, "o", "\u001b[?2004h> "]
[67.281368, "o", "                            continue\r\n\u001b[?2004l\r"]
[67.281392, "o", "\u001b[?2004h> "]
[67.281469, "o", "                            \r\n\u001b[?2004l\r"]
[67.281491, "o", "\u001b[?2004h> "]
[67.281672, "o", "                        artifact_name, log_status, metadata = parsed\r\n\u001b[?2004l\r"]
[67.281703, "o", "\u001b[?2004h> "]
[67.281767, "o", "                        \r\n\u001b[?2004l\r"]
[67.281791, "o", "\u001b[?2004h"]
[67.281814, "o", "> "]
[67.281924, "o", "                        # Apply status override\r\n\u001b[?2004l\r"]
[67.281947, "o", "\u001b[?2004h> "]
[67.282227, "o", "                        final_status = self.get_status_override(experiment_dir, artifact_name) or log_status\r\n\u001b[?2004l\r"]
[67.28225, "o", "\u001b[?2004h> "]
[67.282331, "o", "                        \r\n\u001b[?2004l\r"]
[67.282375, "o", "\u001b[?2004h> "]
[67.282496, "o", "                        # Filter for READY status\r\n\u001b[?2004l\r"]
[67.282523, "o", "\u001b[?2004h"]
[67.282547, "o", "> "]
[67.28266, "o", "                        if final_status != 'READY':\r\n\u001b[?2004l\r"]
[67.282687, "o", "\u001b[?2004h> "]
[67.282775, "o", "                            continue\r\n\u001b[?2004l\r"]
[67.282792, "o", "\u001b[?2004h"]
[67.282814, "o", "> "]
[67.282872, "o", "                            \r\n\u001b[?2004l\r"]
[67.282911, "o", "\u001b[?2004h> "]
[67.283052, "o", "                        # Check against deprecated assets\r\n\u001b[?2004l\r"]
[67.283075, "o", "\u001b[?2004h> "]
[67.283288, "o", "                        root_name = os.path.splitext(artifact_name)[0].lower()\r\n\u001b[?2004l\r"]
[67.283336, "o", "\u001b[?2004h> "]
[67.283479, "o", "                        if root_name in self.exclusion_set:\r\n\u001b[?2004l\r"]
[67.2835, "o", "\u001b[?2004h"]
[67.283522, "o", "> "]
[67.283616, "o", "                            continue\r\n\u001b[?2004l\r"]
[67.283639, "o", "\u001b[?2004h"]
[67.283659, "o", "> "]
[67.283726, "o", "                            \r\n\u001b[?2004l\r"]
[67.283744, "o", "\u001b[?2004h"]
[67.283763, "o", "> "]
[67.283871, "o", "                        # Add to inventory\r\n\u001b[?2004l\r"]
[67.283886, "o", "\u001b[?2004h"]
[67.283913, "o", "> "]
[67.284061, "o", "                        artifact_key = artifact_name.strip()\r\n\u001b[?2004l\r"]
[67.284081, "o", "\u001b[?2004h"]
[67.284101, "o", "> "]
[67.284164, "o", "                        \r\n\u001b[?2004l\r"]
[67.284184, "o", "\u001b[?2004h"]
[67.284203, "o", "> "]
[67.284365, "o", "                        # Calculate MD5 hash of artifact name\r\n\u001b[?2004l\r"]
[67.284382, "o", "\u001b[?2004h"]
[67.284403, "o", "> "]
[67.284627, "o", "                        md5_hash = hashlib.md5(artifact_key.encode('utf-8')).hexdigest()\r\n\u001b[?2004l\r"]
[67.284648, "o", "\u001b[?2004h"]
[67.284666, "o", "> "]
[67.284731, "o", "                        \r\n\u001b[?2004l\r"]
[67.28475, "o", "\u001b[?2004h> "]
[67.284962, "o", "                        # Record location (relative path from /home/user/mlops/)\r\n\u001b[?2004l\r"]
[67.284981, "o", "\u001b[?2004h"]
[67.285001, "o", "> "]
[67.285276, "o", "                        relative_path = os.path.relpath(experiment_dir, os.path.dirname(self.experiments_dir))\r\n\u001b[?2004l\r"]
[67.285294, "o", "\u001b[?2004h"]
[67.285317, "o", "> "]
[67.28552, "o", "                        self.artifact_locations[artifact_key].add(relative_path)\r\n\u001b[?2004l\r"]
[67.285537, "o", "\u001b[?2004h"]
[67.285555, "o", "> "]
[67.285619, "o", "                        \r\n\u001b[?2004l\r"]
[67.285637, "o", "\u001b[?2004h"]
[67.285655, "o", "> "]
[67.285775, "o", "                        # Add to inventory data\r\n\u001b[?2004l\r"]
[67.285793, "o", "\u001b[?2004h"]
[67.285813, "o", "> "]
[67.285975, "o", "                        self.inventory_data.append(artifact_key)\r\n\u001b[?2004l\r"]
[67.285995, "o", "\u001b[?2004h> "]
[67.286073, "o", "                        \r\n\u001b[?2004l\r"]
[67.286092, "o", "\u001b[?2004h"]
[67.286111, "o", "> "]
[67.2862, "o", "            except Exception as e:\r\n\u001b[?2004l\r"]
[67.286219, "o", "\u001b[?2004h"]
[67.286239, "o", "> "]
[67.286395, "o", "                print(f\"Error processing {file_path}: {e}\")\r\n\u001b[?2004l\r"]
[67.286414, "o", "\u001b[?2004h> "]
[67.28645, "o", "        \r\n\u001b[?2004l\r"]
[67.286469, "o", "\u001b[?2004h> "]
[67.286575, "o", "        print(\"Processing complete\")\r\n\u001b[?2004l\r"]
[67.286596, "o", "\u001b[?2004h> "]
[67.286619, "o", "    \r\n\u001b[?2004l\r"]
[67.28664, "o", "\u001b[?2004h> "]
[67.286731, "o", "    def generate_output(self):\r\n\u001b[?2004l\r"]
[67.286751, "o", "\u001b[?2004h"]
[67.286771, "o", "> "]
[67.286865, "o", "        \"\"\"Generate output files\"\"\"\r\n\u001b[?2004l\r"]
[67.286899, "o", "\u001b[?2004h> "]
[67.287011, "o", "        print(\"Generating output files...\")\r\n\u001b[?2004l\r"]
[67.287045, "o", "\u001b[?2004h> "]
[67.28707, "o", "        \r\n\u001b[?2004l\r"]
[67.287096, "o", "\u001b[?2004h> "]
[67.287199, "o", "        # Write corrupted lines report\r\n\u001b[?2004l\r"]
[67.287234, "o", "\u001b[?2004h> "]
[67.287324, "o", "        if self.corrupted_lines:\r\n\u001b[?2004l\r"]
[67.287359, "o", "\u001b[?2004h> "]
[67.287541, "o", "            with open(self.corruption_report, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[67.28756, "o", "\u001b[?2004h"]
[67.287579, "o", "> "]
[67.287703, "o", "                for line in self.corrupted_lines:\r\n\u001b[?2004l\r"]
[67.287725, "o", "\u001b[?2004h> "]
[67.287821, "o", "                    f.write(line)\r\n\u001b[?2004l\r"]
[67.287838, "o", "\u001b[?2004h"]
[67.287859, "o", "> "]
[67.287879, "o", "        \r\n\u001b[?2004l\r"]
[67.287929, "o", "\u001b[?2004h> "]
[67.288001, "o", "        # Generate inventory CSV\r\n\u001b[?2004l\r"]
[67.288046, "o", "\u001b[?2004h> "]
[67.288132, "o", "        if self.inventory_data:\r\n\u001b[?2004l\r"]
[67.288166, "o", "\u001b[?2004h> "]
[67.288359, "o", "            # Create unique artifacts with sorted locations\r\n\u001b[?2004l\r"]
[67.288407, "o", "\u001b[?2004h> "]
[67.288513, "o", "            inventory_entries = []\r\n\u001b[?2004l\r"]
[67.288533, "o", "\u001b[?2004h"]
[67.288551, "o", "> "]
[67.288717, "o", "            for artifact in sorted(set(self.inventory_data)):\r\n\u001b[?2004l\r"]
[67.288733, "o", "\u001b[?2004h"]
[67.288751, "o", "> "]
[67.288957, "o", "                locations = ';'.join(sorted(self.artifact_locations[artifact]))\r\n\u001b[?2004l\r"]
[67.288987, "o", "\u001b[?2004h> "]
[67.289285, "o", "                inventory_entries.append(f\"{artifact}|{hashlib.md5(artifact.encode('utf-8')).hexdigest()}|{locations}\")\r\n\u001b[?2004l\r"]
[67.289309, "o", "\u001b[?2004h"]
[67.297704, "o", ">             \r\n\u001b[?2004l\r"]
[67.297733, "o", "\u001b[?2004h> "]
[67.297814, "o", "            # Add header\r\n\u001b[?2004l\r"]
[67.297831, "o", "\u001b[?2004h> "]
[67.298026, "o", "            inventory_entries.insert(0, \"Artifact|NameHash|Locations\")\r\n\u001b[?2004l\r"]
[67.298044, "o", "\u001b[?2004h> "]
[67.298095, "o", "            \r\n\u001b[?2004l\r"]
[67.298118, "o", "\u001b[?2004h> "]
[67.298317, "o", "            with open(self.inventory_file, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[67.298351, "o", "\u001b[?2004h> "]
[67.298488, "o", "                f.write('\\n'.join(inventory_entries))\r\n\u001b[?2004l\r"]
[67.298511, "o", "\u001b[?2004h"]
[67.298531, "o", "> "]
[67.298553, "o", "        \r\n\u001b[?2004l\r"]
[67.298576, "o", "\u001b[?2004h> "]
[67.298673, "o", "        # Generate manifest YAML\r\n\u001b[?2004l\r"]
[67.298691, "o", "\u001b[?2004h"]
[67.298711, "o", "> "]
[67.298781, "o", "        manifest_data = {\r\n\u001b[?2004l\r"]
[67.298802, "o", "\u001b[?2004h> "]
[67.298972, "o", "            'total_artifacts': len(set(self.inventory_data)),\r\n\u001b[?2004l\r"]
[67.299003, "o", "\u001b[?2004h> "]
[67.29925, "o", "            'inventory_hash': hashlib.sha256(open(self.inventory_file, 'rb').read()).hexdigest(),\r\n\u001b[?2004l\r"]
[67.299273, "o", "\u001b[?2004h> "]
[67.299464, "o", "            'generated_at': datetime.utcnow().strftime('%Y-%m-%d')\r\n\u001b[?2004l\r"]
[67.299492, "o", "\u001b[?2004h"]
[67.299516, "o", "> "]
[67.299548, "o", "        }\r\n\u001b[?2004l\r"]
[67.299567, "o", "\u001b[?2004h"]
[67.299588, "o", "> "]
[67.29961, "o", "        \r\n\u001b[?2004l\r"]
[67.299635, "o", "\u001b[?2004h> "]
[67.299822, "o", "        with open(self.manifest_file, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[67.299855, "o", "\u001b[?2004h> "]
[67.300028, "o", "            yaml.dump(manifest_data, f, default_flow_style=False)\r\n\u001b[?2004l\r"]
[67.300048, "o", "\u001b[?2004h> "]
[67.300084, "o", "        \r\n\u001b[?2004l\r"]
[67.300101, "o", "\u001b[?2004h> "]
[67.30025, "o", "        print(\"Output files generated successfully\")\r\n\u001b[?2004l\r"]
[67.300267, "o", "\u001b[?2004h"]
[67.300279, "o", "> \r\n\u001b[?2004l\r"]
[67.300337, "o", "\u001b[?2004h> "]
[67.300401, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r"]
[67.300425, "o", "\u001b[?2004h"]
[67.300451, "o", "> "]
[67.300531, "o", "    processor = MLOpsProcessor()\r\n\u001b[?2004l\r"]
[67.300559, "o", "\u001b[?2004h> "]
[67.30066, "o", "    processor.process_experiments()\r\n\u001b[?2004l\r"]
[67.300688, "o", "\u001b[?2004h> "]
[67.30078, "o", "    processor.generate_output()\r\n\u001b[?2004l\r"]
[67.300811, "o", "\u001b[?2004h> EOF\r\n"]
[67.300832, "o", "\u001b[?2004l\r"]
[67.302338, "o", "\u001b[?2004h"]
[67.302349, "o", "root@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[68.37075, "i", "chmod +x mlops_processor.py\n"]
[68.370911, "o", "chmod +x mlops_processor.py\r\n\u001b[?2004l\r"]
[68.372135, "o", "\u001b[?2004h"]
[68.372173, "o", "root@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[68.440872, "i", "python3 mlops_processor.py\n"]
[68.441006, "o", "python3 mlops_processor.py\r\n\u001b[?2004l\r"]
[68.449106, "o", "  File \"/home/user/mlops_processor.py\", line 54\r\n    line_without_newline = line.rstrip('\r\n                                       ^\r\nSyntaxError: unterminated string literal (detected at line 54)\r\n"]
[68.450607, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[137.339664, "i", "rm -f mlops_processor.py\n"]
[137.33982, "o", "rm -f mlops_processor.py\r\n\u001b[?2004l\r"]
[137.341371, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[137.485656, "i", "cat > mlops_processor.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport sys\nimport hashlib\nimport shutil\nimport glob\nimport re\nimport yaml\nfrom datetime import datetime\nfrom collections import defaultdict\n\nclass MLOpsProcessor:\n    def __init__(self):\n        self.experiments_dir = '/home/user/mlops/experiments'\n        self.deprecated_dir = '/home/user/mlops/deprecated'\n        self.summary_dir = '/home/user/mlops/summary'\n        self.corruption_report = os.path.join(self.summary_dir, 'corruption_report.txt')\n        self.inventory_file = os.path.join(self.summary_dir, 'inventory.csv')\n        self.manifest_file = os.path.join(self.summary_dir, 'manifest.yaml')\n        \n        # Create summary directory if it doesn't exist\n        os.makedirs(self.summary_dir, exist_ok=True)\n        \n        # Initialize data structures\n        self.corrupted_lines = []\n        self.inventory_data = []\n        self.artifact_locations = defaultdict(set)\n        self.exclusion_set = set()\n        \n    def load_deprecated_a"]
[137.485793, "i", "ssets(self):\n        \"\"\"Load deprecated asset filenames for exclusion\"\"\"\n        if os.path.exists(self.deprecated_dir):\n            for filename in os.listdir(self.deprecated_dir):\n                # Get root name (filename without extension)\n                root_name = os.path.splitext(filename)[0].lower()\n                self.exclusion_set.add(root_name)\n    \n    def detect_encoding(self, file_path):\n        \"\"\"Detect file encoding\"\"\"\n        encodings_to_try = ['utf-8', 'latin-1', 'utf-16-le']\n        \n        for encoding in encodings_to_try:\n            try:\n                with open(file_path, 'r', encoding=encoding) as f:\n                    f.read(1)  # Test read\n                    return encoding\n            except UnicodeDecodeError:\n                continue\n        return None\n    \n    def is_line_corrupted(self, line):\n        \"\"\"Check if line has odd number of trailing spaces\"\"\"\n        # Remove newline character and check trailing spaces\n        line_without_newline = line.rstrip('\\n')\n        "]
[137.485825, "o", "cat > mlops_processor.py << 'EOF'\r\n\u001b[?2004l\r"]
[137.485864, "i", "trailing_spaces = len(line_without_newline) - len(line_without_newline.rstrip())\n        return trailing_spaces % 2 != 0\n    \n    def parse_artifact_line(self, line, experiment_dir):\n        \"\"\"Parse valid artifact line\"\"\"\n        # Remove newline and strip whitespace\n        line = line.rstrip('\\n').strip()\n        if not line or '|' not in line:\n            return None\n            \n        parts = [part.strip() for part in line.split('|')]\n        if len(parts) < 3:\n            return None\n            \n        artifact_name = parts[0]\n        status = parts[1]\n        metadata = parts[2] if len(parts) > 2 else ''\n        \n        return artifact_name, status, metadata\n    \n    def get_status_override(self, experiment_dir, artifact_name):\n        \"\"\"Check for status override file\"\"\"\n        override_file = os.path.join(experiment_dir, 'status_override.map')\n        if os.path.exists(override_file):\n            try:\n                with open(override_file, 'r', encoding='utf-8') as f:\n                    for "]
[137.485891, "o", "ssets(self):\r\n        \"\"\"Load deprecated asset filenames for exclusion\"\"\"\r\n        if os.path.exists(self.deprecated_dir):\r\n            for filename in os.listdir(self.deprecated_dir):\r\n                # Get root name (filename without extension)\r\n                root_name = os.path.splitext(filename)[0].lower()\r\n                self.exclusion_set.add(root_name)\r\n    \r\n    def detect_encoding(self, file_path):\r\n        \"\"\"Detect file e"]
[137.485932, "i", "line in f:\n                        line = line.strip()\n                        if line and '=' in line:\n                            key, value = line.split('=', 1)\n                            key = key.strip()\n                            if key == artifact_name:\n                                return value.strip()\n            except Exception as e:\n                print(f\"Warning: Error reading status override file: {e}\")\n        return None\n    \n    def process_experiments(self):\n        \"\"\"Main processing function\"\"\"\n        print(\"Loading deprecated assets...\")\n        self.load_deprecated_assets()\n        print(f\"Loaded {len(self.exclusion_set)} deprecated assets\")\n        \n        print(\"Processing experiments...\")\n        \n        # Find all artifact files\n        artifact_files = []\n        for root, dirs, files in os.walk(self.experiments_dir):\n            for file in files:\n                if file in ['artifacts.txt', 'artifacts.log']:\n                    artifact_files.append(os.path.join(root, file"]
[137.485947, "o", "ncoding\"\"\"\r\n        encodings_to_try = ['utf-8', 'latin-1', 'utf-16-le']\r\n        \r\n        for encoding in encodings_to_try:\r\n            try:\r\n                with open(file_path, 'r', encoding=encoding) as f:\r\n                    f.read(1)  # Test read\r\n                    return encoding\r\n            except UnicodeDecodeError:\r\n                continue\r\n        return"]
[137.485981, "i", "))\n        \n        print(f\"Found {len(artifact_files)} artifact files\")\n        \n        for file_path in artifact_files:\n            print(f\"Processing: {file_path}\")\n            experiment_dir = os.path.dirname(file_path)\n            \n            # Detect and convert encoding\n            encoding = self.detect_encoding(file_path)\n            if not encoding:\n                print(f\"Warning: Unknown encoding for {file_path}, skipping\")\n                continue\n            \n            try:\n                with open(file_path, 'r', encoding=encoding) as f:\n                    for line_number, line in enumerate(f, start=1):\n                        # Check for corruption (odd trailing spaces)\n                        if self.is_line_corrupted(line):\n                            self.corrupted_lines.append(f\"{file_path}:{line_number}:{line}\")\n                            continue\n                            \n                        # Parse valid line\n                        parsed = self.parse_artifact_line(line, "]
[137.486008, "o", " None\r\n    \r\n    def is_line_corrupted(self, line):\r\n        \"\"\"Check if line has odd number of trailing spaces\"\"\"\r\n        # Remove newline character and check trailing spaces\r\n        line_without_newline = line.rstrip('\\n')\r\n        trailing_spaces = l"]
[137.486045, "i", "experiment_dir)\n                        if not parsed:\n                            continue\n                            \n                        artifact_name, log_status, metadata = parsed\n                        \n                        # Apply status override\n                        final_status = self.get_status_override(experiment_dir, artifact_name) or log_status\n                        \n                        # Filter for READY status\n                        if final_status != 'READY':\n                            continue\n                            \n                        # Check against deprecated assets\n                        root_name = os.path.splitext(artifact_name)[0].lower()\n                        if root_name in self.exclusion_set:\n                            continue\n                            \n                        # Add to inventory\n                        artifact_key = artifact_name.strip()\n                        \n                        # Calculate MD5 hash of artifact name\n     "]
[137.486068, "o", "en(line_without_newline) - len(line_without_newline.rstrip())\r\n        return trailing_spaces % 2 != 0\r\n    \r\n    def parse_artifact_line(self, line, experiment_dir):\r\n        \"\"\"Parse valid artifact line\"\"\"\r\n        # Remove newline and strip whitespace\r\n        line = line.rstrip('\\n').strip()\r\n        if not line or '|' not in line:\r\n            return None\r\n            \r\n        parts = [part.strip() for part in line.split('|')]\r\n        if len(parts) < 3:\r\n            ret"]
[137.486096, "i", "                   md5_hash = hashlib.md5(artifact_key.encode('utf-8')).hexdigest()\n                        \n                        # Record location (relative path from /home/user/mlops/)\n                        relative_path = os.path.relpath(experiment_dir, os.path.dirname(self.experiments_dir))\n                        self.artifact_locations[artifact_key].add(relative_path)\n                        \n                        # Add to inventory data\n                        self.inventory_data.append(artifact_key)\n                        \n            except Exception as e:\n                print(f\"Error processing {file_path}: {e}\")\n        \n        print(\"Processing complete\")\n        \n    def generate_output(self):\n        \"\"\"Generate output files\"\"\"\n        print(\"Generating output files...\")\n        \n        # Write corrupted lines report\n        if self.corrupted_lines:\n            with open(self.corruption_report, 'w', encoding='utf-8') as f:\n                for line in self.corrupted_lines:\n            "]
[137.486121, "o", "urn None\r\n            \r\n        artifact_name = parts[0]\r\n        status = parts[1]\r\n        metadata = parts[2] if len(parts) > 2 else ''\r\n        \r\n        return artifact_name, status, metadata\r\n    \r\n    def get_status_override(self, experiment"]
[137.486161, "i", "        f.write(line)\n        \n        # Generate inventory CSV\n        if self.inventory_data:\n            # Create unique artifacts with sorted locations\n            inventory_entries = []\n            for artifact in sorted(set(self.inventory_data)):\n                locations = ';'.join(sorted(self.artifact_locations[artifact]))\n                inventory_entries.append(f\"{artifact}|{hashlib.md5(artifact.encode('utf-8')).hexdigest()}|{locations}\")\n            \n            # Add header\n            inventory_entries.insert(0, \"Artifact|NameHash|Locations\")\n            \n            with open(self.inventory_file, 'w', encoding='utf-8') as f:\n                f.write('\\n'.join(inventory_entries))\n        \n        # Generate manifest YAML\n        manifest_data = {\n            'total_artifacts': len(set(self.inventory_data)),\n            'inventory_hash': hashlib.sha256(open(self.inventory_file, 'rb').read()).hexdigest(),\n            'generated_at': datetime.utcnow().strftime('%Y-%m-%d')\n        }\n        \n        w"]
[137.486186, "o", "_dir, artifact_name):\r\n        \"\"\"Check for status override file\"\"\"\r\n        override_file = os.path.join(experiment_dir, 'status_override.map')\r\n        if os.path.exists(override_file):\r\n            try:\r\n                with open(override_file, 'r', encoding='utf-8') as f:\r\n                    for line in f:\r\n                        line = line.strip()\r\n                        if line and '=' in line:\r\n                            key, value = line"]
[137.486213, "i", "ith open(self.manifest_file, 'w', encoding='utf-8') as f:\n            yaml.dump(manifest_data, f, default_flow_style=False)\n        \n        print(\"Output files generated successfully\")\n\nif __name__ == '__main__':\n    processor = MLOpsProcessor()\n    processor.process_experiments()\n    processor.generate_output()\nEOF\n"]
[137.486227, "o", ".split('=', 1)\r\n                            key = key.strip()\r\n                            if key == artifact_name:\r\n                                return value.strip()\r\n            except Exception as e:\r\n                print(f\"Warning: Error reading status override file: {e}\")\r\n        return N"]
[137.486279, "o", "one\r\n    \r\n    def process_experiments(self):\r\n        \"\"\"Main processing function\"\"\"\r\n        print(\"Loading deprecated assets...\")\r\n        self.load_deprecated_assets()\r\n        print(f\"Loaded {len(self.exclusion_set)} deprecated assets\")\r\n        \r\n        print(\"Processing experiments...\")\r\n        \r\n   "]
[137.48632, "o", "     # Find all artifact files\r\n        artifact_files = []\r\n        for root, dirs, files in os.walk(self.experiments_dir):\r\n            for file in files:\r\n            "]
[137.486352, "o", "    if file in ['artifacts.txt', 'artifacts.log']:\r\n                    artifact_files.append(os.path.join(root, file))\r\n        \r\n        print(f\"Found"]
[137.486382, "o", "\u001b[?2004h> "]
[137.486474, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[137.4865, "o", "\u001b[?2004h> "]
[137.486553, "o", "import os\r\n\u001b[?2004l\r"]
[137.486577, "o", "\u001b[?2004h> "]
[137.486614, "o", "import sys\r\n\u001b[?2004l\r"]
[137.486639, "o", "\u001b[?2004h> "]
[137.486699, "o", "import hashlib\r\n\u001b[?2004l\r"]
[137.486724, "o", "\u001b[?2004h> "]
[137.486775, "o", "import shutil\r\n\u001b[?2004l\r"]
[137.486799, "o", "\u001b[?2004h> "]
[137.486839, "o", "import glob\r\n\u001b[?2004l\r"]
[137.486863, "o", "\u001b[?2004h> "]
[137.486897, "o", "import re\r\n\u001b[?2004l\r"]
[137.486921, "o", "\u001b[?2004h> "]
[137.486967, "o", "import yaml\r\n\u001b[?2004l\r"]
[137.486993, "o", "\u001b[?2004h> "]
[137.487089, "o", "from datetime import datetime\r\n\u001b[?2004l\r"]
[137.487114, "o", "\u001b[?2004h> "]
[137.487233, "o", "from collections import defaultdict\r\n\u001b[?2004l\r"]
[137.487258, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[137.487286, "o", "\u001b[?2004h> "]
[137.48739, "o", "class MLOpsProcessor:\r\n\u001b[?2004l\r"]
[137.487416, "o", "\u001b[?2004h> "]
[137.487508, "o", "    def __init__(self):\r\n\u001b[?2004l\r"]
[137.487532, "o", "\u001b[?2004h> "]
[137.487769, "o", "        self.experiments_dir = '/home/user/mlops/experiments'\r\n\u001b[?2004l\r"]
[137.487794, "o", "\u001b[?2004h> "]
[137.488025, "o", "        self.deprecated_dir = '/home/user/mlops/deprecated'\r\n\u001b[?2004l\r"]
[137.48805, "o", "\u001b[?2004h> "]
[137.488259, "o", "        self.summary_dir = '/home/user/mlops/summary'\r\n\u001b[?2004l\r"]
[137.488284, "o", "\u001b[?2004h> "]
[137.488642, "o", "        self.corruption_report = os.path.join(self.summary_dir, 'corruption_report.txt')\r\n\u001b[?2004l\r"]
[137.488667, "o", "\u001b[?2004h> "]
[137.488957, "o", "        self.inventory_file = os.path.join(self.summary_dir, 'inventory.csv')\r\n\u001b[?2004l\r"]
[137.488983, "o", "\u001b[?2004h> "]
[137.489284, "o", "        self.manifest_file = os.path.join(self.summary_dir, 'manifest.yaml')\r\n\u001b[?2004l\r"]
[137.489324, "o", "\u001b[?2004h> "]
[137.489356, "o", "        \r\n\u001b[?2004l\r"]
[137.489386, "o", "n (od\u001b[?2004h> "]
[137.489595, "o", "        # Create summary directory if it doesn't exist\r\n\u001b[?2004l\r"]
[137.489621, "o", "\u001b[?2004h> "]
[137.489819, "o", "        os.makedirs(self.summary_dir, exist_ok=True)\r\n\u001b[?2004l\r"]
[137.489845, "o", "\u001b[?2004h> "]
[137.489876, "o", "        \r\n\u001b[?2004l\r"]
[137.4899, "o", "\u001b[?2004h> "]
[137.49005, "o", "        # Initialize data structures\r\n\u001b[?2004l\r"]
[137.490076, "o", "\u001b[?2004h> "]
[137.490207, "o", "        self.corrupted_lines = []\r\n\u001b[?2004l\r"]
[137.490231, "o", "\u001b[?2004h"]
[137.490255, "o", "> "]
[137.490377, "o", "        self.inventory_data = []\r\n\u001b[?2004l\r"]
[137.490405, "o", "\u001b[?2004h> "]
[137.4906, "o", "        self.artifact_locations = defaultdict(set)\r\n\u001b[?2004l\r"]
[137.490628, "o", "\u001b[?2004h> "]
[137.490761, "o", "        self.exclusion_set = set()\r\n\u001b[?2004l\r"]
[137.490773, "o", "\u001b[?2004h"]
[137.490783, "o", "> "]
[137.490841, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h"]
[137.490855, "o", "> "]
[137.491011, "o", "    def load_deprecated_assets(self):\r\n\u001b[?2004l\r"]
[137.491066, "o", "\u001b[?2004h> "]
[137.491281, "o", "        \"\"\"Load deprecated asset filenames for exclusion\"\"\"\r\n\u001b[?2004l\r"]
[137.491295, "o", "\u001b[?2004h"]
[137.491395, "o", "> "]
[137.491501, "o", "        if os.path.exists(self.deprecated_dir):\r\n\u001b[?2004l\r"]
[137.491547, "o", "\u001b[?2004h> "]
[137.491739, "o", "            for filename in os.listdir(self.deprecated_dir):\r\n\u001b[?2004l\r"]
[137.491765, "o", "\u001b[?2004h> "]
[137.491966, "o", "                # Get root name (filename without extension)\r\n\u001b[?2004l\r"]
[137.491991, "o", "\u001b[?2004h> "]
[137.492209, "o", "                root_name = os.path.splitext(filename)[0].lower()\r\n\u001b[?2004l\r"]
[137.492242, "o", "\u001b[?2004h> "]
[137.49241, "o", "                self.exclusion_set.add(root_name)\r\n\u001b[?2004l\r"]
[137.492435, "o", "\u001b[?2004h> "]
[137.492459, "o", "    \r\n\u001b[?2004l\r"]
[137.492483, "o", "\u001b[?2004h> "]
[137.492624, "o", "    def detect_encoding(self, file_path):\r\n\u001b[?2004l\r"]
[137.492648, "o", "\u001b[?2004h> "]
[137.49277, "o", "        \"\"\"Detect file encoding\"\"\"\r\n\u001b[?2004l\r"]
[137.492794, "o", "\u001b[?2004h> "]
[137.493033, "o", "        encodings_to_try = ['utf-8', 'latin-1', 'utf-16-le']\r\n\u001b[?2004l\r"]
[137.493049, "o", "\u001b[?2004h"]
[137.49312, "o", "> "]
[137.493165, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[137.493273, "o", "        for encoding in encodings_to_try:\r\n\u001b[?2004l\r"]
[137.493294, "o", "\u001b[?2004h> "]
[137.493368, "o", "            try:\r\n\u001b[?2004l\r"]
[137.49339, "o", "\u001b[?2004h> "]
[137.493604, "o", "                with open(file_path, 'r', encoding=encoding) as f:\r\n\u001b[?2004l\r"]
[137.493623, "o", "\u001b[?2004h> "]
[137.493787, "o", "                    f.read(1)  # Test read\r\n\u001b[?2004l\r"]
[137.493807, "o", "\u001b[?2004h> "]
[137.493945, "o", "                    return encoding\r\n\u001b[?2004l\r"]
[137.493968, "o", "\u001b[?2004h> "]
[137.494101, "o", "            except UnicodeDecodeError:\r\n\u001b[?2004l\r"]
[137.494119, "o", "\u001b[?2004h> "]
[137.494203, "o", "                continue\r\n\u001b[?2004l\r"]
[137.494226, "o", "\u001b[?2004h> "]
[137.494295, "o", "        return None\r\n\u001b[?2004l\r"]
[137.494319, "o", "\u001b[?2004h> "]
[137.494344, "o", "    \r\n\u001b[?2004l\r"]
[137.494367, "o", "\u001b[?2004h> "]
[137.494493, "o", "    def is_line_corrupted(self, line):\r\n\u001b[?2004l\r"]
[137.494518, "o", "\u001b[?2004h> "]
[137.494712, "o", "        \"\"\"Check if line has odd number of trailing spaces\"\"\"\r\n\u001b[?2004l\r"]
[137.494732, "o", "\u001b[?2004h> "]
[137.49495, "o", "        # Remove newline character and check trailing spaces\r\n\u001b[?2004l\r"]
[137.49497, "o", "\u001b[?2004h> "]
[137.495161, "o", "        line_without_newline = line.rstrip('\\n')\r\n\u001b[?2004l\r"]
[137.49518, "o", "\u001b[?2004h> "]
[137.495502, "o", "        trailing_spaces = len(line_without_newline) - len(line_without_newline.rstrip())\r\n\u001b[?2004l\r"]
[137.495525, "o", "\u001b[?2004h> "]
[137.495664, "o", "        return trailing_spaces % 2 != 0\r\n\u001b[?2004l\r"]
[137.495683, "o", "\u001b[?2004h> "]
[137.495713, "o", "    \r\n\u001b[?2004l\r"]
[137.495729, "o", "\u001b[?2004h> "]
[137.495925, "o", "    def parse_artifact_line(self, line, experiment_dir):\r\n\u001b[?2004l\r"]
[137.495944, "o", "\u001b[?2004h> "]
[137.496091, "o", "        \"\"\"Parse valid artifact line\"\"\"\r\n\u001b[?2004l\r"]
[137.496114, "o", "\u001b[?2004h> "]
[137.496278, "o", "        # Remove newline and strip whitespace\r\n\u001b[?2004l\r"]
[137.496307, "o", "\u001b[?2004h> "]
[137.496464, "o", "        line = line.rstrip('\\n').strip()\r\n\u001b[?2004l\r"]
[137.496486, "o", "\u001b[?2004h> "]
[137.496616, "o", "        if not line or '|' not in line:\r\n\u001b[?2004l\r"]
[137.496637, "o", "\u001b[?2004h> "]
[137.496719, "o", "            return None\r\n\u001b[?2004l\r"]
[137.496741, "o", "\u001b[?2004h> "]
[137.496793, "o", "            \r\n\u001b[?2004l\r"]
[137.496816, "o", "\u001b[?2004h> "]
[137.49703, "o", "        parts = [part.strip() for part in line.split('|')]\r\n\u001b[?2004l\r"]
[137.497049, "o", "\u001b[?2004h> "]
[137.497152, "o", "        if len(parts) < 3:\r\n\u001b[?2004l\r"]
[137.497171, "o", "\u001b[?2004h"]
[137.49719, "o", "> "]
[137.497262, "o", "            return None\r\n\u001b[?2004l\r"]
[137.497284, "o", "\u001b[?2004h> "]
[137.497334, "o", "            \r\n\u001b[?2004l\r"]
[137.497351, "o", "\u001b[?2004h> "]
[137.497478, "o", "        artifact_name = parts[0]\r\n\u001b[?2004l\r"]
[137.497497, "o", "\u001b[?2004h"]
[137.497515, "o", "> "]
[137.497584, "o", "        status = parts[1]\r\n\u001b[?2004l\r"]
[137.497602, "o", "\u001b[?2004h> "]
[137.49779, "o", "        metadata = parts[2] if len(parts) > 2 else ''\r\n\u001b[?2004l\r"]
[137.497813, "o", "\u001b[?2004h> "]
[137.497843, "o", "        \r\n\u001b[?2004l\r"]
[137.497862, "o", "\u001b[?2004h> "]
[137.49802, "o", "        return artifact_name, status, metadata\r\n\u001b[?2004l\r"]
[137.498037, "o", "\u001b[?2004h"]
[137.498054, "o", ">     \r\n\u001b[?2004l\r"]
[137.498078, "o", "\u001b[?2004h"]
[137.498093, "o", "> "]
[137.498289, "o", "    def get_status_override(self, experiment_dir, artifact_name):\r\n\u001b[?2004l\r"]
[137.498325, "o", "\u001b[?2004h> "]
[137.498457, "o", "        \"\"\"Check for status override file\"\"\"\r\n\u001b[?2004l\r"]
[137.498476, "o", "\u001b[?2004h> "]
[137.498737, "o", "        override_file = os.path.join(experiment_dir, 'status_override.map')\r\n\u001b[?2004l\r"]
[137.498756, "o", "\u001b[?2004h> "]
[137.498893, "o", "        if os.path.exists(override_file):\r\n\u001b[?2004l\r"]
[137.49891, "o", "\u001b[?2004h> "]
[137.498973, "o", "            try:\r\n\u001b[?2004l\r"]
[137.498991, "o", "\u001b[?2004h> "]
[137.499214, "o", "                with open(override_file, 'r', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[137.499233, "o", "\u001b[?2004h> "]
[137.499352, "o", "                    for line in f:\r\n\u001b[?2004l\r"]
[137.49937, "o", "\u001b[?2004h> "]
[137.499514, "o", "                        line = line.strip()\r\n\u001b[?2004l\r"]
[137.499531, "o", "\u001b[?2004h> "]
[137.499707, "o", "                        if line and '=' in line:\r\n\u001b[?2004l\r"]
[137.499731, "o", "\u001b[?2004h> "]
[137.499919, "o", "                            key, value = line.split('=', 1)\r\n\u001b[?2004l\r"]
[137.499936, "o", "\u001b[?2004h> "]
[137.500082, "o", "                            key = key.strip()\r\n\u001b[?2004l\r"]
[137.500099, "o", "\u001b[?2004h> "]
[137.500296, "o", "                            if key == artifact_name:\r\n\u001b[?2004l\r"]
[137.500333, "o", "\u001b[?2004h> "]
[137.500484, "o", "                                return value.strip()\r\n\u001b[?2004l\r"]
[137.500504, "o", "\u001b[?2004h> "]
[137.500622, "o", "            except Exception as e:\r\n\u001b[?2004l\r"]
[137.500639, "o", "\u001b[?2004h> "]
[137.500876, "o", "                print(f\"Warning: Error reading status override file: {e}\")\r\n\u001b[?2004l\r"]
[137.500892, "o", "\u001b[?2004h> "]
[137.500961, "o", "        return None\r\n\u001b[?2004l\r"]
[137.500978, "o", "\u001b[?2004h> "]
[137.501001, "o", "    \r\n\u001b[?2004l\r"]
[137.50102, "o", "\u001b[?2004h"]
[137.501035, "o", "> "]
[137.501136, "o", "    def process_experiments(self):\r\n\u001b[?2004l\r"]
[137.501156, "o", "\u001b[?2004h> "]
[137.501286, "o", "        \"\"\"Main processing function\"\"\"\r\n\u001b[?2004l\r"]
[137.501311, "o", "\u001b[?2004h> "]
[137.501466, "o", "        print(\"Loading deprecated assets...\")\r\n\u001b[?2004l\r"]
[137.501483, "o", "\u001b[?2004h> "]
[137.50161, "o", "        self.load_deprecated_assets()\r\n\u001b[?2004l\r"]
[137.501627, "o", "\u001b[?2004h> "]
[137.501854, "o", "        print(f\"Loaded {len(self.exclusion_set)} deprecated assets\")\r\n\u001b[?2004l\r"]
[137.501874, "o", "\u001b[?2004h> "]
[137.501906, "o", "        \r\n\u001b[?2004l\r"]
[137.501931, "o", "\u001b[?2004h> "]
[137.502093, "o", "        print(\"Processing experiments...\")\r\n\u001b[?2004l\r"]
[137.502112, "o", "\u001b[?2004h> "]
[137.502145, "o", "        \r\n\u001b[?2004l\r"]
[137.502162, "o", "\u001b[?2004h> "]
[137.502278, "o", "        # Find all artifact files\r\n\u001b[?2004l\r"]
[137.502301, "o", "\u001b[?2004h"]
[137.502317, "o", "> "]
[137.502408, "o", "        artifact_files = []\r\n\u001b[?2004l\r"]
[137.502424, "o", "\u001b[?2004h> "]
[137.502666, "o", "        for root, dirs, files in os.walk(self.experiments_dir):\r\n\u001b[?2004l\r"]
[137.50269, "o", "\u001b[?2004h> "]
[137.502806, "o", "            for file in files:\r\n\u001b[?2004l\r"]
[137.502827, "o", "\u001b[?2004h> "]
[137.503052, "o", "                if file in ['artifacts.txt', 'artifacts.log']:\r\n\u001b[?2004l\r"]
[137.503071, "o", "\u001b[?2004h> "]
[137.503286, "o", "                    artifact_files.append(os.path.join(root, file))\r\n\u001b[?2004l\r"]
[137.503334, "o", "\u001b[?2004h"]
[137.503359, "o", ">         \r\n\u001b[?2004l\r\u001b[?2004h"]
[137.503384, "o", "> "]
[137.503575, "o", "        print(f\"Found {len(artifact_files)} artifact files\")\r\n\u001b[?2004l\r"]
[137.5036, "o", "\u001b[?2004h> "]
[137.503642, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h"]
[137.503668, "o", "> "]
[137.503784, "o", "        for file_path in artifact_files:\r\n\u001b[?2004l\r"]
[137.503808, "o", "\u001b[?2004h> "]
[137.503969, "o", "            print(f\"Processing: {file_path}\")\r\n\u001b[?2004l\r"]
[137.503989, "o", "\u001b[?2004h> "]
[137.504169, "o", "            experiment_dir = os.path.dirname(file_path)\r\n\u001b[?2004l\r"]
[137.504187, "o", "\u001b[?2004h> "]
[137.504233, "o", "            \r\n\u001b[?2004l\r"]
[137.504251, "o", "\u001b[?2004h> "]
[137.504394, "o", "            # Detect and convert encoding\r\n\u001b[?2004l\r"]
[137.504416, "o", "\u001b[?2004h> "]
[137.504578, "o", "            encoding = self.detect_encoding(file_path)\r\n\u001b[?2004l\r"]
[137.504598, "o", "\u001b[?2004h> "]
[137.504674, "o", "            if not encoding:\r\n\u001b[?2004l\r"]
[137.504695, "o", "\u001b[?2004h> "]
[137.504897, "o", "                print(f\"Warning: Unknown encoding for {file_path}, skipping\")\r\n\u001b[?2004l\r"]
[137.504917, "o", "\u001b[?2004h> "]
[137.504984, "o", "                continue\r\n\u001b[?2004l\r"]
[137.505002, "o", "\u001b[?2004h> "]
[137.50504, "o", "            \r\n\u001b[?2004l\r"]
[137.505058, "o", "\u001b[?2004h> "]
[137.50511, "o", "            try:\r\n\u001b[?2004l\r"]
[137.505128, "o", "\u001b[?2004h> "]
[137.505304, "o", "                with open(file_path, 'r', encoding=encoding) as f:\r\n\u001b[?2004l\r"]
[137.505325, "o", "\u001b[?2004h> "]
[137.5055, "o", "                    for line_number, line in enumerate(f, start=1):\r\n\u001b[?2004l\r"]
[137.505521, "o", "\u001b[?2004h> "]
[137.505695, "o", "                        # Check for corruption (odd trailing spaces)\r\n\u001b[?2004l\r"]
[137.505714, "o", "\u001b[?2004h> "]
[137.505859, "o", "                        if self.is_line_corrupted(line):\r\n\u001b[?2004l\r"]
[137.505879, "o", "\u001b[?2004h> "]
[137.506113, "o", "                            self.corrupted_lines.append(f\"{file_path}:{line_number}:{line}\")\r\n\u001b[?2004l\r"]
[137.506135, "o", "\u001b[?2004h> "]
[137.506229, "o", "                            continue\r\n\u001b[?2004l\r"]
[137.506249, "o", "\u001b[?2004h> "]
[137.506332, "o", "                            \r\n\u001b[?2004l\r"]
[137.506354, "o", "\u001b[?2004h> "]
[137.506463, "o", "                        # Parse valid line\r\n\u001b[?2004l\r"]
[137.506485, "o", "\u001b[?2004h> "]
[137.506685, "o", "                        parsed = self.parse_artifact_line(line, experiment_dir)\r\n\u001b[?2004l\r"]
[137.506705, "o", "\u001b[?2004h> "]
[137.506824, "o", "                        if not parsed:\r\n\u001b[?2004l\r"]
[137.506842, "o", "\u001b[?2004h> "]
[137.506956, "o", "                            continue\r\n\u001b[?2004l\r"]
[137.506975, "o", "\u001b[?2004h> "]
[137.507066, "o", "                            \r\n\u001b[?2004l\r"]
[137.507085, "o", "\u001b[?2004h> "]
[137.507283, "o", "                        artifact_name, log_status, metadata = parsed\r\n\u001b[?2004l\r"]
[137.507305, "o", "\u001b[?2004h> "]
[137.507391, "o", "                        \r\n\u001b[?2004l\r"]
[137.507404, "o", "\u001b[?2004h> "]
[137.507553, "o", "                        # Apply status override\r\n\u001b[?2004l\r"]
[137.507565, "o", "\u001b[?2004h> "]
[137.507884, "o", "                        final_status = self.get_status_override(experiment_dir, artifact_name) or log_status\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.507921, "o", "> "]
[137.507979, "o", "                        \r\n\u001b[?2004l\r"]
[137.508003, "o", "\u001b[?2004h> "]
[137.508156, "o", "                        # Filter for READY status\r\n\u001b[?2004l\r"]
[137.508176, "o", "\u001b[?2004h> "]
[137.508348, "o", "                        if final_status != 'READY':\r\n\u001b[?2004l\r"]
[137.508371, "o", "\u001b[?2004h> "]
[137.508483, "o", "                            continue\r\n\u001b[?2004l\r"]
[137.508502, "o", "\u001b[?2004h> "]
[137.5086, "o", "                            \r\n\u001b[?2004l\r"]
[137.508619, "o", "\u001b[?2004h> "]
[137.508803, "o", "                        # Check against deprecated assets\r\n\u001b[?2004l\r"]
[137.508835, "o", "\u001b[?2004h> "]
[137.509057, "o", "                        root_name = os.path.splitext(artifact_name)[0].lower()\r\n\u001b[?2004l\r"]
[137.50908, "o", "\u001b[?2004h> "]
[137.509234, "o", "                        if root_name in self.exclusion_set:\r\n\u001b[?2004l\r"]
[137.509254, "o", "\u001b[?2004h> "]
[137.509364, "o", "                            continue\r\n\u001b[?2004l\r"]
[137.509383, "o", "\u001b[?2004h> "]
[137.509458, "o", "                            \r\n\u001b[?2004l\r"]
[137.509479, "o", "\u001b[?2004h> "]
[137.509589, "o", "                        # Add to inventory\r\n\u001b[?2004l\r"]
[137.50961, "o", "\u001b[?2004h> "]
[137.509767, "o", "                        artifact_key = artifact_name.strip()\r\n\u001b[?2004l\r"]
[137.509786, "o", "\u001b[?2004h> "]
[137.509853, "o", "                        \r\n\u001b[?2004l\r"]
[137.509873, "o", "\u001b[?2004h> "]
[137.510032, "o", "                        # Calculate MD5 hash of artifact name\r\n\u001b[?2004l\r"]
[137.510053, "o", "\u001b[?2004h> "]
[137.510276, "o", "                        md5_hash = hashlib.md5(artifact_key.encode('utf-8')).hexdigest()\r\n\u001b[?2004l\r"]
[137.510291, "o", "\u001b[?2004h> "]
[137.51037, "o", "                        \r\n\u001b[?2004l\r"]
[137.510386, "o", "\u001b[?2004h> "]
[137.510599, "o", "                        # Record location (relative path from /home/user/mlops/)\r\n\u001b[?2004l\r"]
[137.510618, "o", "\u001b[?2004h> "]
[137.510896, "o", "                        relative_path = os.path.relpath(experiment_dir, os.path.dirname(self.experiments_dir))\r\n\u001b[?2004l\r"]
[137.510924, "o", "\u001b[?2004h> "]
[137.511119, "o", "                        self.artifact_locations[artifact_key].add(relative_path)\r\n\u001b[?2004l\r"]
[137.511139, "o", "\u001b[?2004h> "]
[137.511211, "o", "                        \r\n\u001b[?2004l\r"]
[137.511229, "o", "\u001b[?2004h> "]
[137.511363, "o", "                        # Add to inventory data\r\n\u001b[?2004l\r"]
[137.511383, "o", "\u001b[?2004h> "]
[137.511549, "o", "                        self.inventory_data.append(artifact_key)\r\n\u001b[?2004l\r"]
[137.511568, "o", "\u001b[?2004h> "]
[137.511632, "o", "                        \r\n\u001b[?2004l\r"]
[137.511653, "o", "\u001b[?2004h> "]
[137.511744, "o", "            except Exception as e:\r\n\u001b[?2004l\r"]
[137.511762, "o", "\u001b[?2004h> "]
[137.511918, "o", "                print(f\"Error processing {file_path}: {e}\")\r\n\u001b[?2004l\r"]
[137.511937, "o", "\u001b[?2004h> "]
[137.511965, "o", "        \r\n\u001b[?2004l\r"]
[137.511985, "o", "\u001b[?2004h> "]
[137.512085, "o", "        print(\"Processing complete\")\r\n\u001b[?2004l\r"]
[137.512108, "o", "\u001b[?2004h> "]
[137.512133, "o", "        \r\n\u001b[?2004l\r"]
[137.512155, "o", "\u001b[?2004h> "]
[137.512233, "o", "    def generate_output(self):\r\n\u001b[?2004l\r"]
[137.512256, "o", "\u001b[?2004h> "]
[137.512353, "o", "        \"\"\"Generate output files\"\"\"\r\n\u001b[?2004l\r"]
[137.512374, "o", "\u001b[?2004h> "]
[137.512485, "o", "        print(\"Generating output files...\")\r\n\u001b[?2004l\r"]
[137.512504, "o", "\u001b[?2004h> "]
[137.51253, "o", "        \r\n\u001b[?2004l\r"]
[137.51255, "o", "\u001b[?2004h> "]
[137.512652, "o", "        # Write corrupted lines report\r\n\u001b[?2004l\r"]
[137.512675, "o", "\u001b[?2004h> "]
[137.512759, "o", "        if self.corrupted_lines:\r\n\u001b[?2004l\r"]
[137.512779, "o", "\u001b[?2004h> "]
[137.512973, "o", "            with open(self.corruption_report, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[137.513021, "o", "\u001b[?2004h> "]
[137.513144, "o", "                for line in self.corrupted_lines:\r\n\u001b[?2004l\r"]
[137.513169, "o", "\u001b[?2004h> "]
[137.513263, "o", "                    f.write(line)\r\n\u001b[?2004l\r"]
[137.513275, "o", "\u001b[?2004h"]
[137.513286, "o", "> "]
[137.513332, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h"]
[137.513344, "o", "> "]
[137.513453, "o", "        # Generate inventory CSV\r\n\u001b[?2004l\r"]
[137.513465, "o", "\u001b[?2004h"]
[137.513493, "o", "> "]
[137.513583, "o", "        if self.inventory_data:\r\n\u001b[?2004l\r"]
[137.514182, "o", "\u001b[?2004h>             # Create unique artifacts with sorted locations\r\n\u001b[?2004l\r\u001b[?2004h>             inventory_entries = []\r\n\u001b[?2004l\r\u001b[?2004h>             for artifact in sorted(set(self.inventory_data)):\r\n\u001b[?2004l\r\u001b[?2004h> "]
[137.514423, "o", "                locations = ';'.join(sorted(self.artifact_locations[artifact]))\r\n\u001b[?2004l\r\u001b[?2004h> "]
[137.514966, "o", "                inventory_entries.append(f\"{artifact}|{hashlib.md5(artifact.encode('utf-8')).hexdigest()}|{locations}\")\r\n\u001b[?2004l\r\u001b[?2004h>             \r\n\u001b[?2004l\r\u001b[?2004h>             # Add header\r\n\u001b[?2004l\r"]
[137.515196, "o", "\u001b[?2004h> "]
[137.515312, "o", "            inventory_entries.insert(0, \"Artifact|NameHash|Locations\")\r\n\u001b[?2004l\r\u001b[?2004h>             \r\n\u001b[?2004l\r\u001b[?2004h> "]
[137.515536, "o", "            with open(self.inventory_file, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.515584, "o", "> "]
[137.51572, "o", "                f.write('\\n'.join(inventory_entries))\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.515756, "o", "> "]
[137.515789, "o", "        \r\n\u001b[?2004l\r\u001b[?2004h> "]
[137.515907, "o", "        # Generate manifest YAML\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.515941, "o", "> "]
[137.516016, "o", "        manifest_data = {\r\n\u001b[?2004l\r"]
[137.516049, "o", "\u001b[?2004h> "]
[137.516234, "o", "            'total_artifacts': len(set(self.inventory_data)),\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.516277, "o", "> "]
[137.51654, "o", "            'inventory_hash': hashlib.sha256(open(self.inventory_file, 'rb').read()).hexdigest(),\r\n\u001b[?2004l\r\u001b[?2004h"]
[137.516576, "o", "> "]
[137.516741, "o", "            'generated_at': datetime.utcnow().strftime('%Y-%m-%d')\r\n\u001b[?2004l\r"]
[137.516775, "o", "\u001b[?2004h> "]
[137.516809, "o", "        }\r\n\u001b[?2004l\r\u001b[?2004h> "]
[137.516843, "o", "        \r\n\u001b[?2004l\r"]
[137.516876, "o", "\u001b[?2004h> "]
[137.517068, "o", "        with open(self.manifest_file, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[137.517101, "o", "\u001b[?2004h> "]
[137.517294, "o", "            yaml.dump(manifest_data, f, default_flow_style=False)\r\n\u001b[?2004l\r"]
[137.517346, "o", "\u001b[?2004h>         \r\n\u001b[?2004l\r"]
[137.517384, "o", "\u001b[?2004h> "]
[137.517542, "o", "        print(\"Output files generated successfully\")\r\n\u001b[?2004l\r"]
[137.517576, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h"]
[137.517609, "o", "> "]
[137.517685, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r"]
[137.517719, "o", "\u001b[?2004h> "]
[137.517819, "o", "    processor = MLOpsProcessor()\r\n\u001b[?2004l\r"]
[137.517853, "o", "\u001b[?2004h> "]
[137.51796, "o", "    processor.process_experiments()\r\n\u001b[?2004l\r"]
[137.517994, "o", "\u001b[?2004h> "]
[137.518089, "o", "    processor.generate_output()\r\n\u001b[?2004l\r"]
[137.518121, "o", "\u001b[?2004h> EOF\r\n\u001b[?2004l\r"]
[137.519475, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[137.977279, "i", "chmod +x mlops_processor.py\n"]
[137.977433, "o", "chmod +x mlops_processor.py\r\n\u001b[?2004l\r"]
[137.978393, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[138.109774, "i", "python3 mlops_processor.py\n"]
[138.110024, "o", "python3 mlops_processor.py\r\n\u001b[?2004l\r"]
[138.135035, "o", "Loading deprecated assets...\r\nLoaded 1 deprecated assets\r\nProcessing experiments...\r\n"]
[138.135199, "o", "Found 4 artifact files\r\nProcessing: /home/user/mlops/experiments/exp_alpha/artifacts.txt\r\n"]
[138.135271, "o", "Processing: /home/user/mlops/experiments/exp_beta/artifacts.log\r\n"]
[138.135666, "o", "Processing: /home/user/mlops/experiments/exp_gamma/artifacts.txt\r\n"]
[138.135729, "o", "Processing: /home/user/mlops/experiments/exp_legacy/artifacts.txt\r\n"]
[138.135772, "o", "Processing complete\r\nGenerating output files...\r\n"]
[138.135899, "o", "Traceback (most recent call last):\r\n  File \"/home/user/mlops_processor.py\", line 203, in <module>\r\n"]
[138.135966, "o", "    processor.generate_output()\r\n  File \"/home/user/mlops_processor.py\", line 191, in generate_output\r\n"]
[138.136011, "o", "    'inventory_hash': hashlib.sha256(open(self.inventory_file, 'rb').read()).hexdigest(),\r\nFileNotFoundError: [Errno 2] No such file or directory: '/home/user/mlops/summary/inventory.csv'\r\n"]
[138.139568, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[154.722262, "i", "ls -la /home/user/mlops/summary/\n"]
[154.722506, "o", "ls -la /home/user/mlops/summary/\r\n\u001b[?2004l\r"]
[154.723974, "o", "total 4\r\n"]
[154.72404, "o", "drwxr-xr-x 2 root root 43 Feb 13 19:11 \u001b[0m\u001b[01;34m.\u001b[0m\r\ndrwxr-xr-x 1 user user 29 Feb 13 19:10 \u001b[01;34m..\u001b[0m\r\n-rw-r--r-- 1 root root 79 Feb 13 19:11 corruption_report.txt\r\n"]
[154.724249, "o", "\u001b[?2004h"]
[154.72429, "o", "root@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[154.82885, "i", "cat /home/user/mlops/summary/corruption_report.txt\n"]
[154.829099, "o", "cat /home/user/mlops/summary/corruption_report.txt\r\n\u001b[?2004l\r"]
[154.830323, "o", "/home/user/mlops/experiments/exp_alpha/artifacts.txt:3:data_dirty.csv | READY \r\n"]
[154.83058, "o", "\u001b[?2004hroot@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
[154.98628, "i", "echo \"Processing complete. Final verification:\" && echo \"Total artifacts: $(python3 -c 'import os; path = \\\"/home/user/mlops/summary/inventory.csv\\\"; print(0 if not os.path.exists(path) else len(open(path).readlines()) - 1)')\" && echo \"Inventory hash: $(if [ -f /home/user/mlops/summary/inventory.csv ]; then sha256sum /home/user/mlops/summary/inventory.csv | cut -d' ' -f1; else echo 'N/A'; fi)\" && echo \"Generated at: $(date -u +\\\"%Y-%m-%d\\\")\"\n"]
[154.987485, "o", "echo \"Processing complete. Final verification:\" && echo \"Total artifacts: $(python3 -c 'import os; path = \\\"/home/user/mlops/summary/inventory.csv\\\"; print(0 if not os.path.exists(path) else len(open(path).readlines()) - 1)')\" && echo \"Inventory hash: $(if [ -f /home/user/mlops/summary/inventory.csv ]; then sha256sum /home/user/mlops/summary/inventory.csv | cut -d' ' -f1; else echo 'N/A'; fi)\" && echo \"Generated at: $(date -u +\\\"%Y-%m-%d\\\")\"\r\n\u001b[?2004l\r"]
[154.987579, "o", "Processing complete. Final verification:\r\n"]
[154.994992, "o", "  File \"<string>\", line 1\r\n    import os; path = \\\"/home/user/mlops/summary/inventory.csv\\\"; print(0 if not os.path.exists(path) else len(open(path).readlines()) - 1)\r\n                       ^\r\nSyntaxError: unexpected character after line continuation character\r\n"]
[154.996352, "o", "Total artifacts: \r\n"]
[154.996828, "o", "Inventory hash: N/A\r\n"]
[154.997765, "o", "Generated at: \"2026-02-13\"\r\n"]
[154.997809, "o", "\u001b[?2004h"]
[154.997821, "o", "root@f967e1a2-efdb-47f8-a4d7-b6e959aa0025:/home/user# "]
