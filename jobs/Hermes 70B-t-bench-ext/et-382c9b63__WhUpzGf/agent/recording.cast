{"version": 2, "width": 160, "height": 40, "timestamp": 1771008695, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.005999, "o", "\u001b[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:~# "]
[0.984229, "i", "clear\r"]
[0.984374, "o", "clear\r\n\u001b[?2004l\r"]
[0.98541, "o", "\u001b[H\u001b[J"]
[0.985586, "o", "\u001b[?2004h"]
[0.985629, "o", "root@d311dcd6-b0da-41ad-83e2-23023af13f30:~# "]
[50.294772, "i", "mkdir -p /home/user/ingest /home/user/scan_logs /home/user/scan_logs/archive\n"]
[50.295044, "o", "mkdir -p /home/user/ingest /home/user/scan_logs /home/user/scan_logs/archive\r\n\u001b[?2004l\r"]
[50.300627, "o", "\u001b[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:~# "]
[50.387804, "i", "cd /home/user\n"]
[50.387937, "o", "cd /home/user\r\n\u001b[?2004l\r"]
[50.387973, "o", "\u001b[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:/home/user# "]
[50.476236, "i", "find . -type f -name \"*.log\" ! -path \"*/backup/*\" > /tmp/log_files.txt\n"]
[50.476512, "o", "find . -type f -name \"*.log\" ! -path \"*/backup/*\" > /tmp/log_files.txt\r\n\u001b[?2004l\r"]
[50.478477, "o", "\u001b[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:/home/user# "]
[51.01602, "i", "python3 - <<EOF\n#!/usr/bin/env python3\nimport re, os, json, hashlib\nimport subprocess\n\nlog_files = []\nwith open('/tmp/log_files.txt', 'r') as f:\n    log_files = [line.strip() for line in f.readlines() if line.strip() and not line.strip().startswith('.')] # Filter out empty lines and hidden files if any\n\nmanifest_data = {\"scans\": []}\nsorted_host_data = []\ncritical_gateways = []\nstandard_hosts = []\nppb_comment = \"\"\nmay_2023_epoch_start = 1680451200 # May 1, 2023 00:00:00 UTC\nmay_2023_epoch_end = 1683052799   # May 31, 2023 23:59:59 UTC\n\nfor log_file in log_files:\n    try:\n        # 1. Determine encoding and read header for date\n        header = \"\"\n        encoding = 'utf-8'\n        with open(log_file, 'rb') as f:\n            # Read first 1KB to check for BOM and get a sample for encoding\n            header_bytes = f.read(1024)\n            \n            # Check for UTF-8 BOM\n            if header_bytes.startswith(b'\\xef\\xbb\\xbf'):\n                encoding = 'utf-8-sig' # Python will strip BOM\n                head"]
[51.01614, "i", "er_bytes = header_bytes[3:]\n            elif header_bytes.startswith(b'\\xff\\xfe'): # UTF-16LE\n                # This file is UTF-16LE, but we expect ASCII/ISO-8859-1 for Nmap\n                # For date parsing, we'll read as latin1, then re-read as utf-16\n                # This indicates a problem for the header itself if it's mixed\n                # Let's try to decode first part as latin1 for the header\n                potential_latin1_header = header_bytes.decode('latin1', errors='ignore')\n                # Re-encode to bytes to pass to utf-16 decoder if needed by Nmap\n                # This is tricky, assuming header is mostly ASCII\n                # For now, let's assume if it starts with BOM, it's utf16 for whole file\n                # So, we'll read the whole file as utf-16 later\n                pass # Actual reading will be handled based on detected encoding\n            \n            # Try to decode as UTF-8 (or utf-8-sig) to get header text\n            try:\n                header = header_bytes.decode"]
[51.016167, "o", "python3 - <<EOF\r\n\u001b[?2004l\rer_bytes = header_bytes[3:]\r\n"]
[51.016201, "i", "('utf-8-sig', errors='ignore')\n            except UnicodeDecodeError:\n                try:\n                    header = header_bytes.decode('ISO-8859-1', errors='ignore')\n                    encoding = 'ISO-8859-1'\n                except UnicodeDecodeError:\n                    # If still an error, it's a very strange file, skip or mark error\n                    # For now, assume it's ISO-8859-1 if utf-8 fails\n                    header = header_bytes.decode('ISO-8859-1', errors='ignore')\n                    encoding = 'ISO-8859-1'\n        \n        date_match = re.search(r\"Starting Nmap (?:Script|Scan) [^ ]+ [^ ]+ at (\\d{4}-\\d{2}-\\d{2})\", header)\n        if not date_match:\n            continue\n        \n        scan_date_str = date_match.group(1)\n        if scan_date_str[:7] != \"2023-05\":\n            continue # Not May 2023\n\n        # 2. Calculate SHA256\n        with open(log_file, 'rb') as f:\n            file_hash = hashlib.sha256(f.read()).hexdigest()\n        manifest_data[\"scans\"].append({\"filename\": os.path"]
[51.016212, "o", "            elif header_bytes.startswith(b'\\xff\\xfe'): # UTF-16LE\r\n                # This file is UTF-16LE, but we expect ASCII/ISO-8859-1 for Nmap\r\n                # For date parsing, we'll read as latin1, then re-read as"]
[51.016249, "i", ".basename(log_file), \"sha256\": file_hash})\n\n        # 3. Parse Nmap output (ports, duration)\n        # If encoding was utf-16, read whole file in utf-16\n        if encoding == 'utf-16': # Simplified check\n             with open(log_file, 'r', encoding='utf-16') as f:\n                content = f.read()\n        else: # utf-8 or ISO-8859-1\n             with open(log_file, 'r', encoding=encoding, errors='ignore') as f:\n                content = f.read()\n\n        # Extract Host IP\n        host_match = re.search(r\"Nmap scan report for (\\S+)\", content)\n        if not host_match:\n            continue\n        host_ip = host_match.group(1)\n\n        # Extract Open Ports and Duration\n        open_ports = []\n        duration_line = re.search(r\"#METRIC (?:Nmap|Scan) .+? duration=([0-9.]+)s\", content)\n        \n        for line in content.split('\\n'):\n            port_match = re.match(r\"\\d{1,5}/open .+/(\\w+)/(\\w+)\", line)\n            if port_match:\n                port_num = int(port_match.group(1))\n                open_port"]
[51.01627, "o", " utf-16\r\n                # This indicates a problem for the header itself if it's mixed\r\n                # Let's try to decode first part as latin1 for the header\r\n                potential_latin1_header = header_bytes.decode('latin1', errors='ignore')\r\n"]
[51.016313, "i", "s.append(port_num)\n        \n        if not open_ports:\n            continue\n        \n        open_ports.sort()\n        ports_str = \",\".join(map(str, open_ports))\n        \n        if duration_line:\n            duration = float(duration_line.group(1))\n        else:\n            duration = 0.0\n\n        pps = 0.00\n        if duration > 0 and len(open_ports) > 0:\n            pps = round(len(open_ports) / duration, 2)\n        \n        # 4. Categorize for sorting\n        is_critical_gateway = 80 in open_ports and 443 in open_ports\n        if is_critical_gateway:\n            critical_gateways.append({\"hash\": file_hash[:8], \"ip\": host_ip, \"ports\": ports_str, \"duration\": duration, \"pps\": pps})\n        else:\n            standard_hosts.append({\"hash\": file_hash[:8], \"ip\": host_ip, \"ports\": ports_str, \"duration\": duration, \"pps\": pps})\n\n    except Exception as e:\n        print(f\"Error processing {log_file}: {e}\", file=sys.stderr)\n        pass\n\n# Sort hosts\nip_sort_key = lambda x: tuple(map(int, x['ip'].split('.'))) if x['i"]
[51.016325, "o", "                # Re-encode to bytes to pass to utf-16 decoder if needed by Nmap\r\n                # This is tricky, assuming header is mostly ASCII\r\n                # For now, let's assume if it starts with BOM, it's utf16 for whole file\r\n            "]
[51.016353, "i", "p'].count('.') == 3 else x['ip']\ncritical_gateways.sort(key=ip_sort_key)\nstandard_hosts.sort(key=ip_sort_key)\n\n# Combine sorted data\nall_parsed_data = critical_gateways + standard_hosts\n\n# Generate CSV content (UTF-8 for now, will be converted)\ncsv_content_lines = []\nfor item in all_parsed_data:\n    csv_content_lines.append(f\"{item['hash']},{item['ip']},\\\"{item['ports']}\\\",{item['duration']},{item['pps']}\")\n\ncsv_header = \"Hash,Host,Open_Ports,Duration,PPS\\n\"\ncsv_body = \"\\n\".join(csv_content_lines)\n\nfull_csv_utf8 = csv_header + csv_body\n\n# Calculate parser runtime (this script's execution time)\nimport time\nstart_time = time.time()\nEOF"]
[51.016376, "o", "    # So, we'll read the whole file as utf-16 later\r\n                pass # Actual reading will be handled based on detected encoding\r\n            \r\n            # Try to decode as UTF-8 (or utf-8-sig) to get header text\r\n            try:\r\n            "]
[51.016397, "o", "    header = header_bytes.decode"]
[51.016407, "o", "('utf-8-sig', errors='ignore')\r\n            ex"]
[51.016432, "o", "cept UnicodeDecodeError:\r\n                try:\r\n                    header = hea"]
[51.016462, "o", "der_bytes.decode('ISO-8859-1', errors='ignore')\r\n                    encoding = 'ISO-8859-1'\r\n                except UnicodeDecodeError:\r\n                    # If still an erro"]
[51.016484, "o", "r, it's a very strange file, skip or mark error\r\n                    # For now, assume it's ISO-8859-1 if utf-8 fails\r\n                    header "]
[51.016509, "o", "= header_bytes.decode('ISO-8859-1', errors='ignore')\r\n "]
[51.016533, "o", "                   encoding = 'ISO-8859-1'\r\n        \r\n        date_match = re.search(r\"Starting Nmap (?:Script|Scan) [^ ]+ [^ ]+ at (\\d{4}-\\d{2}-\\d{2})\", header)\r\n        if not date_"]
[51.016553, "o", "match:\r\n            continue\r\n        \r\n        scan_date_str = dat"]
[51.016575, "o", "e_match.group(1)\r\n        if scan_date_str[:7] != \"2023-05\":\r\n            c"]
[51.016592, "o", "ontinue # Not May 2023\r\n\r\n        # 2. Calculate SHA256\r\n        with open(log_file, 'rb') as f:\r\n            file"]
[51.01662, "o", "_hash = hashlib.sha256(f.read()).hexdigest()\r\n        manifest_data[\"scans\"].append({"]
[51.016638, "o", "\"filename\": os.path.basename(log_file), \"sha256\": file_hash})\r\n\r\n        # 3. Parse Nmap output (ports, duration)\r\n   "]
[51.016656, "o", "     # If encoding was utf-16, read whole file in utf-16\r\n        if encoding == 'ut"]
[51.016672, "o", "f-16': # Simplified check\r\n "]
[51.016691, "o", "            with open(log_file, 'r', encoding='utf-16') as f:\r\n                content = f.read()\r\n        "]
[51.01671, "o", "else: # utf-8 or ISO-8859-1\r\n             with open(log_file, 'r', encoding=encoding, errors='ignore') as f:\r\n                content ="]
[51.016729, "o", " f.read()\r\n"]
[51.016747, "o", "\r\n        # Extract Host IP\r\n        host_match = re.search(r\"Nmap scan report for (\\S+)\", content)\r\n        if not host_match:\r\n"]
[51.016766, "o", "            continue\r\n        host_ip = host_match.group(1)\r\n\r\n        # Extract Open Ports and Duration\r\n        open_po"]
[51.016791, "o", "rts = []\r\n        duration_line = re.search(r\"#METRIC (?:Nmap|Scan) .+? duration=([0-9.]"]
[51.016814, "o", "+)s\", content)\r\n        \r\n        for line in content.split('\\n'):\r\n            port"]
[51.016833, "o", "_match = re.match(r\"\\d{1,5}/open .+/(\\w+)/(\\w+)\", line)\r\n            if port_match:\r\n                port_num = int(port_match.group(1))"]
[51.016854, "o", "\r\n                open_ports.append(port_n"]
[51.016872, "o", "\u001b[?2004h> "]
[51.016951, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[51.016972, "o", "\u001b[?2004h"]
[51.016992, "o", "> "]
[51.017077, "o", "import re, os, json, hashlib\r\n\u001b[?2004l\r"]
[51.017097, "o", "\u001b[?2004h> "]
[51.017173, "o", "import subprocess\r\n\u001b[?2004l\r"]
[51.017199, "o", "\u001b[?2004h"]
[51.017217, "o", "> \r\n\u001b[?2004l\r"]
[51.017238, "o", "\u001b[?2004h> "]
[51.017299, "o", "log_files = []\r\n\u001b[?2004l\r"]
[51.017319, "o", "\u001b[?2004h> "]
[51.017472, "o", "with open('/tmp/log_files.txt', 'r') as f:\r\n\u001b[?2004l\r"]
[51.017488, "o", "\u001b[?2004h"]
[51.017509, "o", "> "]
[51.018022, "o", "    log_files = [line.strip() for line in f.readlines() if line.strip() and not line.strip().startswith('.')] # Filter out empty lines and hidden files if any \r"]
[51.018047, "o", "\u001bM\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[Ky\r\n\u001b[?2004l\r\u001b[?2004h"]
[51.018067, "o", "> \r\n\u001b[?2004l\r"]
[51.018084, "o", "\u001b[?2004h"]
[51.018101, "o", "> "]
[51.018203, "o", "manifest_data = {\"scans\": []}\r\n\u001b[?2004l\r"]
[51.018223, "o", "\u001b[?2004h> "]
[51.018311, "o", "sorted_host_data = []\r\n\u001b[?2004l\r"]
[51.018333, "o", "\u001b[?2004h> "]
[51.018419, "o", "critical_gateways = []\r\n\u001b[?2004l\r"]
[51.018439, "o", "\u001b[?2004h> "]
[51.018518, "o", "standard_hosts = []\r\n\u001b[?2004l\r"]
[51.018536, "o", "\u001b[?2004h"]
[51.018553, "o", "> "]
[51.018619, "o", "ppb_comment = \"\"\r\n\u001b[?2004l\r"]
[51.018671, "o", "\u001b[?2004h> "]
[51.018846, "o", "may_2023_epoch_start = 1680451200 # May 1, 2023 00:00:00 UTC\r\n\u001b[?2004l\r"]
[51.018871, "o", "\u001b[?2004h> "]
[51.019063, "o", "may_2023_epoch_end = 1683052799   # May 31, 2023 23:59:59 UTC\r\n\u001b[?2004l\r"]
[51.019085, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[51.019105, "o", "\u001b[?2004h> "]
[51.019192, "o", "for log_file in log_files:\r\n\u001b[?2004l\r"]
[51.019211, "o", "\u001b[?2004h"]
[51.019229, "o", "> "]
[51.019247, "o", "    try:\r\n\u001b[?2004l\r"]
[51.019267, "o", "\u001b[?2004h> "]
[51.019447, "o", "        # 1. Determine encoding and read header for date\r\n\u001b[?2004l\r"]
[51.019465, "o", "\u001b[?2004h> "]
[51.019528, "o", "        header = \"\"\r\n\u001b[?2004l\r"]
[51.01955, "o", "\u001b[?2004h"]
[51.019572, "o", "> "]
[51.019664, "o", "        encoding = 'utf-8'\r\n\u001b[?2004l\r"]
[51.0197, "o", "\u001b[?2004h> "]
[51.01983, "o", "        with open(log_file, 'rb') as f:\r\n\u001b[?2004l\r"]
[51.01985, "o", "\u001b[?2004h> "]
[51.020109, "o", "            # Read first 1KB to check for BOM and get a sample for encoding\r\n\u001b[?2004l\r"]
[51.020135, "o", "\u001b[?2004h> "]
[51.020277, "o", "            header_bytes = f.read(1024)\r\n\u001b[?2004l\r"]
[51.020293, "o", "\u001b[?2004h"]
[51.020314, "o", "> "]
[51.020359, "o", "            \r\n\u001b[?2004l\r"]
[51.020379, "o", "\u001b[?2004h"]
[51.020399, "o", "> "]
[51.020515, "o", "            # Check for UTF-8 BOM\r\n\u001b[?2004l\r"]
[51.02053, "o", "\u001b[?2004h"]
[51.020551, "o", "> "]
[51.020746, "o", "            if header_bytes.startswith(b'\\xef\\xbb\\xbf'):\r\n\u001b[?2004l\r"]
[51.020782, "o", "\u001b[?2004h> "]
[51.020986, "o", "                encoding = 'utf-8-sig' # Python will strip BOM\r\n\u001b[?2004l\r"]
[51.021004, "o", "\u001b[?2004h> "]
[51.021166, "o", "                header_bytes = header_bytes[3:]\r\n\u001b[?2004l\r"]
[51.021184, "o", "\u001b[?2004h"]
[51.02121, "o", "> "]
[51.021417, "o", "            elif header_bytes.startswith(b'\\xff\\xfe'): # UTF-16LE\r\n\u001b[?2004l\r"]
[51.021443, "o", "\u001b[?2004h> "]
[51.02171, "o", "                # This file is UTF-16LE, but we expect ASCII/ISO-8859-1 for Nmap\r\n\u001b[?2004l\r"]
[51.021736, "o", "\u001b[?2004h> "]
[51.022003, "o", "                # For date parsing, we'll read as latin1, then re-read as utf-16\r\n\u001b[?2004l\r"]
[51.022024, "o", "\u001b[?2004h> "]
[51.022282, "o", "                # This indicates a problem for the header itself if it's mixed\r\n\u001b[?2004l\r"]
[51.022304, "o", "\u001b[?2004h> "]
[51.02255, "o", "                # Let's try to decode first part as latin1 for the header\r\n\u001b[?2004l\r"]
[51.022569, "o", "\u001b[?2004h"]
[51.022587, "o", "> "]
[51.022875, "o", "                potential_latin1_header = header_bytes.decode('latin1', errors='ignore')\r\n\u001b[?2004l\r"]
[51.022894, "o", "\u001b[?2004h"]
[51.022912, "o", "> "]
[51.02317, "o", "                # Re-encode to bytes to pass to utf-16 decoder if needed by Nmap\r\n\u001b[?2004l\r"]
[51.023193, "o", "\u001b[?2004h> "]
[51.023412, "o", "                # This is tricky, assuming header is mostly ASCII\r\n\u001b[?2004l\r"]
[51.023427, "o", "\u001b[?2004h"]
[51.023445, "o", "> "]
[51.023704, "o", "                # For now, let's assume if it starts with BOM, it's utf16 for whole file\r\n\u001b[?2004l\r"]
[51.023726, "o", "\u001b[?2004h> "]
[51.023902, "o", "                # So, we'll read the whole file as utf-16 later\r\n\u001b[?2004l\r"]
[51.023925, "o", "\u001b[?2004h> "]
[51.024139, "o", "                pass # Actual reading will be handled based on detected encoding\r\n\u001b[?2004l\r"]
[51.024159, "o", "\u001b[?2004h"]
[51.024175, "o", "> "]
[51.024208, "o", "            \r\n\u001b[?2004l\r"]
[51.024228, "o", "\u001b[?2004h"]
[51.024244, "o", "> "]
[51.024429, "o", "            # Try to decode as UTF-8 (or utf-8-sig) to get header text\r\n\u001b[?2004l\r"]
[51.024449, "o", "\u001b[?2004h> "]
[51.024505, "o", "            try:\r\n\u001b[?2004l\r"]
[51.024524, "o", "\u001b[?2004h> "]
[51.024733, "o", "                header = header_bytes.decode('utf-8-sig', errors='ignore')\r\n\u001b[?2004l\r"]
[51.024753, "o", "\u001b[?2004h> "]
[51.02487, "o", "            except UnicodeDecodeError:\r\n\u001b[?2004l\r"]
[51.024893, "o", "\u001b[?2004h> "]
[51.024955, "o", "                try:\r\n\u001b[?2004l\r"]
[51.024972, "o", "\u001b[?2004h"]
[51.024991, "o", "> "]
[51.025194, "o", "                    header = header_bytes.decode('ISO-8859-1', errors='ignore')\r\n\u001b[?2004l\r"]
[51.025215, "o", "\u001b[?2004h> "]
[51.025339, "o", "                    encoding = 'ISO-8859-1'\r\n\u001b[?2004l\r"]
[51.025357, "o", "\u001b[?2004h> "]
[51.025479, "o", "                except UnicodeDecodeError:\r\n\u001b[?2004l\r"]
[51.025498, "o", "\u001b[?2004h"]
[51.025513, "o", "> "]
[51.025738, "o", "                    # If still an error, it's a very strange file, skip or mark error\r\n\u001b[?2004l\r"]
[51.025766, "o", "\u001b[?2004h> "]
[51.025944, "o", "                    # For now, assume it's ISO-8859-1 if utf-8 fails\r\n\u001b[?2004l\r"]
[51.025963, "o", "\u001b[?2004h"]
[51.025979, "o", "> "]
[51.02618, "o", "                    header = header_bytes.decode('ISO-8859-1', errors='ignore')\r\n\u001b[?2004l\r"]
[51.026201, "o", "\u001b[?2004h"]
[51.026219, "o", "> "]
[51.026333, "o", "                    encoding = 'ISO-8859-1'\r\n\u001b[?2004l\r"]
[51.026352, "o", "\u001b[?2004h> "]
[51.026387, "o", "        \r\n\u001b[?2004l\r"]
[51.026418, "o", "\u001b[?2004h> "]
[51.026701, "o", "        date_match = re.search(r\"Starting Nmap (?:Script|Scan) [^ ]+ [^ ]+ at (\\d{4}-\\d{2}-\\d{2})\", header)\r\n\u001b[?2004l\r"]
[51.026719, "o", "\u001b[?2004h"]
[51.026733, "o", "> "]
[51.026809, "o", "        if not date_match:\r\n\u001b[?2004l\r"]
[51.026824, "o", "\u001b[?2004h"]
[51.026842, "o", "> "]
[51.026901, "o", "            continue\r\n\u001b[?2004l\r"]
[51.026917, "o", "\u001b[?2004h"]
[51.026934, "o", "> "]
[51.026958, "o", "        \r\n\u001b[?2004l\r"]
[51.026978, "o", "\u001b[?2004h"]
[51.026995, "o", "> "]
[51.027113, "o", "        scan_date_str = date_match.group(1)\r\n\u001b[?2004l\r"]
[51.02713, "o", "\u001b[?2004h"]
[51.027147, "o", "> "]
[51.027263, "o", "        if scan_date_str[:7] != \"2023-05\":\r\n\u001b[?2004l\r"]
[51.027283, "o", "\u001b[?2004h"]
[51.027303, "o", "> "]
[51.027394, "o", "            continue # Not May 2023\r\n\u001b[?2004l\r"]
[51.02741, "o", "\u001b[?2004h"]
[51.02743, "o", "> \r\n\u001b[?2004l\r"]
[51.027448, "o", "\u001b[?2004h> "]
[51.027536, "o", "        # 2. Calculate SHA256\r\n\u001b[?2004l\r"]
[51.027554, "o", "\u001b[?2004h"]
[51.027588, "o", "> "]
[51.027688, "o", "        with open(log_file, 'rb') as f:\r\n\u001b[?2004l\r"]
[51.027704, "o", "\u001b[?2004h> "]
[51.027873, "o", "            file_hash = hashlib.sha256(f.read()).hexdigest()\r\n\u001b[?2004l\r"]
[51.027895, "o", "\u001b[?2004h"]
[51.027912, "o", "> "]
[51.028165, "o", "        manifest_data[\"scans\"].append({\"filename\": os.path.basename(log_file), \"sha256\": file_hash})\r\n\u001b[?2004l\r"]
[51.028183, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[51.028209, "o", "\u001b[?2004h"]
[51.028227, "o", "> "]
[51.028356, "o", "        # 3. Parse Nmap output (ports, duration)\r\n\u001b[?2004l\r"]
[51.028376, "o", "\u001b[?2004h> "]
[51.028537, "o", "        # If encoding was utf-16, read whole file in utf-16\r\n\u001b[?2004l\r"]
[51.028558, "o", "\u001b[?2004h"]
[51.028579, "o", "> "]
[51.028715, "o", "        if encoding == 'utf-16': # Simplified check\r\n\u001b[?2004l\r"]
[51.028737, "o", "\u001b[?2004h"]
[51.028756, "o", "> "]
[51.02891, "o", "             with open(log_file, 'r', encoding='utf-16') as f:\r\n\u001b[?2004l\r"]
[51.028929, "o", "\u001b[?2004h"]
[51.028953, "o", "> "]
[51.029039, "o", "                content = f.read()\r\n\u001b[?2004l\r"]
[51.029059, "o", "\u001b[?2004h> "]
[51.029165, "o", "        else: # utf-8 or ISO-8859-1\r\n\u001b[?2004l\r"]
[51.029185, "o", "\u001b[?2004h> "]
[51.029404, "o", "             with open(log_file, 'r', encoding=encoding, errors='ignore') as f:\r\n\u001b[?2004l\r"]
[51.029429, "o", "\u001b[?2004h> "]
[51.029525, "o", "                content = f.read()\r\n\u001b[?2004l\r"]
[51.029547, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[51.02958, "o", "\u001b[?2004h> "]
[51.029658, "o", "        # Extract Host IP\r\n\u001b[?2004l\r"]
[51.02968, "o", "\u001b[?2004h> "]
[51.029871, "o", "        host_match = re.search(r\"Nmap scan report for (\\S+)\", content)\r\n\u001b[?2004l\r"]
[51.029897, "o", "\u001b[?2004h> "]
[51.029971, "o", "        if not host_match:\r\n\u001b[?2004l\r"]
[51.029994, "o", "\u001b[?2004h> "]
[51.030057, "o", "            continue\r\n\u001b[?2004l\r"]
[51.030077, "o", "\u001b[?2004h> "]
[51.030183, "o", "        host_ip = host_match.group(1)\r\n\u001b[?2004l\r"]
[51.030203, "o", "\u001b[?2004h"]
[51.030221, "o", "> \r\n\u001b[?2004l\r"]
[51.03024, "o", "\u001b[?2004h> "]
[51.030356, "o", "        # Extract Open Ports and Duration\r\n\u001b[?2004l\r"]
[51.030373, "o", "\u001b[?2004h> "]
[51.030445, "o", "        open_ports = []\r\n\u001b[?2004l\r"]
[51.03046, "o", "\u001b[?2004h"]
[51.03048, "o", "> "]
[51.030719, "o", "        duration_line = re.search(r\"#METRIC (?:Nmap|Scan) .+? duration=([0-9.]+)s\", content)\r\n\u001b[?2004l\r"]
[51.030737, "o", "\u001b[?2004h"]
[51.030756, "o", "> "]
[51.030772, "o", "        \r\n\u001b[?2004l\r"]
[51.030792, "o", "\u001b[?2004h"]
[51.030809, "o", "> "]
[51.030913, "o", "        for line in content.split('\\n'):\r\n\u001b[?2004l\r"]
[51.030927, "o", "\u001b[?2004h"]
[51.030946, "o", "> "]
[51.031124, "o", "            port_match = re.match(r\"\\d{1,5}/open .+/(\\w+)/(\\w+)\", line)\r\n\u001b[?2004l\r"]
[51.031146, "o", "\u001b[?2004h"]
[51.031163, "o", "> "]
[51.031229, "o", "            if port_match:\r\n\u001b[?2004l\r"]
[51.031253, "o", "\u001b[?2004h> "]
[51.031389, "o", "                port_num = int(port_match.group(1))\r\n\u001b[?2004l\r"]
[51.031418, "o", "\u001b[?2004h> "]
[51.031533, "o", "                open_ports.append(port_num)\r\n\u001b[?2004l\r"]
[51.03156, "o", "\u001b[?2004h> "]
[51.031636, "o", "        \r\n\u001b[?2004l\r"]
[51.03166, "o", "\u001b[?2004h> "]
[51.031739, "o", "        if not open_ports:\r\n\u001b[?2004l\r"]
[51.031768, "o", "\u001b[?2004h> "]
[51.031827, "o", "            continue\r\n\u001b[?2004l\r"]
[51.031859, "o", "\u001b[?2004h> "]
[51.031882, "o", "        \r\n\u001b[?2004l\r"]
[51.031901, "o", "\u001b[?2004h"]
[51.031923, "o", "> "]
[51.031991, "o", "        open_ports.sort()\r\n\u001b[?2004l\r"]
[51.032012, "o", "\u001b[?2004h"]
[51.032032, "o", "> "]
[51.032154, "o", "        ports_str = \",\".join(map(str, open_ports))\r\n\u001b[?2004l\r"]
[51.03217, "o", "\u001b[?2004h> "]
[51.032207, "o", "        \r\n\u001b[?2004l\r"]
[51.032229, "o", "\u001b[?2004h> "]
[51.032304, "o", "        if duration_line:\r\n\u001b[?2004l\r"]
[51.03232, "o", "\u001b[?2004h"]
[51.032338, "o", "> "]
[51.032471, "o", "            duration = float(duration_line.group(1))\r\n\u001b[?2004l\r"]
[51.032486, "o", "\u001b[?2004h"]
[51.032504, "o", "> "]
[51.032542, "o", "        else:\r\n\u001b[?2004l\r"]
[51.032557, "o", "\u001b[?2004h"]
[51.032581, "o", "> "]
[51.03265, "o", "            duration = 0.0\r\n\u001b[?2004l\r"]
[51.032664, "o", "\u001b[?2004h"]
[51.032683, "o", "> \r\n\u001b[?2004l\r"]
[51.032701, "o", "\u001b[?2004h> "]
[51.032759, "o", "        pps = 0.00\r\n\u001b[?2004l\r"]
[51.032779, "o", "\u001b[?2004h> "]
[51.032912, "o", "        if duration > 0 and len(open_ports) > 0:\r\n\u001b[?2004l\r"]
[51.032928, "o", "\u001b[?2004h"]
[51.032945, "o", "> "]
[51.033085, "o", "            pps = round(len(open_ports) / duration, 2)\r\n\u001b[?2004l\r"]
[51.033116, "o", "\u001b[?2004h> "]
[51.033144, "o", "        \r\n\u001b[?2004l\r"]
[51.033159, "o", "\u001b[?2004h"]
[51.033177, "o", "> "]
[51.033286, "o", "        # 4. Categorize for sorting\r\n\u001b[?2004l\r"]
[51.033318, "o", "\u001b[?2004h> "]
[51.033493, "o", "        is_critical_gateway = 80 in open_ports and 443 in open_ports\r\n\u001b[?2004l\r"]
[51.033524, "o", "\u001b[?2004h> "]
[51.033612, "o", "        if is_critical_gateway:\r\n\u001b[?2004l\r"]
[51.033644, "o", "\u001b[?2004h> "]
[51.03397, "o", "            critical_gateways.append({\"hash\": file_hash[:8], \"ip\": host_ip, \"ports\": ports_str, \"duration\": duration, \"pps\": pps})\r\n\u001b[?2004l\r"]
[51.034001, "o", "\u001b[?2004h> "]
[51.034041, "o", "        else:\r\n\u001b[?2004l\r"]
[51.03406, "o", "\u001b[?2004h> "]
[51.034392, "o", "            standard_hosts.append({\"hash\": file_hash[:8], \"ip\": host_ip, \"ports\": ports_str, \"duration\": duration, \"pps\": pps})\r\n\u001b[?2004l\r"]
[51.034408, "o", "\u001b[?2004h"]
[51.034426, "o", "> \r\n\u001b[?2004l\r"]
[51.03445, "o", "\u001b[?2004h> "]
[51.03452, "o", "    except Exception as e:\r\n\u001b[?2004l\r"]
[51.034534, "o", "\u001b[?2004h"]
[51.03455, "o", "> "]
[51.034731, "o", "        print(f\"Error processing {log_file}: {e}\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[51.034749, "o", "\u001b[?2004h> "]
[51.034794, "o", "        pass\r\n\u001b[?2004l\r"]
[51.034808, "o", "\u001b[?2004h"]
[51.034825, "o", "> \r\n\u001b[?2004l\r"]
[51.034844, "o", "\u001b[?2004h> "]
[51.03489, "o", "# Sort hosts\r\n\u001b[?2004l\r"]
[51.034911, "o", "\u001b[?2004h> "]
[51.035168, "o", "ip_sort_key = lambda x: tuple(map(int, x['ip'].split('.'))) if x['ip'].count('.') == 3 else x['ip']\r\n\u001b[?2004l\r"]
[51.035188, "o", "\u001b[?2004h> "]
[51.0353, "o", "critical_gateways.sort(key=ip_sort_key)\r\n\u001b[?2004l\r"]
[51.035318, "o", "\u001b[?2004h"]
[51.03534, "o", "> "]
[51.035435, "o", "standard_hosts.sort(key=ip_sort_key)\r\n\u001b[?2004l\r"]
[51.035455, "o", "\u001b[?2004h> "]
[51.035476, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[51.035485, "o", "> "]
[51.035555, "o", "# Combine sorted data\r\n\u001b[?2004l\r"]
[51.035586, "o", "\u001b[?2004h> "]
[51.035734, "o", "all_parsed_data = critical_gateways + standard_hosts\r\n\u001b[?2004l\r"]
[51.035756, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[51.035777, "o", "\u001b[?2004h"]
[51.0358, "o", "> "]
[51.035944, "o", "# Generate CSV content (UTF-8 for now, will be converted)\r\n\u001b[?2004l\r"]
[51.035963, "o", "\u001b[?2004h> "]
[51.036032, "o", "csv_content_lines = []\r\n\u001b[?2004l\r"]
[51.036048, "o", "\u001b[?2004h"]
[51.036071, "o", "> "]
[51.03614, "o", "for item in all_parsed_data:\r\n\u001b[?2004l\r"]
[51.036156, "o", "\u001b[?2004h"]
[51.036174, "o", "> "]
[51.036456, "o", "    csv_content_lines.append(f\"{item['hash']},{item['ip']},\\\"{item['ports']}\\\",{item['duration']},{item['pps']}\")\r\n\u001b[?2004l\r"]
[51.036473, "o", "\u001b[?2004h"]
[51.036492, "o", "> \r\n\u001b[?2004l\r"]
[51.036511, "o", "\u001b[?2004h> "]
[51.036659, "o", "csv_header = \"Hash,Host,Open_Ports,Duration,PPS\\n\"\r\n\u001b[?2004l\r"]
[51.036685, "o", "\u001b[?2004h> "]
[51.036792, "o", "csv_body = \"\\n\".join(csv_content_lines)\r\n\u001b[?2004l\r"]
[51.036816, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[51.036834, "o", "\u001b[?2004h"]
[51.036862, "o", "> "]
[51.036955, "o", "full_csv_utf8 = csv_header + csv_body\r\n\u001b[?2004l\r"]
[51.036988, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[51.037013, "o", "\u001b[?2004h> "]
[51.037166, "o", "# Calculate parser runtime (this script's execution time)\r\n\u001b[?2004l\r"]
[51.037195, "o", "\u001b[?2004h> "]
[51.03723, "o", "import time\r\n\u001b[?2004l\r"]
[51.037253, "o", "\u001b[?2004h"]
[51.037273, "o", "> "]
[51.037333, "o", "start_time = time.time()\r\n\u001b[?2004l\r"]
[51.037356, "o", "\u001b[?2004h> "]
[51.037373, "o", "EOF"]
[66.033632, "i", "python3 - <<EOF\nimport subprocess\nimport sys\nimport os\n\n# Previous part of the script should have populated these variables:\n# manifest_data (already written to JSON)\n# full_csv_utf8\n# start_time (for performance comment)\n# For this to work, the Python script needs to be self-contained or variables passed.\n# Assuming this is a continuation, let's refactor the Python code to be in one block for simplicity.\n# The previous block should have created /tmp/manifest.json and /tmp/csv_content.txt\n# Let's assume the first block did:\n# json.dump(manifest_data, open('/tmp/manifest.json', 'w'), indent=2)\n# open('/tmp/csv_content.txt', 'w').write(full_csv_utf8)\n\n# (Simulating that the previous block produced these files)\n# Create dummy manifest.json and csv_content.txt for this part to work\n# This is only for the structure; actual data comes from the main script\n\n# 1. Write Manifest (already done by main script)\n# subprocess.run(['cat', '/tmp/manifest.json'], check=True) # This would display it\n\n# 2. Prepare and write CSV"]
[66.033756, "i", " in UTF-16LE with BOM\n# Read the UTF-8 CSV content\ntry:\n    with open('/tmp/csv_content.txt', 'r', encoding='utf-8') as f:\n        utf8_csv_content = f.read()\nexcept FileNotFoundError:\n    print(\"CSV content not found at /tmp/csv_content.txt. Ensure the first part of the script ran successfully.\", file=sys.stderr)\n    sys.exit(1)\n\n# Encode to UTF-16LE with BOM\ncsv_utf16le = utf8_csv_content.encode('utf-16le')\n# Add BOM (which is already implicitly added by encode('utf-16le') in Python, but we can be explicit)\n# bom = b'\\xfe\\xff'\n# final_csv_content = bom + csv_utf16le # Python's encode('utf-16le') adds this\n\n# Calculate parser runtime\nimport time\nend_time = time.time()\nparser_runtime_seconds = round(end_time - start_time, 3) # Millisecond precision\n\n# Append performance comment (encoded in UTF-16LE)\nperformance_comment = f\"#parser_runtime={parser_runtime_seconds}\\n\"\nperformance_comment_utf16le = performance_comment.encode('utf-16le')\n\n# Combine and write final CSV\nfinal_output_path = \"/home/user/ingest/cyber_"]
[66.033782, "o", "python3 - <<EOF\r\n\u001b[?2004l\r"]
[66.033808, "i", "core_import.csv\"\noutput_dir = os.path.dirname(final_output_path)\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nwith open(final_output_path, 'wb') as f:\n    f.write(csv_utf16le)\n    f.write(performance_comment_utf16le)\n\nprint(\"Manifest and CSV files generated. Displaying manifest:\")\nprint(open('/tmp/manifest.json').read())\nEOF"]
[66.033839, "o", "core_import.csv\"\r\noutput_dir = os.path.dirname(final_output_pat"]
[66.033851, "o", "h)\r\nif not os.path.exists(output_dir):\r\n    os.makedirs(output_dir)\r\n\r\nwith open(final_ou"]
[66.033897, "o", "tput_path, 'wb') as f:\r\n    f.write(csv_utf16le)\r\n    f.write(performance_comment_utf16le)\r\n\r\nprint(\"Manifest and CSV files generated. Displaying manifest:\")\r\nprint(open('/tmp/manifest.json').read())\r\nEOF"]
[66.033964, "o", "\u001b[?2004h> "]
[66.034025, "o", "import subprocess\r\n"]
[66.03405, "o", "\u001b[?2004l\r"]
[66.034082, "o", "\u001b[?2004h> "]
[66.034134, "o", "import sys\r\n\u001b[?2004l\r"]
[66.034162, "o", "\u001b[?2004h"]
[66.034195, "o", "> "]
[66.034234, "o", "import os\r\n\u001b[?2004l\r"]
[66.034273, "o", "\u001b[?2004h> "]
[66.034283, "o", "\r\n\u001b[?2004l\r"]
[66.034333, "o", "\u001b[?2004h> "]
[66.034674, "o", "# Previous part of the script should have populated these variables:\r\n\u001b[?2004l\r"]
[66.034715, "o", "\u001b[?2004h"]
[66.034725, "o", "> "]
[66.034932, "o", "# manifest_data (already written to JSON)\r\n\u001b[?2004l\r"]
[66.034961, "o", "\u001b[?2004h"]
[66.034976, "o", "> "]
[66.035066, "o", "# full_csv_utf8\r\n\u001b[?2004l\r"]
[66.035096, "o", "\u001b[?2004h"]
[66.03512, "o", "> "]
[66.035326, "o", "# start_time (for performance comment)\r\n"]
[66.035344, "o", "\u001b[?2004l\r"]
[66.035376, "o", "\u001b[?2004h"]
[66.035393, "o", "> "]
[66.03588, "o", "# For this to work, the Python script needs to be self-contained or variables passed.\r\n"]
[66.035901, "o", "\u001b[?2004l\r"]
[66.035917, "o", "\u001b[?2004h"]
[66.035936, "o", "> "]
[66.036453, "o", "# Assuming this is a continuation, let's refactor the Python code to be in one block for simplicity.\r\n\u001b[?2004l\r"]
[66.036557, "o", "\u001b[?2004h> "]
[66.036936, "o", "# The previous block should have created /tmp/manifest.json and /tmp/csv_content.txt\r\n\u001b[?2004l\r"]
[66.036988, "o", "\u001b[?2004h> "]
[66.037146, "o", "# Let's assume the first block did:\r\n\u001b[?2004l\r"]
[66.037185, "o", "\u001b[?2004h> "]
[66.037492, "o", "# json.dump(manifest_data, open('/tmp/manifest.json', 'w'), indent=2)\r\n\u001b[?2004l\r"]
[66.037531, "o", "\u001b[?2004h> "]
[66.037791, "o", "# open('/tmp/csv_content.txt', 'w').write(full_csv_utf8)\r\n\u001b[?2004l\r"]
[66.037843, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[66.037881, "o", "\u001b[?2004h> "]
[66.038143, "o", "# (Simulating that the previous block produced these files)\r\n\u001b[?2004l\r"]
[66.038187, "o", "\u001b[?2004h> "]
[66.038491, "o", "# Create dummy manifest.json and csv_content.txt for this part to work\r\n\u001b[?2004l\r"]
[66.038528, "o", "\u001b[?2004h> "]
[66.038856, "o", "# This is only for the structure; actual data comes from the main script\r\n\u001b[?2004l\r"]
[66.038895, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[66.038931, "o", "\u001b[?2004h> "]
[66.039162, "o", "# 1. Write Manifest (already done by main script)\r\n\u001b[?2004l\r"]
[66.039199, "o", "\u001b[?2004h> "]
[66.03958, "o", "# subprocess.run(['cat', '/tmp/manifest.json'], check=True) # This would display it\r\n\u001b[?2004l\r"]
[66.039622, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[66.03966, "o", "\u001b[?2004h> "]
[66.039881, "o", "# 2. Prepare and write CSV in UTF-16LE with BOM\r\n\u001b[?2004l\r"]
[66.039914, "o", "\u001b[?2004h> "]
[66.040045, "o", "# Read the UTF-8 CSV content\r\n\u001b[?2004l\r"]
[66.040083, "o", "\u001b[?2004h> "]
[66.040112, "o", "try:\r\n\u001b[?2004l\r"]
[66.040148, "o", "\u001b[?2004h> "]
[66.040445, "o", "    with open('/tmp/csv_content.txt', 'r', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[66.040482, "o", "\u001b[?2004h> "]
[66.040654, "o", "        utf8_csv_content = f.read()\r\n\u001b[?2004l\r"]
[66.040688, "o", "\u001b[?2004h"]
[66.04072, "o", "> "]
[66.040826, "o", "except FileNotFoundError:\r\n\u001b[?2004l\r"]
[66.040864, "o", "\u001b[?2004h> "]
[66.041434, "o", "    print(\"CSV content not found at /tmp/csv_content.txt. Ensure the first part of the script ran successfully.\", file=sys.stderr)\r\n\u001b[?2004l\r"]
[66.041472, "o", "\u001b[?2004h> "]
[66.041551, "o", "    sys.exit(1)\r\n\u001b[?2004l\r"]
[66.041595, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[66.041625, "o", "\u001b[?2004h"]
[66.041657, "o", "> "]
[66.041783, "o", "# Encode to UTF-16LE with BOM\r\n\u001b[?2004l\r"]
[66.04182, "o", "\u001b[?2004h> "]
[66.042044, "o", "csv_utf16le = utf8_csv_content.encode('utf-16le')\r\n\u001b[?2004l\r"]
[66.04208, "o", "\u001b[?2004h> "]
[66.042526, "o", "# Add BOM (which is already implicitly added by encode('utf-16le') in Python, but we can be explicit)\r\n\u001b[?2004l\r"]
[66.042563, "o", "\u001b[?2004h> "]
[66.042661, "o", "# bom = b'\\xfe\\xff'\r\n\u001b[?2004l\r"]
[66.042698, "o", "\u001b[?2004h> "]
[66.043048, "o", "# final_csv_content = bom + csv_utf16le # Python's encode('utf-16le') adds this\r\n\u001b[?2004l\r"]
[66.043086, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[66.043122, "o", "\u001b[?2004h> "]
[66.043255, "o", "# Calculate parser runtime\r\n\u001b[?2004l\r"]
[66.043292, "o", "\u001b[?2004h> "]
[66.043353, "o", "import time\r\n\u001b[?2004l\r"]
[66.04339, "o", "\u001b[?2004h> "]
[66.043501, "o", "end_time = time.time()\r\n\u001b[?2004l\r"]
[66.043534, "o", "\u001b[?2004h> "]
[66.043899, "o", "parser_runtime_seconds = round(end_time - start_time, 3) # Millisecond precision\r\n\u001b[?2004l\r"]
[66.043939, "o", "\u001b[?2004h> \r\n"]
[66.043971, "o", "\u001b[?2004l\r\u001b[?2004h"]
[66.044003, "o", "> "]
[66.044226, "o", "# Append performance comment (encoded in UTF-16LE)\r\n\u001b[?2004l\r"]
[66.044265, "o", "\u001b[?2004h> "]
[66.044575, "o", "performance_comment = f\"#parser_runtime={parser_runtime_seconds}\\n\"\r\n\u001b[?2004l\r"]
[66.044612, "o", "\u001b[?2004h> "]
[66.044915, "o", "performance_comment_utf16le = performance_comment.encode('utf-16le')\r\n\u001b[?2004l\r"]
[66.044953, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[66.04498, "o", "\u001b[?2004h"]
[66.045012, "o", "> "]
[66.045136, "o", "# Combine and write final CSV\r\n\u001b[?2004l\r"]
[66.045174, "o", "\u001b[?2004h> "]
[66.045515, "o", "final_output_path = \"/home/user/ingest/cyber_core_import.csv\"\r\n\u001b[?2004l\r"]
[66.045552, "o", "\u001b[?2004h> "]
[66.045785, "o", "output_dir = os.path.dirname(final_output_path)\r\n\u001b[?2004l\r"]
[66.045821, "o", "\u001b[?2004h> "]
[66.045979, "o", "if not os.path.exists(output_dir):\r\n\u001b[?2004l\r"]
[66.046016, "o", "\u001b[?2004h> "]
[66.046144, "o", "    os.makedirs(output_dir)\r\n\u001b[?2004l\r"]
[66.046181, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[66.046208, "o", "\u001b[?2004h"]
[66.046241, "o", "> "]
[66.046414, "o", "with open(final_output_path, 'wb') as f:\r\n\u001b[?2004l\r"]
[66.046451, "o", "\u001b[?2004h> "]
[66.046576, "o", "    f.write(csv_utf16le)\r\n\u001b[?2004l\r"]
[66.046603, "o", "\u001b[?2004h> "]
[66.046802, "o", "    f.write(performance_comment_utf16le)\r\n\u001b[?2004l\r"]
[66.04684, "o", "\u001b[?2004h> "]
[66.046868, "o", "\r\n\u001b[?2004l\r\u001b[?2004h"]
[66.046901, "o", "> "]
[66.047188, "o", "print(\"Manifest and CSV files generated. Displaying manifest:\")\r\n\u001b[?2004l\r"]
[66.047218, "o", "\u001b[?2004h> "]
[66.047415, "o", "print(open('/tmp/manifest.json').read())\r\n\u001b[?2004l\r"]
[66.047444, "o", "\u001b[?2004h> "]
[66.047471, "o", "EOF"]
