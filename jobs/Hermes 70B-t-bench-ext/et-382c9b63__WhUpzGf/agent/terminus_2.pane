[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:~# asciinema rec --stdin /logs/agent/recording.cast
[?2004l[0;32masciinema: recording asciicast to /logs/agent/recording.cast[0m
[0;32masciinema: press <ctrl-d> or type "exit" when you're done[0m
[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:~# clear
[?2004l[H[J[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:~# mkdir -p /home/user/ingest /home/user/scan_logs /home/user/scan_logs/archive
[?2004l[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:~# cd /home/user
[?2004l[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:/home/user# find . -type f -name "*.log" ! -path "*/backup/*" > /tmp/log_files.txt
[?2004l[?2004hroot@d311dcd6-b0da-41ad-83e2-23023af13f30:/home/user# python3 - <<EOF
[?2004ler_bytes = header_bytes[3:]
            elif header_bytes.startswith(b'\xff\xfe'): # UTF-16LE
                # This file is UTF-16LE, but we expect ASCII/ISO-8859-1 for Nmap
                # For date parsing, we'll read as latin1, then re-read as utf-16
                # This indicates a problem for the header itself if it's mixed
                # Let's try to decode first part as latin1 for the header
                potential_latin1_header = header_bytes.decode('latin1', errors='ignore')
                # Re-encode to bytes to pass to utf-16 decoder if needed by Nmap
                # This is tricky, assuming header is mostly ASCII
                # For now, let's assume if it starts with BOM, it's utf16 for whole file
                # So, we'll read the whole file as utf-16 later
                pass # Actual reading will be handled based on detected encoding
            
            # Try to decode as UTF-8 (or utf-8-sig) to get header text
            try:
                header = header_bytes.decode('utf-8-sig', errors='ignore')
            except UnicodeDecodeError:
                try:
                    header = header_bytes.decode('ISO-8859-1', errors='ignore')
                    encoding = 'ISO-8859-1'
                except UnicodeDecodeError:
                    # If still an error, it's a very strange file, skip or mark error
                    # For now, assume it's ISO-8859-1 if utf-8 fails
                    header = header_bytes.decode('ISO-8859-1', errors='ignore')
                    encoding = 'ISO-8859-1'
        
        date_match = re.search(r"Starting Nmap (?:Script|Scan) [^ ]+ [^ ]+ at (\d{4}-\d{2}-\d{2})", header)
        if not date_match:
            continue
        
        scan_date_str = date_match.group(1)
        if scan_date_str[:7] != "2023-05":
            continue # Not May 2023

        # 2. Calculate SHA256
        with open(log_file, 'rb') as f:
            file_hash = hashlib.sha256(f.read()).hexdigest()
        manifest_data["scans"].append({"filename": os.path.basename(log_file), "sha256": file_hash})

        # 3. Parse Nmap output (ports, duration)
        # If encoding was utf-16, read whole file in utf-16
        if encoding == 'utf-16': # Simplified check
             with open(log_file, 'r', encoding='utf-16') as f:
                content = f.read()
        else: # utf-8 or ISO-8859-1
             with open(log_file, 'r', encoding=encoding, errors='ignore') as f:
                content = f.read()

        # Extract Host IP
        host_match = re.search(r"Nmap scan report for (\S+)", content)
        if not host_match:
            continue
        host_ip = host_match.group(1)

        # Extract Open Ports and Duration
        open_ports = []
        duration_line = re.search(r"#METRIC (?:Nmap|Scan) .+? duration=([0-9.]+)s", content)
        
        for line in content.split('\n'):
            port_match = re.match(r"\d{1,5}/open .+/(\w+)/(\w+)", line)
            if port_match:
                port_num = int(port_match.group(1))
                open_ports.append(port_n[?2004h> #!/usr/bin/env python3
[?2004l[?2004h> import re, os, json, hashlib
[?2004l[?2004h> import subprocess
[?2004l[?2004h> 
[?2004l[?2004h> log_files = []
[?2004l[?2004h> with open('/tmp/log_files.txt', 'r') as f:
[?2004l[?2004h>     log_files = [line.strip() for line in f.readlines() if line.strip() and not line.strip().startswith('.')] # Filter out empty lines and hidden files if any M[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Ky
[?2004l[?2004h> 
[?2004l[?2004h> manifest_data = {"scans": []}
[?2004l[?2004h> sorted_host_data = []
[?2004l[?2004h> critical_gateways = []
[?2004l[?2004h> standard_hosts = []
[?2004l[?2004h> ppb_comment = ""
[?2004l[?2004h> may_2023_epoch_start = 1680451200 # May 1, 2023 00:00:00 UTC
[?2004l[?2004h> may_2023_epoch_end = 1683052799   # May 31, 2023 23:59:59 UTC
[?2004l[?2004h> 
[?2004l[?2004h> for log_file in log_files:
[?2004l[?2004h>     try:
[?2004l[?2004h>         # 1. Determine encoding and read header for date
[?2004l[?2004h>         header = ""
[?2004l[?2004h>         encoding = 'utf-8'
[?2004l[?2004h>         with open(log_file, 'rb') as f:
[?2004l[?2004h>             # Read first 1KB to check for BOM and get a sample for encoding
[?2004l[?2004h>             header_bytes = f.read(1024)
[?2004l[?2004h>             
[?2004l[?2004h>             # Check for UTF-8 BOM
[?2004l[?2004h>             if header_bytes.startswith(b'\xef\xbb\xbf'):
[?2004l[?2004h>                 encoding = 'utf-8-sig' # Python will strip BOM
[?2004l[?2004h>                 header_bytes = header_bytes[3:]
[?2004l[?2004h>             elif header_bytes.startswith(b'\xff\xfe'): # UTF-16LE
[?2004l[?2004h>                 # This file is UTF-16LE, but we expect ASCII/ISO-8859-1 for Nmap
[?2004l[?2004h>                 # For date parsing, we'll read as latin1, then re-read as utf-16
[?2004l[?2004h>                 # This indicates a problem for the header itself if it's mixed
[?2004l[?2004h>                 # Let's try to decode first part as latin1 for the header
[?2004l[?2004h>                 potential_latin1_header = header_bytes.decode('latin1', errors='ignore')
[?2004l[?2004h>                 # Re-encode to bytes to pass to utf-16 decoder if needed by Nmap
[?2004l[?2004h>                 # This is tricky, assuming header is mostly ASCII
[?2004l[?2004h>                 # For now, let's assume if it starts with BOM, it's utf16 for whole file
[?2004l[?2004h>                 # So, we'll read the whole file as utf-16 later
[?2004l[?2004h>                 pass # Actual reading will be handled based on detected encoding
[?2004l[?2004h>             
[?2004l[?2004h>             # Try to decode as UTF-8 (or utf-8-sig) to get header text
[?2004l[?2004h>             try:
[?2004l[?2004h>                 header = header_bytes.decode('utf-8-sig', errors='ignore')
[?2004l[?2004h>             except UnicodeDecodeError:
[?2004l[?2004h>                 try:
[?2004l[?2004h>                     header = header_bytes.decode('ISO-8859-1', errors='ignore')
[?2004l[?2004h>                     encoding = 'ISO-8859-1'
[?2004l[?2004h>                 except UnicodeDecodeError:
[?2004l[?2004h>                     # If still an error, it's a very strange file, skip or mark error
[?2004l[?2004h>                     # For now, assume it's ISO-8859-1 if utf-8 fails
[?2004l[?2004h>                     header = header_bytes.decode('ISO-8859-1', errors='ignore')
[?2004l[?2004h>                     encoding = 'ISO-8859-1'
[?2004l[?2004h>         
[?2004l[?2004h>         date_match = re.search(r"Starting Nmap (?:Script|Scan) [^ ]+ [^ ]+ at (\d{4}-\d{2}-\d{2})", header)
[?2004l[?2004h>         if not date_match:
[?2004l[?2004h>             continue
[?2004l[?2004h>         
[?2004l[?2004h>         scan_date_str = date_match.group(1)
[?2004l[?2004h>         if scan_date_str[:7] != "2023-05":
[?2004l[?2004h>             continue # Not May 2023
[?2004l[?2004h> 
[?2004l[?2004h>         # 2. Calculate SHA256
[?2004l[?2004h>         with open(log_file, 'rb') as f:
[?2004l[?2004h>             file_hash = hashlib.sha256(f.read()).hexdigest()
[?2004l[?2004h>         manifest_data["scans"].append({"filename": os.path.basename(log_file), "sha256": file_hash})
[?2004l[?2004h> 
[?2004l[?2004h>         # 3. Parse Nmap output (ports, duration)
[?2004l[?2004h>         # If encoding was utf-16, read whole file in utf-16
[?2004l[?2004h>         if encoding == 'utf-16': # Simplified check
[?2004l[?2004h>              with open(log_file, 'r', encoding='utf-16') as f:
[?2004l[?2004h>                 content = f.read()
[?2004l[?2004h>         else: # utf-8 or ISO-8859-1
[?2004l[?2004h>              with open(log_file, 'r', encoding=encoding, errors='ignore') as f:
[?2004l[?2004h>                 content = f.read()
[?2004l[?2004h> 
[?2004l[?2004h>         # Extract Host IP
[?2004l[?2004h>         host_match = re.search(r"Nmap scan report for (\S+)", content)
[?2004l[?2004h>         if not host_match:
[?2004l[?2004h>             continue
[?2004l[?2004h>         host_ip = host_match.group(1)
[?2004l[?2004h> 
[?2004l[?2004h>         # Extract Open Ports and Duration
[?2004l[?2004h>         open_ports = []
[?2004l[?2004h>         duration_line = re.search(r"#METRIC (?:Nmap|Scan) .+? duration=([0-9.]+)s", content)
[?2004l[?2004h>         
[?2004l[?2004h>         for line in content.split('\n'):
[?2004l[?2004h>             port_match = re.match(r"\d{1,5}/open .+/(\w+)/(\w+)", line)
[?2004l[?2004h>             if port_match:
[?2004l[?2004h>                 port_num = int(port_match.group(1))
[?2004l[?2004h>                 open_ports.append(port_num)
[?2004l[?2004h>         
[?2004l[?2004h>         if not open_ports:
[?2004l[?2004h>             continue
[?2004l[?2004h>         
[?2004l[?2004h>         open_ports.sort()
[?2004l[?2004h>         ports_str = ",".join(map(str, open_ports))
[?2004l[?2004h>         
[?2004l[?2004h>         if duration_line:
[?2004l[?2004h>             duration = float(duration_line.group(1))
[?2004l[?2004h>         else:
[?2004l[?2004h>             duration = 0.0
[?2004l[?2004h> 
[?2004l[?2004h>         pps = 0.00
[?2004l[?2004h>         if duration > 0 and len(open_ports) > 0:
[?2004l[?2004h>             pps = round(len(open_ports) / duration, 2)
[?2004l[?2004h>         
[?2004l[?2004h>         # 4. Categorize for sorting
[?2004l[?2004h>         is_critical_gateway = 80 in open_ports and 443 in open_ports
[?2004l[?2004h>         if is_critical_gateway:
[?2004l[?2004h>             critical_gateways.append({"hash": file_hash[:8], "ip": host_ip, "ports": ports_str, "duration": duration, "pps": pps})
[?2004l[?2004h>         else:
[?2004l[?2004h>             standard_hosts.append({"hash": file_hash[:8], "ip": host_ip, "ports": ports_str, "duration": duration, "pps": pps})
[?2004l[?2004h> 
[?2004l[?2004h>     except Exception as e:
[?2004l[?2004h>         print(f"Error processing {log_file}: {e}", file=sys.stderr)
[?2004l[?2004h>         pass
[?2004l[?2004h> 
[?2004l[?2004h> # Sort hosts
[?2004l[?2004h> ip_sort_key = lambda x: tuple(map(int, x['ip'].split('.'))) if x['ip'].count('.') == 3 else x['ip']
[?2004l[?2004h> critical_gateways.sort(key=ip_sort_key)
[?2004l[?2004h> standard_hosts.sort(key=ip_sort_key)
[?2004l[?2004h> 
[?2004l[?2004h> # Combine sorted data
[?2004l[?2004h> all_parsed_data = critical_gateways + standard_hosts
[?2004l[?2004h> 
[?2004l[?2004h> # Generate CSV content (UTF-8 for now, will be converted)
[?2004l[?2004h> csv_content_lines = []
[?2004l[?2004h> for item in all_parsed_data:
[?2004l[?2004h>     csv_content_lines.append(f"{item['hash']},{item['ip']},\"{item['ports']}\",{item['duration']},{item['pps']}")
[?2004l[?2004h> 
[?2004l[?2004h> csv_header = "Hash,Host,Open_Ports,Duration,PPS\n"
[?2004l[?2004h> csv_body = "\n".join(csv_content_lines)
[?2004l[?2004h> 
[?2004l[?2004h> full_csv_utf8 = csv_header + csv_body
[?2004l[?2004h> 
[?2004l[?2004h> # Calculate parser runtime (this script's execution time)
[?2004l[?2004h> import time
[?2004l[?2004h> start_time = time.time()
[?2004l[?2004h> EOFpython3 - <<EOF
[?2004lcore_import.csv"
output_dir = os.path.dirname(final_output_path)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

with open(final_output_path, 'wb') as f:
    f.write(csv_utf16le)
    f.write(performance_comment_utf16le)

print("Manifest and CSV files generated. Displaying manifest:")
print(open('/tmp/manifest.json').read())
EOF[?2004h> import subprocess
[?2004l[?2004h> import sys
[?2004l[?2004h> import os
[?2004l[?2004h> 
[?2004l[?2004h> # Previous part of the script should have populated these variables:
[?2004l[?2004h> # manifest_data (already written to JSON)
[?2004l[?2004h> # full_csv_utf8
[?2004l[?2004h> # start_time (for performance comment)
[?2004l[?2004h> # For this to work, the Python script needs to be self-contained or variables passed.
[?2004l[?2004h> # Assuming this is a continuation, let's refactor the Python code to be in one block for simplicity.
[?2004l[?2004h> # The previous block should have created /tmp/manifest.json and /tmp/csv_content.txt
[?2004l[?2004h> # Let's assume the first block did:
[?2004l[?2004h> # json.dump(manifest_data, open('/tmp/manifest.json', 'w'), indent=2)
[?2004l[?2004h> # open('/tmp/csv_content.txt', 'w').write(full_csv_utf8)
[?2004l[?2004h> 
[?2004l[?2004h> # (Simulating that the previous block produced these files)
[?2004l[?2004h> # Create dummy manifest.json and csv_content.txt for this part to work
[?2004l[?2004h> # This is only for the structure; actual data comes from the main script
[?2004l[?2004h> 
[?2004l[?2004h> # 1. Write Manifest (already done by main script)
[?2004l[?2004h> # subprocess.run(['cat', '/tmp/manifest.json'], check=True) # This would display it
[?2004l[?2004h> 
[?2004l[?2004h> # 2. Prepare and write CSV in UTF-16LE with BOM
[?2004l[?2004h> # Read the UTF-8 CSV content
[?2004l[?2004h> try:
[?2004l[?2004h>     with open('/tmp/csv_content.txt', 'r', encoding='utf-8') as f:
[?2004l[?2004h>         utf8_csv_content = f.read()
[?2004l[?2004h> except FileNotFoundError:
[?2004l[?2004h>     print("CSV content not found at /tmp/csv_content.txt. Ensure the first part of the script ran successfully.", file=sys.stderr)
[?2004l[?2004h>     sys.exit(1)
[?2004l[?2004h> 
[?2004l[?2004h> # Encode to UTF-16LE with BOM
[?2004l[?2004h> csv_utf16le = utf8_csv_content.encode('utf-16le')
[?2004l[?2004h> # Add BOM (which is already implicitly added by encode('utf-16le') in Python, but we can be explicit)
[?2004l[?2004h> # bom = b'\xfe\xff'
[?2004l[?2004h> # final_csv_content = bom + csv_utf16le # Python's encode('utf-16le') adds this
[?2004l[?2004h> 
[?2004l[?2004h> # Calculate parser runtime
[?2004l[?2004h> import time
[?2004l[?2004h> end_time = time.time()
[?2004l[?2004h> parser_runtime_seconds = round(end_time - start_time, 3) # Millisecond precision
[?2004l[?2004h> 
[?2004l[?2004h> # Append performance comment (encoded in UTF-16LE)
[?2004l[?2004h> performance_comment = f"#parser_runtime={parser_runtime_seconds}\n"
[?2004l[?2004h> performance_comment_utf16le = performance_comment.encode('utf-16le')
[?2004l[?2004h> 
[?2004l[?2004h> # Combine and write final CSV
[?2004l[?2004h> final_output_path = "/home/user/ingest/cyber_core_import.csv"
[?2004l[?2004h> output_dir = os.path.dirname(final_output_path)
[?2004l[?2004h> if not os.path.exists(output_dir):
[?2004l[?2004h>     os.makedirs(output_dir)
[?2004l[?2004h> 
[?2004l[?2004h> with open(final_output_path, 'wb') as f:
[?2004l[?2004h>     f.write(csv_utf16le)
[?2004l[?2004h>     f.write(performance_comment_utf16le)
[?2004l[?2004h> 
[?2004l[?2004h> print("Manifest and CSV files generated. Displaying manifest:")
[?2004l[?2004h> print(open('/tmp/manifest.json').read())
[?2004l[?2004h> EOF