{"version": 2, "width": 160, "height": 40, "timestamp": 1771009194, "env": {"SHELL": "/bin/bash", "TERM": "screen"}}
[0.007598, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[0.934316, "i", "clear\r"]
[0.934506, "o", "clear\r\n\u001b[?2004l\r"]
[0.935568, "o", "\u001b[H\u001b[J"]
[0.935776, "o", "\u001b[?2004h"]
[0.935802, "o", "root@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[75.522855, "i", "pwd\n"]
[75.523001, "o", "pwd\r\n\u001b[?2004l\r/home/user/cloud\r\n\u001b[?2004h"]
[75.523049, "o", "root@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[75.637551, "i", "ls -la\n"]
[75.637689, "o", "ls -la\r\n\u001b[?2004l\r"]
[75.639689, "o", "total 8\r\n"]
[75.639743, "o", "drwxrwxrwx 1 root root  90 Feb 13 00:00 \u001b[0m\u001b[34;42m.\u001b[0m\r\ndrwxr-xr-x 1 root root  27 Feb 13 00:00 \u001b[01;34m..\u001b[0m\r\n-rwxrwxrwx 1 root root  15 Feb 13 00:00 \u001b[01;32m.config\u001b[0m\r\n-rwxrwxrwx 1 root root 107 Feb 13 00:00 \u001b[01;32minventory.csv\u001b[0m\r\ndrwxrwxrwx 1 root root   6 Feb 13 00:00 \u001b[34;42mlegacy\u001b[0m\r\ndrwxrwxrwx 1 root root   6 Feb 13 00:00 \u001b[34;42moutput\u001b[0m\r\n"]
[75.639973, "o", "\u001b[?2004h"]
[75.640002, "o", "root@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[75.756405, "i", "mkdir -p /home/user/cloud/output\n"]
[75.756676, "o", "mkdir -p /home/user/cloud/output\r\n\u001b[?2004l\r"]
[75.758047, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[75.854886, "i", "echo \"Source inventory file: $LEGACY_INVENTORY\"\n"]
[75.855067, "o", "echo \"Source inventory file: $LEGACY_INVENTORY\"\r\n\u001b[?2004l\r"]
[75.855109, "o", "Source inventory file: /home/user/cloud/legacy/mainframe_export.csv\r\n"]
[75.855139, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[75.979605, "i", "cat /home/user/cloud/.config\n"]
[75.97976, "o", "cat /home/user/cloud/.config\r\n\u001b[?2004l\r"]
[75.980661, "o", ".internal.corp\r\n"]
[75.980839, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[76.118902, "i", "cat > /tmp/migration_script.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport sys\nimport csv\nimport json\nimport io\nimport hashlib\nimport re\nfrom pathlib import Path\n\n# --- Configuration ---\nSOURCE_INVENTORY = os.environ.get('LEGACY_INVENTORY')\nCONFIG_FILE = '/home/user/cloud/.config'\nOUTPUT_DIR = '/home/user/cloud/output'\nDOMAIN_SUFFIX = None\nMIGRATION_PLAN_FILE = os.path.join(OUTPUT_DIR, 'migration_plan.tsv')\nMANIFEST_FILE = os.path.join(OUTPUT_DIR, 'manifest.json')\nAUDIT_LOG_FILE = os.path.join(OUTPUT_DIR, 'audit.log')\n\nclass CRLFWriter:\n    \"\"\"A CSV writer that forces CRLF line endings and encodes to ISO-8859-1.\"\"\"\n    def __init__(self, f):\n        self.writer = csv.writer(f, dialect='excel', lineterminator='\\r\\n')\n\n    def writerow(self, row):\n        self.writer.writerow(row)\n\n    def writerows(self, rows):\n        for r in rows:\n            self.writerow(r)\n\ndef remove_zwsp(text):\n    \"\"\"Remove Zero Width Spaces (U+200B).\"\"\"\n    if text:\n        return text.replace('\\u200b', '')\n    return text\n\ndef "]
[76.11902, "i", "main():\n    if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Read domain suffix\n    try:\n        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:\n            domain_suffix_content = f.read().strip()\n            if domain_suffix_content:\n                DOMAIN_SUFFIX = domain_suffix_content\n                print(f\"Domain suffix read: {DOMAIN_SUFFIX}\")\n            else:\n                print(f\"Warning: Domain suffix file {CONFIG_FILE} is empty or not readable. Using empty suffix.\")\n                DOMAIN_SUFFIX = ''\n    except FileNotFoundError:\n        print(f\"Error: Config file {CONFIG_FILE} not found. Using empty suffix.\")\n        DOMAIN_SUFFIX = ''\n\n    if not SOURCE_INVENTORY:\n        print(\"Error: LEGACY_INVENTORY environment variable not set or is empty.\")\n        sys.exit(1)\n    if not os.path.exists(SOURCE_INVENTORY):\n        print(f\"Error: Source inventory file {SOURCE_INVENTORY} not found.\")\n        sys.exit(1)\n\n    print(f\"Processing source inventory: {SOUR"]
[76.119049, "i", "CE_INVENTORY}\")\n    \n    processed_rows = []\n    # Header for the TSV output\n    output_header = [\"ServiceName\", \"TargetFQDN\"]\n\n    try:\n        with open(SOURCE_INVENTORY, 'rb') as f_in:\n            # UTF-16LE with BOM, decode to Python strings (Unicode)\n            decoded_lines = []\n            for line_bytes in f_in:\n                # Decode bytes to Unicode string, stripping CRLF if present\n                line = line_bytes.decode('utf-16le').strip('\r\n')\n                decoded_lines.append(line)\n            \n            # CSV reader for the decoded lines (now strings)\n            # Assumes standard CSV structure after decoding\n            # Pandas is not standard, using pure Python CSV module\n            reader = csv.reader(decoded_lines, delimiter=',')\n            \n            for row_idx, row_data in enumerate(reader):\n                if not row_data: # Skip empty rows\n                    continue\n                \n                # Check for comment lines (lines starting with # after decoding)\n       "]
[76.11907, "i", "         if row_data and row_data[0].startswith('#'):\n                    continue\n                \n                # Filter: Status should not be 'Decom' or 'Pending'\n                # Filter: NewServer should not be empty or only whitespace\n                # Assume column names: 'ServiceName', 'Status', 'NewServer' exist in the CSV\n                \n                try:\n                    service_name = row_data[0] \n                    status = row_data[1] \n                    new_server_val = row_data[2]\n                except IndexError:\n                    print(f\"Warning: Skipping malformed row {row_idx+1} (not enough columns). Expected at least 3 columns.\")\n                    continue\n                    \n                if status in ['Decom', 'Pending']:\n                    continue\n                \n                if not new_server_val or new_server_val.isspace():\n                    continue\n                \n                # Transformations:\n                # ServiceName: Convert to all lowercase\n"]
[76.119089, "o", "cat > /tmp/migration_script.py << 'EOF'\r\n"]
[76.119114, "i", "                processed_service_name = service_name.lower()\n                # TargetFQDN: Clean NewServer, append domain suffix\n                target_fqdn = remove_zwsp(new_server_val) + DOMAIN_SUFFIX\n                \n                processed_rows.append([processed_service_name, target_fqdn])\n        \n        # Sort the output by ServiceName (alphabetically)\n        processed_rows.sort(key=lambda x: x[0])\n\n        # Write migration_plan.tsv (ISO-8859-1, CRLF)\n        # ISO-8859-1 is a subset of UTF-16LE for common Western characters.\n        # If data only contains these, encoding should be fine.\n        # If data contains characters not in ISO-8859-1, they will be replaced (e.g., by '?')\n        try:\n            with open(MIGRATION_PLAN_FILE, 'w', encoding='iso-8859-1', errors='replace') as f_out:\n                tsv_writer = CRLFWriter(f_out) \n                tsv_writer.writerow(output_header)\n                tsv_writer.writerows(processed_rows)\n            print(f\"Generated migration plan: {MIGRATION_P"]
[76.119133, "o", "\u001b[?2004l\r                processed_service_name "]
[76.119155, "i", "LAN_FILE}\")\n        except UnicodeEncodeError as e:\n            print(f\"Error: Could not encode data to ISO-8859-1: {e}. Please check source data for non-Latin-1 characters.\")\n            sys.exit(1)\n        except Exception as e:\n            print(f\"Error writing {MIGRATION_PLAN_FILE}: {e}\")\n            sys.exit(1)\n        \n        # Generate manifest.json\n        # Record count = number of data rows in TSV, excluding header\n        record_count = len(processed_rows)\n        \n        # MD5 hash of the final migration_plan.tsv\n        md5_hash = \"\"\n        try:\n            with open(MIGRATION_PLAN_FILE, 'rb') as f:\n                md5_hash = hashlib.md5(f.read()).hexdigest()\n        except FileNotFoundError:\n            print(f\"Error: {MIGRATION_PLAN_FILE} not found for MD5 calculation.\")\n            sys.exit(1)\n        except Exception as e:\n            print(f\"Error calculating MD5 for {MIGRATION_PLAN_FILE}: {e}\")\n            sys.exit(1)\n            \n        manifest_data = {\n            \"source_file\": SOUR"]
[76.119172, "o", "\u001b[?2004h> "]
[76.119195, "i", "CE_INVENTORY,\n            \"record_count\": record_count,\n            \"output_md5\": md5_hash\n        }\n        try:\n            with open(MANIFEST_FILE, 'w', encoding='utf-8') as f:\n                json.dump(manifest_data, f, indent=2)\n            print(f\"Generated manifest: {MANIFEST_FILE}\")\n        except Exception as e:\n            print(f\"Error writing {MANIFEST_FILE}: {e}\")\n            sys.exit(1)\n            \n        # Generate audit.log\n        # Size in bytes of the final migration_plan.tsv\n        file_size_bytes = 0\n        if os.path.exists(MIGRATION_PLAN_FILE):\n            file_size_bytes = os.path.getsize(MIGRATION_PLAN_FILE)\n        \n        audit_log_content = f\"{file_size_bytes} bytes generated\"\n        try:\n            with open(AUDIT_LOG_FILE, 'w', encoding='utf-8') as f:\n                f.write(audit_log_content + \"\\n\") \n            print(f\"Generated audit log: {AUDIT_LOG_FILE}\")\n        except Exception as e:\n            print(f\"Error writing {AUDIT_LOG_FILE}: {e}\")\n            sys.exit(1)\n "]
[76.119221, "i", "           \n        print(\"All tasks completed successfully.\")\n        \n    except FileNotFoundError:\n        print(f\"Error: Input file {SOURCE_INVENTORY} not found. Check $LEGACY_INVENTORY variable.\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        sys.exit(1)\n\nif __name__ == '__main__':\n    main()\nEOF\n"]
[76.119261, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[76.119287, "o", "\u001b[?2004h> "]
[76.119333, "o", "import os\r\n\u001b[?2004l\r"]
[76.119355, "o", "\u001b[?2004h> "]
[76.119401, "o", "import sys\r\n\u001b[?2004l\r"]
[76.119422, "o", "\u001b[?2004h"]
[76.119455, "o", "> "]
[76.119487, "o", "import csv\r\n\u001b[?2004l\r"]
[76.119506, "o", "\u001b[?2004h> "]
[76.119551, "o", "import json\r\n\u001b[?2004l\r"]
[76.119572, "o", "\u001b[?2004h"]
[76.119593, "o", "> "]
[76.11962, "o", "import io\r\n\u001b[?2004l\r"]
[76.11964, "o", "\u001b[?2004h"]
[76.119659, "o", "> "]
[76.119711, "o", "import hashlib\r\n\u001b[?2004l\r"]
[76.119833, "o", "\u001b[?2004h> import re\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.119903, "o", "from pathlib import Path\r\n\u001b[?2004l\r"]
[76.119931, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[76.119964, "o", "\u001b[?2004h"]
[76.119989, "o", "> "]
[76.120086, "o", "# --- Configuration ---\r\n\u001b[?2004l\r"]
[76.120125, "o", "\u001b[?2004h> "]
[76.120309, "o", "SOURCE_INVENTORY = os.environ.get('LEGACY_INVENTORY')\r\n\u001b[?2004l\r"]
[76.120324, "o", "\u001b[?2004h> "]
[76.120474, "o", "CONFIG_FILE = '/home/user/cloud/.config'\r\n\u001b[?2004l\r"]
[76.120497, "o", "\u001b[?2004h> "]
[76.120621, "o", "OUTPUT_DIR = '/home/user/cloud/output'\r\n\u001b[?2004l\r"]
[76.120634, "o", "\u001b[?2004h> "]
[76.120705, "o", "DOMAIN_SUFFIX = None\r\n\u001b[?2004l\r"]
[76.120731, "o", "\u001b[?2004h"]
[76.120758, "o", "> "]
[76.120947, "o", "MIGRATION_PLAN_FILE = os.path.join(OUTPUT_DIR, 'migration_plan.tsv')\r\n\u001b[?2004l\r"]
[76.120963, "o", "\u001b[?2004h> "]
[76.121156, "o", "MANIFEST_FILE = os.path.join(OUTPUT_DIR, 'manifest.json')\r\n\u001b[?2004l\r"]
[76.121201, "o", "\u001b[?2004h"]
[76.121227, "o", "> "]
[76.121376, "o", "AUDIT_LOG_FILE = os.path.join(OUTPUT_DIR, 'audit.log')\r\n\u001b[?2004l\r"]
[76.121412, "o", "\u001b[?2004h"]
[76.121433, "o", "> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.121515, "o", "class CRLFWriter:\r\n\u001b[?2004l\r"]
[76.12156, "o", "\u001b[?2004h"]
[76.121597, "o", "> "]
[76.121805, "o", "    \"\"\"A CSV writer that forces CRLF line endings and encodes to ISO-8859-1.\"\"\"\r\n\u001b[?2004l\r"]
[76.121819, "o", "\u001b[?2004h> "]
[76.121925, "o", "    def __init__(self, f):\r\n\u001b[?2004l\r"]
[76.12197, "o", "\u001b[?2004h> "]
[76.122253, "o", "        self.writer = csv.writer(f, dialect='excel', lineterminator='\\r\\n')\r\n\u001b[?2004l\r"]
[76.122294, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.122329, "o", "> "]
[76.122417, "o", "    def writerow(self, row):\r\n\u001b[?2004l\r"]
[76.122429, "o", "\u001b[?2004h"]
[76.122501, "o", "> "]
[76.122586, "o", "        self.writer.writerow(row)\r\n\u001b[?2004l\r"]
[76.122633, "o", "\u001b[?2004h> \r\n\u001b[?2004l\rr\u001b[?2004h"]
[76.122673, "o", "> "]
[76.122757, "o", "    def writerows(self, rows):\r\n\u001b[?2004l\r"]
[76.122793, "o", "\u001b[?2004h"]
[76.122808, "o", "> "]
[76.122872, "o", "        for r in rows:\r\n\u001b[?2004l\r"]
[76.122913, "o", "\u001b[?2004h> "]
[76.123003, "o", "            self.writerow(r)\r\n\u001b[?2004l\r"]
[76.123014, "o", "\u001b[?2004h"]
[76.123065, "o", "> \r\n\u001b[?2004l\rt\u001b[?2004h> "]
[76.123133, "o", "def remove_zwsp(text):\r\n\u001b[?2004l\r"]
[76.123166, "o", "\u001b[?2004h> "]
[76.123348, "o", "    \"\"\"Remove Zero Width Spaces (U+200B).\"\"\"\r\n\u001b[?2004l\r"]
[76.123362, "o", "\u001b[?2004h> "]
[76.123421, "o", "    if text:\r\n\u001b[?2004l\r"]
[76.123481, "o", "\u001b[?2004h> "]
[76.123607, "o", "        return text.replace('\\u200b', '')\r\n\u001b[?2004l\r"]
[76.123647, "o", "\u001b[?2004h"]
[76.123685, "o", ">     return text\r\n"]
[76.12372, "o", "\u001b[?2004l\r"]
[76.123754, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.123788, "o", "> "]
[76.123821, "o", "def main():\r\n\u001b[?2004l\r"]
[76.123854, "o", "\u001b[?2004h> "]
[76.123968, "o", "    if not os.path.exists(OUTPUT_DIR):\r\n\u001b[?2004l\r"]
[76.124002, "o", "\u001b[?2004h> "]
[76.12418, "o", "        os.makedirs(OUTPUT_DIR, exist_ok=True)\r\n\u001b[?2004l\r"]
[76.124217, "o", "\u001b[?2004h> \r\n"]
[76.124253, "o", "\u001b[?2004l\r\u001b[?2004h> "]
[76.124316, "o", "    # Read domain suffix\r\n\u001b[?2004l\r"]
[76.12435, "o", "\u001b[?2004h> "]
[76.124379, "o", "    try:\r\n\u001b[?2004l\r"]
[76.124411, "o", "\u001b[?2004h"]
[76.124445, "o", "> "]
[76.124609, "o", "        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[76.124644, "o", "\u001b[?2004h> "]
[76.124829, "o", "            domain_suffix_content = f.read().strip()\r\n\u001b[?2004l\r"]
[76.124864, "o", "\u001b[?2004h> "]
[76.125, "o", "            if domain_suffix_content:\r\n\u001b[?2004l\r"]
[76.125035, "o", "\u001b[?2004h> "]
[76.125233, "o", "                DOMAIN_SUFFIX = domain_suffix_content\r\n\u001b[?2004l\r"]
[76.125268, "o", "\u001b[?2004h"]
[76.125289, "o", "> "]
[76.125508, "o", "                print(f\"Domain suffix read: {DOMAIN_SUFFIX}\")\r\n\u001b[?2004l\r"]
[76.125545, "o", "\u001b[?2004h> "]
[76.125593, "o", "            else:\r\n\u001b[?2004l\r"]
[76.125626, "o", "\u001b[?2004h"]
[76.125659, "o", "> "]
[76.126062, "o", "                print(f\"Warning: Domain suffix file {CONFIG_FILE} is empty or not readable. Using empty suffix.\")\r\n\u001b[?2004l\r"]
[76.1261, "o", "\u001b[?2004h> "]
[76.126219, "o", "                DOMAIN_SUFFIX = ''\r\n\u001b[?2004l\r"]
[76.126251, "o", "\u001b[?2004h"]
[76.126286, "o", "> "]
[76.126362, "o", "    except FileNotFoundError:\r\n\u001b[?2004l\r"]
[76.126394, "o", "\u001b[?2004h"]
[76.126428, "o", "> "]
[76.126717, "o", "        print(f\"Error: Config file {CONFIG_FILE} not found. Using empty suffix.\")\r\n\u001b[?2004l\r"]
[76.126759, "o", "\u001b[?2004h> "]
[76.126843, "o", "        DOMAIN_SUFFIX = ''\r\n\u001b[?2004l\r"]
[76.126865, "o", "\u001b[?2004h> "]
[76.126888, "o", "\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.127012, "o", "    if not SOURCE_INVENTORY:\r\n\u001b[?2004l\r"]
[76.127038, "o", "\u001b[?2004h> "]
[76.12732, "o", "        print(\"Error: LEGACY_INVENTORY environment variable not set or is empty.\")\r\n\u001b[?2004l\r"]
[76.127343, "o", "\u001b[?2004h> "]
[76.127424, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[76.127474, "o", "\u001b[?2004h> "]
[76.127653, "o", "    if not os.path.exists(SOURCE_INVENTORY):\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.127697, "o", "> "]
[76.127964, "o", "        print(f\"Error: Source inventory file {SOURCE_INVENTORY} not found.\")\r\n\u001b[?2004l\r"]
[76.128013, "o", "\u001b[?2004h> "]
[76.128063, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[76.128129, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.128356, "o", "    print(f\"Processing source inventory: {SOURCE_INVENTORY}\")\r\n\u001b[?2004l\r"]
[76.128401, "o", "\u001b[?2004h>     \r\n\u001b[?2004l\r"]
[76.128456, "o", "\u001b[?2004h> "]
[76.128543, "o", "    processed_rows = []\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.12859, "o", "> "]
[76.128685, "o", "    # Header for the TSV output\r\n\u001b[?2004l\r"]
[76.12873, "o", "\u001b[?2004h> "]
[76.128899, "o", "    output_header = [\"ServiceName\", \"TargetFQDN\"]\r\n\u001b[?2004l\r"]
[76.128942, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.129022, "o", ">     try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.129209, "o", "        with open(SOURCE_INVENTORY, 'rb') as f_in:\r\n\u001b[?2004l\r"]
[76.129225, "o", "\u001b[?2004h"]
[76.129272, "o", "> "]
[76.129502, "o", "            # UTF-16LE with BOM, decode to Python strings (Unicode)\r\n\u001b[?2004l\r"]
[76.129515, "o", "\u001b[?2004h"]
[76.129554, "o", "> "]
[76.129685, "o", "            decoded_lines = []\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.129856, "o", "            for line_bytes in f_in:\r\n\u001b[?2004l\r"]
[76.12987, "o", "\u001b[?2004h"]
[76.129912, "o", "> "]
[76.130167, "o", "                # Decode bytes to Unicode string, stripping CRLF if present\r\n\u001b[?2004l\r"]
[76.130208, "o", "\u001b[?2004h"]
[76.130248, "o", "> "]
[76.130424, "o", "                line = line_bytes.decode('utf-16le').strip('\r\n\u001b[?2004l\r"]
[76.130482, "o", "\u001b[?2004h"]
[76.130514, "o", "> \r\n\u001b[?2004l\re\u001b[?2004h> ')\r\n\u001b[?2004l\r"]
[76.13056, "o", "\u001b[?2004h> "]
[76.130686, "o", "                decoded_lines.append(line)\r\n\u001b[?2004l\r"]
[76.130723, "o", "\u001b[?2004h"]
[76.130762, "o", "> "]
[76.130804, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.130842, "o", "> "]
[76.131012, "o", "            # CSV reader for the decoded lines (now strings)\r\n\u001b[?2004l\r"]
[76.13105, "o", "\u001b[?2004h> "]
[76.131269, "o", "            # Assumes standard CSV structure after decoding\r\n\u001b[?2004l\r"]
[76.131304, "o", "\u001b[?2004h> "]
[76.131588, "o", "            # Pandas is not standard, using pure Python CSV module\r\n\u001b[?2004l\r"]
[76.13163, "o", "\u001b[?2004h"]
[76.131673, "o", "> "]
[76.131858, "o", "            reader = csv.reader(decoded_lines, delimiter=',')\r\n\u001b[?2004l\r"]
[76.131896, "o", "\u001b[?2004h"]
[76.13191, "o", "> "]
[76.131952, "o", "            \r\n\u001b[?2004l\rn\u001b[?2004h"]
[76.131981, "o", "> "]
[76.132184, "o", "            for row_idx, row_data in enumerate(reader):\r\n\u001b[?2004l\r"]
[76.132229, "o", "\u001b[?2004h> "]
[76.132415, "o", "                if not row_data: # Skip empty rows\r\n\u001b[?2004l\r"]
[76.132462, "o", "\u001b[?2004h"]
[76.132498, "o", "> "]
[76.132557, "o", "                    continue\r\n\u001b[?2004l\r"]
[76.132592, "o", "\u001b[?2004h> "]
[76.132655, "o", "                \r\n\u001b[?2004l\r"]
[76.132699, "o", "\u001b[?2004h"]
[76.132713, "o", "> "]
[76.132983, "o", "                # Check for comment lines (lines starting with # after decoding)\r\n\u001b[?2004l\r"]
[76.133029, "o", "\u001b[?2004h"]
[76.133048, "o", "> "]
[76.133238, "o", "                if row_data and row_data[0].startswith('#'):\r\n\u001b[?2004l\r"]
[76.133276, "o", "\u001b[?2004h"]
[76.133289, "o", "> "]
[76.133372, "o", "                    continue\r\n\u001b[?2004l\r"]
[76.133407, "o", "\u001b[?2004h> "]
[76.133474, "o", "                \r\n\u001b[?2004l\r"]
[76.133503, "o", "\u001b[?2004h> "]
[76.133782, "o", "                # Filter: Status should not be 'Decom' or 'Pending'\r\n\u001b[?2004l\r"]
[76.133806, "o", "\u001b[?2004h> "]
[76.134089, "o", "                # Filter: NewServer should not be empty or only whitespace\r\n\u001b[?2004l\r"]
[76.134131, "o", "\u001b[?2004h"]
[76.134176, "o", "> "]
[76.134487, "o", "                # Assume column names: 'ServiceName', 'Status', 'NewServer' exist in the CSV\r\n\u001b[?2004l\r"]
[76.134528, "o", "\u001b[?2004h"]
[76.134543, "o", "> "]
[76.13458, "o", "                \r\n\u001b[?2004l\r"]
[76.134612, "o", "\u001b[?2004h"]
[76.134628, "o", "> "]
[76.134694, "o", "                try:\r\n\u001b[?2004l\r"]
[76.134722, "o", "\u001b[?2004h"]
[76.134755, "o", "> "]
[76.134869, "o", "                    service_name = row_data[0] \r\n\u001b[?2004l\r"]
[76.134899, "o", "\u001b[?2004h"]
[76.13493, "o", "> "]
[76.135026, "o", "                    status = row_data[1] \r\n\u001b[?2004l\r"]
[76.135055, "o", "\u001b[?2004h"]
[76.135087, "o", "> "]
[76.135206, "o", "                    new_server_val = row_data[2]\r\n\u001b[?2004l\r"]
[76.135246, "o", "\u001b[?2004h"]
[76.135274, "o", "> "]
[76.135345, "o", "                except IndexError:\r\n\u001b[?2004l\r"]
[76.135374, "o", "\u001b[?2004h> "]
[76.135772, "o", "                    print(f\"Warning: Skipping malformed row {row_idx+1} (not enough columns). Expected at least 3 columns.\")\r\n\u001b[?2004l\r"]
[76.135807, "o", "\u001b[?2004h"]
[76.135838, "o", "> "]
[76.135893, "o", "                    continue\r\n\u001b[?2004l\r"]
[76.135908, "o", "\u001b[?2004h> "]
[76.135997, "o", "                    \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.136065, "o", "> "]
[76.136167, "o", "                if status in ['Decom', 'Pending']:\r\n\u001b[?2004l\r"]
[76.136214, "o", "\u001b[?2004h> "]
[76.136296, "o", "                    continue\r\n\u001b[?2004l\r"]
[76.136335, "o", "\u001b[?2004h> "]
[76.136377, "o", "                \r\n\u001b[?2004l\r"]
[76.136415, "o", "\u001b[?2004h> "]
[76.136619, "o", "                if not new_server_val or new_server_val.isspace():\r\n\u001b[?2004l\r"]
[76.136659, "o", "\u001b[?2004h> "]
[76.136739, "o", "                    continue\r\n\u001b[?2004l\r"]
[76.136779, "o", "\u001b[?2004h> "]
[76.136825, "o", "                \r\n\u001b[?2004l\r"]
[76.136854, "o", "\u001b[?2004h> "]
[76.13697, "o", "                # Transformations:\r\n\u001b[?2004l\r"]
[76.137003, "o", "\u001b[?2004h> "]
[76.13718, "o", "                # ServiceName: Convert to all lowercase\r\n\u001b[?2004l\r"]
[76.137216, "o", "\u001b[?2004h> "]
[76.137424, "o", "                processed_service_name = service_name.lower()\r\n\u001b[?2004l\r"]
[76.137471, "o", "\u001b[?2004h> "]
[76.137693, "o", "                # TargetFQDN: Clean NewServer, append domain suffix\r\n\u001b[?2004l\r"]
[76.137731, "o", "\u001b[?2004h> "]
[76.137975, "o", "                target_fqdn = remove_zwsp(new_server_val) + DOMAIN_SUFFIX\r\n\u001b[?2004l\r"]
[76.138009, "o", "\u001b[?2004h> "]
[76.138061, "o", "                \r\n\u001b[?2004l\r"]
[76.138104, "o", "\u001b[?2004h> "]
[76.138348, "o", "                processed_rows.append([processed_service_name, target_fqdn])\r\n\u001b[?2004l\r"]
[76.138358, "o", "\u001b[?2004h"]
[76.138405, "o", ">         \r\n\u001b[?2004l\r"]
[76.138463, "o", "\u001b[?2004h> "]
[76.138638, "o", "        # Sort the output by ServiceName (alphabetically)\r\n\u001b[?2004l\r"]
[76.138681, "o", "\u001b[?2004h> "]
[76.138829, "o", "        processed_rows.sort(key=lambda x: x[0])\r\n\u001b[?2004l\r"]
[76.138873, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.138882, "o", "> "]
[76.13907, "o", "        # Write migration_plan.tsv (ISO-8859-1, CRLF)\r\n\u001b[?2004l\r"]
[76.139112, "o", "\u001b[?2004h> "]
[76.13935, "o", "        # ISO-8859-1 is a subset of UTF-16LE for common Western characters.\r\n\u001b[?2004l\r"]
[76.139388, "o", "\u001b[?2004h> "]
[76.139559, "o", "        # If data only contains these, encoding should be fine.\r\n\u001b[?2004l\r"]
[76.139569, "o", "\u001b[?2004h"]
[76.139616, "o", "> "]
[76.139817, "o", "        # If data contains characters not in ISO-8859-1, they will be replaced (e.g., by '?')\r\n\u001b[?2004l\r"]
[76.139828, "o", "\u001b[?2004h"]
[76.139875, "o", ">         try:\r\n\u001b[?2004l\r"]
[76.13997, "o", "\u001b[?2004h> "]
[76.140143, "o", "            with open(MIGRATION_PLAN_FILE, 'w', encoding='iso-8859-1', errors='replace') as f_out:\r\n\u001b[?2004l\r"]
[76.140166, "o", "\u001b[?2004h> "]
[76.140293, "o", "                tsv_writer = CRLFWriter(f_out) \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.140315, "o", "> "]
[76.140451, "o", "                tsv_writer.writerow(output_header)\r\n\u001b[?2004l\r"]
[76.14047, "o", "\u001b[?2004h"]
[76.140491, "o", "> "]
[76.140608, "o", "                tsv_writer.writerows(processed_rows)\r\n\u001b[?2004l\r"]
[76.140631, "o", "\u001b[?2004h> "]
[76.140823, "o", "            print(f\"Generated migration plan: {MIGRATION_PLAN_FILE}\")\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.14093, "o", "        except UnicodeEncodeError as e:\r\n\u001b[?2004l\r"]
[76.140974, "o", "\u001b[?2004h> "]
[76.141259, "o", "            print(f\"Error: Could not encode data to ISO-8859-1: {e}. Please check source data for non-Latin-1 characters.\")\r\n\u001b[?2004l\r"]
[76.141272, "o", "\u001b[?2004h"]
[76.141321, "o", "> "]
[76.141365, "o", "            sys.exit(1)\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.141449, "o", "        except Exception as e:\r\n\u001b[?2004l\r"]
[76.141461, "o", "\u001b[?2004h"]
[76.141507, "o", "> "]
[76.141629, "o", "            print(f\"Error writing {MIGRATION_PLAN_FILE}: {e}\")\r\n\u001b[?2004l\r"]
[76.141641, "o", "\u001b[?2004h"]
[76.141688, "o", "> "]
[76.141729, "o", "            sys.exit(1)\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.141768, "o", "        \r\n\u001b[?2004l\r"]
[76.141781, "o", "\u001b[?2004h> "]
[76.141861, "o", "        # Generate manifest.json\r\n\u001b[?2004l\r"]
[76.141873, "o", "\u001b[?2004h"]
[76.141917, "o", "> "]
[76.142057, "o", "        # Record count = number of data rows in TSV, excluding header\r\n\u001b[?2004l\r"]
[76.14207, "o", "\u001b[?2004h"]
[76.142116, "o", "> "]
[76.142243, "o", "        record_count = len(processed_rows)\r\n\u001b[?2004l\r\u001b[?2004h>         \r\n\u001b[?2004l\r"]
[76.142307, "o", "\u001b[?2004h> "]
[76.142401, "o", "        # MD5 hash of the final migration_plan.tsv\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.142461, "o", "> "]
[76.142529, "o", "        md5_hash = \"\"\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.142605, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.142761, "o", "            with open(MIGRATION_PLAN_FILE, 'rb') as f:\r\n\u001b[?2004l\r"]
[76.142831, "o", "\u001b[?2004h> "]
[76.142977, "o", "                md5_hash = hashlib.md5(f.read()).hexdigest()\r\n\u001b[?2004l\r"]
[76.143038, "o", "\u001b[?2004h> "]
[76.143117, "o", "        except FileNotFoundError:\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.14318, "o", "> "]
[76.143354, "o", "            print(f\"Error: {MIGRATION_PLAN_FILE} not found for MD5 calculation.\")\r\n\u001b[?2004l\r"]
[76.143399, "o", "\u001b[?2004h> "]
[76.143439, "o", "            sys.exit(1)\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.1435, "o", "> "]
[76.143548, "o", "        except Exception as e:\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.14359, "o", "> "]
[76.143771, "o", "            print(f\"Error calculating MD5 for {MIGRATION_PLAN_FILE}: {e}\")\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.143825, "o", "> "]
[76.143867, "o", "            sys.exit(1)\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.143913, "o", "            \r\n\u001b[?2004l\r"]
[76.143954, "o", "\u001b[?2004h> "]
[76.144014, "o", "        manifest_data = {\r\n\u001b[?2004l\r"]
[76.144057, "o", "\u001b[?2004h> "]
[76.144151, "o", "            \"source_file\": SOURCE_INVENTORY,\r\n\u001b[?2004l\r"]
[76.144193, "o", "\u001b[?2004h> "]
[76.144294, "o", "            \"record_count\": record_count,\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.144336, "o", "> "]
[76.144406, "o", "            \"output_md5\": md5_hash\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.14446, "o", ">         }\r\n\u001b[?2004l\r"]
[76.144502, "o", "\u001b[?2004h> "]
[76.144543, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.144585, "o", "> "]
[76.144723, "o", "            with open(MANIFEST_FILE, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.144766, "o", "> "]
[76.144883, "o", "                json.dump(manifest_data, f, indent=2)\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.144926, "o", "> "]
[76.145047, "o", "            print(f\"Generated manifest: {MANIFEST_FILE}\")\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.14509, "o", "> "]
[76.145147, "o", "        except Exception as e:\r\n\u001b[?2004l\r"]
[76.145189, "o", "\u001b[?2004h> "]
[76.145312, "o", "            print(f\"Error writing {MANIFEST_FILE}: {e}\")\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.145354, "o", "> "]
[76.145395, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[76.145437, "o", "\u001b[?2004h> "]
[76.14549, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.145556, "o", "        # Generate audit.log\r\n\u001b[?2004l\r"]
[76.145598, "o", "\u001b[?2004h> "]
[76.145724, "o", "        # Size in bytes of the final migration_plan.tsv\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.145766, "o", "> "]
[76.145815, "o", "        file_size_bytes = 0\r\n\u001b[?2004l\r"]
[76.145858, "o", "\u001b[?2004h> "]
[76.145959, "o", "        if os.path.exists(MIGRATION_PLAN_FILE):\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.146011, "o", "> "]
[76.146147, "o", "            file_size_bytes = os.path.getsize(MIGRATION_PLAN_FILE)\r\n\u001b[?2004l\r"]
[76.146196, "o", "\u001b[?2004h>         \r\n\u001b[?2004l\r\u001b[?2004h"]
[76.146238, "o", "> "]
[76.146376, "o", "        audit_log_content = f\"{file_size_bytes} bytes generated\"\r\n\u001b[?2004l\r"]
[76.146419, "o", "\u001b[?2004h> "]
[76.14647, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.146631, "o", "            with open(AUDIT_LOG_FILE, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[76.146674, "o", "\u001b[?2004h> "]
[76.146789, "o", "                f.write(audit_log_content + \"\\n\") \r\n\u001b[?2004l\r"]
[76.146832, "o", "\u001b[?2004h> "]
[76.146965, "o", "            print(f\"Generated audit log: {AUDIT_LOG_FILE}\")\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.147008, "o", "> "]
[76.147069, "o", "        except Exception as e:\r\n\u001b[?2004l\r"]
[76.14711, "o", "\u001b[?2004h> "]
[76.147244, "o", "            print(f\"Error writing {AUDIT_LOG_FILE}: {e}\")\r\n\u001b[?2004l\r"]
[76.147285, "o", "\u001b[?2004h> "]
[76.14733, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[76.147372, "o", "\u001b[?2004h> "]
[76.147415, "o", "            \r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.147563, "o", "        print(\"All tasks completed successfully.\")\r\n\u001b[?2004l\r"]
[76.147609, "o", "\u001b[?2004h>         \r\n\u001b[?2004l\r"]
[76.147665, "o", "\u001b[?2004h> "]
[76.147717, "o", "    except FileNotFoundError:\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.147768, "o", "> "]
[76.147988, "o", "        print(f\"Error: Input file {SOURCE_INVENTORY} not found. Check $LEGACY_INVENTORY variable.\")\r\n\u001b[?2004l\r"]
[76.148035, "o", "\u001b[?2004h> "]
[76.148097, "o", "        sys.exit(1)\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.148166, "o", "    except Exception as e:\r\n\u001b[?2004l\r\u001b[?2004h"]
[76.148223, "o", "> "]
[76.14833, "o", "        print(f\"An unexpected error occurred: {e}\")\r\n\u001b[?2004l\r"]
[76.148377, "o", "\u001b[?2004h> "]
[76.14843, "o", "        sys.exit(1)\r\n\u001b[?2004l\r\u001b[?2004h> \r\n\u001b[?2004l\r"]
[76.148504, "o", "\u001b[?2004h> "]
[76.14856, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r\u001b[?2004h> "]
[76.148617, "o", "    main()\r\n\u001b[?2004l\r\u001b[?2004h> EOF\r\n\u001b[?2004l\r"]
[76.149864, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[76.26971, "i", "python3 /tmp/migration_script.py\n"]
[76.269855, "o", "python3 /tmp/migration_script.py\r\n\u001b[?2004l\r"]
[76.27763, "o", "  File \"/tmp/migration_script.py\", line 75\r\n    line = line_bytes.decode('utf-16le').strip('\r\n                                               ^\r\nSyntaxError: unterminated string literal (detected at line 75)\r\n"]
[76.27919, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[81.32772, "i", "ls -la /home/user/cloud/output/\n"]
[81.327997, "o", "ls -la /home/user/cloud/output/\r\n"]
[81.328012, "o", "\u001b[?2004l\r"]
[81.330621, "o", "total 0\r\n"]
[81.330702, "o", "drwxrwxrwx 1 root root  6 Feb 13 00:00 \u001b[0m\u001b[34;42m.\u001b[0m\r\ndrwxrwxrwx 1 root root 90 Feb 13 00:00 \u001b[34;42m..\u001b[0m\r\n"]
[81.331044, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[194.061317, "i", "rm -f /tmp/migration_script.py\n"]
[194.061466, "o", "rm -f /tmp/migration_script.py\r\n\u001b[?2004l\r"]
[194.062724, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[194.157332, "i", "cat > /tmp/migration_script.py << 'EOF'\n#!/usr/bin/env python3\nimport os\nimport sys\nimport csv\nimport json\nimport hashlib\nimport re\nfrom pathlib import Path\n\n# --- Configuration ---\nSOURCE_INVENTORY = os.environ.get('LEGACY_INVENTORY')\nCONFIG_FILE = '/home/user/cloud/.config'\nOUTPUT_DIR = '/home/user/cloud/output'\nDOMAIN_SUFFIX = None\nMIGRATION_PLAN_FILE = os.path.join(OUTPUT_DIR, 'migration_plan.tsv')\nMANIFEST_FILE = os.path.join(OUTPUT_DIR, 'manifest.json')\nAUDIT_LOG_FILE = os.path.join(OUTPUT_DIR, 'audit.log')\n\nclass CRLFWriter:\n    \"\"\"A CSV writer that forces CRLF line endings and encodes to ISO-8859-1.\"\"\"\n    def __init__(self, f):\n        self.writer = csv.writer(f, dialect='excel', lineterminator='\\r\\n')\n\n    def writerow(self, row):\n        self.writer.writerow(row)\n\n    def writerows(self, rows):\n        for r in rows:\n            self.writerow(r)\n\ndef remove_zwsp(text):\n    \"\"\"Remove Zero Width Spaces (U+200B).\"\"\"\n    if text:\n        return text.replace('\\u200b', '')\n    return text\n\ndef main():\n  "]
[194.157461, "i", "  if not os.path.exists(OUTPUT_DIR):\n        os.makedirs(OUTPUT_DIR, exist_ok=True)\n\n    # Read domain suffix\n    try:\n        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:\n            domain_suffix_content = f.read().strip()\n            if domain_suffix_content:\n                DOMAIN_SUFFIX = domain_suffix_content\n            else:\n                DOMAIN_SUFFIX = ''\n    except FileNotFoundError:\n        DOMAIN_SUFFIX = ''\n\n    if not SOURCE_INVENTORY:\n        sys.exit(1)\n    if not os.path.exists(SOURCE_INVENTORY):\n        sys.exit(1)\n\n    processed_rows = []\n    output_header = [\"ServiceName\", \"TargetFQDN\"]\n\n    try:\n        with open(SOURCE_INVENTORY, 'rb') as f_in:\n            decoded_lines = []\n            for line_bytes in f_in:\n                line = line_bytes.decode('utf-16le').strip('\\r\\n')\n                decoded_lines.append(line)\n            \n            reader = csv.reader(decoded_lines, delimiter=',')\n            for row_idx, row_data in enumerate(reader):\n                if not row_data"]
[194.157507, "i", ":\n                    continue\n                if row_data and row_data[0].startswith('#'):\n                    continue\n                try:\n                    service_name = row_data[0] \n                    status = row_data[1] \n                    new_server_val = row_data[2]\n                except IndexError:\n                    continue\n                if status in ['Decom', 'Pending']:\n                    continue\n                if not new_server_val or new_server_val.isspace():\n                    continue\n                processed_service_name = service_name.lower()\n                target_fqdn = remove_zwsp(new_server_val) + DOMAIN_SUFFIX\n                processed_rows.append([processed_service_name, target_fqdn])\n        \n        processed_rows.sort(key=lambda x: x[0])\n\n        try:\n            with open(MIGRATION_PLAN_FILE, 'w', encoding='iso-8859-1', errors='replace') as f_out:\n                tsv_writer = CRLFWriter(f_out)\n                tsv_writer.writerow(output_header)\n                tsv_wr"]
[194.157539, "i", "iter.writerows(processed_rows)\n        except UnicodeEncodeError:\n            sys.exit(1)\n        except Exception:\n            sys.exit(1)\n\n        record_count = len(processed_rows)\n        \n        md5_hash = \"\"\n        try:\n            with open(MIGRATION_PLAN_FILE, 'rb') as f:\n                md5_hash = hashlib.md5(f.read()).hexdigest()\n        except FileNotFoundError:\n            sys.exit(1)\n        except Exception:\n            sys.exit(1)\n            \n        manifest_data = {\n            \"source_file\": SOURCE_INVENTORY,\n            \"record_count\": record_count,\n            \"output_md5\": md5_hash\n        }\n        try:\n            with open(MANIFEST_FILE, 'w', encoding='utf-8') as f:\n                json.dump(manifest_data, f, indent=2)\n        except Exception:\n            sys.exit(1)\n            \n        file_size_bytes = 0\n        if os.path.exists(MIGRATION_PLAN_FILE):\n            file_size_bytes = os.path.getsize(MIGRATION_PLAN_FILE)\n\n        audit_log_content = f\"{file_size_bytes} bytes generat"]
[194.157569, "i", "ed\"\n        try:\n            with open(AUDIT_LOG_FILE, 'w', encoding='utf-8') as f:\n                f.write(audit_log_content + \"\\n\")\n        except Exception:\n            sys.exit(1)\n            \n    except FileNotFoundError:\n        sys.exit(1)\n    except Exception:\n        sys.exit(1)\n\nif __name__ == '__main__':\n    main()\nEOF\n"]
[194.157636, "o", "cat > /tmp/migration_script.py << 'EOF'\r\n"]
[194.157654, "o", "\u001b[?2004l\r"]
[194.157691, "o", "\u001b[?2004h"]
[194.157717, "o", "> "]
[194.157832, "o", "#!/usr/bin/env python3\r\n\u001b[?2004l\r"]
[194.157861, "o", "\u001b[?2004h"]
[194.157885, "o", "> "]
[194.157931, "o", "import os\r\n\u001b[?2004l\r"]
[194.157962, "o", "\u001b[?2004h"]
[194.15799, "o", "> "]
[194.158034, "o", "import sys\r\n\u001b[?2004l\r"]
[194.158061, "o", "\u001b[?2004h"]
[194.158087, "o", "> "]
[194.158135, "o", "import csv\r\n"]
[194.158162, "o", "\u001b[?2004l\r\u001b[?2004h"]
[194.15819, "o", "> "]
[194.158241, "o", "import json\r\n\u001b[?2004l\r"]
[194.158267, "o", "\u001b[?2004h"]
[194.158284, "o", "> "]
[194.158365, "o", "import hashlib\r\n\u001b[?2004l\r"]
[194.158392, "o", "\u001b[?2004h"]
[194.158426, "o", "> "]
[194.158504, "o", "import re\r\n\u001b[?2004l\r\u001b[?2004h"]
[194.158533, "o", "> "]
[194.158651, "o", "from pathlib import Path\r\n\u001b[?2004l\r"]
[194.158676, "o", "\u001b[?2004h"]
[194.158703, "o", "> \r\n\u001b[?2004l\r"]
[194.158727, "o", "\u001b[?2004h> "]
[194.158861, "o", "# --- Configuration ---\r\n\u001b[?2004l\r"]
[194.158893, "o", "\u001b[?2004h"]
[194.15892, "o", "> "]
[194.159195, "o", "SOURCE_INVENTORY = os.environ.get('LEGACY_INVENTORY')\r\n\u001b[?2004l\r"]
[194.159233, "o", "\u001b[?2004h"]
[194.159259, "o", "> "]
[194.159472, "o", "CONFIG_FILE = '/home/user/cloud/.config'\r\n\u001b[?2004l\r"]
[194.159506, "o", "\u001b[?2004h"]
[194.159534, "o", "> "]
[194.15973, "o", "OUTPUT_DIR = '/home/user/cloud/output'\r\n\u001b[?2004l\r"]
[194.159761, "o", "\u001b[?2004h"]
[194.159789, "o", "> "]
[194.159885, "o", "DOMAIN_SUFFIX = None\r\n\u001b[?2004l\r"]
[194.159916, "o", "\u001b[?2004h"]
[194.15994, "o", "> "]
[194.16023, "o", "MIGRATION_PLAN_FILE = os.path.join(OUTPUT_DIR, 'migration_plan.tsv')\r\n\u001b[?2004l\r"]
[194.160269, "o", "\u001b[?2004h"]
[194.160293, "o", "> "]
[194.160549, "o", "MANIFEST_FILE = os.path.join(OUTPUT_DIR, 'manifest.json')\r\n\u001b[?2004l\r"]
[194.160579, "o", "\u001b[?2004h"]
[194.160603, "o", "> "]
[194.160839, "o", "AUDIT_LOG_FILE = os.path.join(OUTPUT_DIR, 'audit.log')\r\n\u001b[?2004l\r"]
[194.160864, "o", "\u001b[?2004h"]
[194.16089, "o", "> \r\n\u001b[?2004l\r"]
[194.160913, "o", "\u001b[?2004h"]
[194.160936, "o", "> "]
[194.161009, "o", "class CRLFWriter:\r\n\u001b[?2004l\r"]
[194.161041, "o", "\u001b[?2004h"]
[194.161065, "o", "> "]
[194.161409, "o", "    \"\"\"A CSV writer that forces CRLF line endings and encodes to ISO-8859-1.\"\"\"\r\n\u001b[?2004l\r"]
[194.161435, "o", "\u001b[?2004h"]
[194.161473, "o", "> "]
[194.161585, "o", "    def __init__(self, f):\r\n\u001b[?2004l\r"]
[194.161614, "o", "\u001b[?2004h"]
[194.161638, "o", "> "]
[194.161961, "o", "        self.writer = csv.writer(f, dialect='excel', lineterminator='\\r\\n')\r\n\u001b[?2004l\r"]
[194.161994, "o", "\u001b[?2004h"]
[194.16202, "o", "> \r\n\u001b[?2004l\r"]
[194.162047, "o", "\u001b[?2004h> "]
[194.162182, "o", "    def writerow(self, row):\r\n\u001b[?2004l\r"]
[194.162214, "o", "\u001b[?2004h"]
[194.162238, "o", "> "]
[194.162379, "o", "        self.writer.writerow(row)\r\n\u001b[?2004l\r"]
[194.162405, "o", "\u001b[?2004h"]
[194.162431, "o", "> \r\n\u001b[?2004l\r"]
[194.162463, "o", "\u001b[?2004h> "]
[194.162606, "o", "    def writerows(self, rows):\r\n\u001b[?2004l\r"]
[194.162635, "o", "\u001b[?2004h"]
[194.162658, "o", "> "]
[194.162752, "o", "        for r in rows:\r\n\u001b[?2004l\r"]
[194.16278, "o", "\u001b[?2004h"]
[194.162803, "o", "> "]
[194.162921, "o", "            self.writerow(r)\r\n\u001b[?2004l\r"]
[194.162952, "o", "\u001b[?2004h"]
[194.162978, "o", "> \r\n\u001b[?2004l\r"]
[194.163003, "o", "\u001b[?2004h"]
[194.163026, "o", "> "]
[194.163124, "o", "def remove_zwsp(text):\r\n\u001b[?2004l\r"]
[194.163152, "o", "\u001b[?2004h"]
[194.163175, "o", "> "]
[194.163362, "o", "    \"\"\"Remove Zero Width Spaces (U+200B).\"\"\"\r\n\u001b[?2004l\r"]
[194.16339, "o", "\u001b[?2004h"]
[194.163414, "o", "> "]
[194.163472, "o", "    if text:\r\n\u001b[?2004l\r"]
[194.163497, "o", "\u001b[?2004h"]
[194.163521, "o", "> "]
[194.163695, "o", "        return text.replace('\\u200b', '')\r\n\u001b[?2004l\r"]
[194.16373, "o", "\u001b[?2004h"]
[194.163754, "o", "> "]
[194.163816, "o", "    return text\r\n\u001b[?2004l\r"]
[194.163844, "o", "\u001b[?2004h"]
[194.163869, "o", "> \r\n\u001b[?2004l\r"]
[194.163895, "o", "\u001b[?2004h> "]
[194.163955, "o", "def main():\r\n\u001b[?2004l\r"]
[194.163984, "o", "\u001b[?2004h"]
[194.164009, "o", "> "]
[194.164169, "o", "    if not os.path.exists(OUTPUT_DIR):\r\n\u001b[?2004l\r"]
[194.164205, "o", "\u001b[?2004h"]
[194.164229, "o", "> "]
[194.164428, "o", "        os.makedirs(OUTPUT_DIR, exist_ok=True)\r\n\u001b[?2004l\r"]
[194.164468, "o", "\u001b[?2004h"]
[194.164494, "o", "> \r\n\u001b[?2004l\r"]
[194.164526, "o", "\u001b[?2004h"]
[194.16455, "o", "> "]
[194.16464, "o", "    # Read domain suffix\r\n\u001b[?2004l\r"]
[194.164674, "o", "\u001b[?2004h"]
[194.164699, "o", "> "]
[194.164731, "o", "    try:\r\n\u001b[?2004l\r"]
[194.164758, "o", "\u001b[?2004h"]
[194.164783, "o", "> "]
[194.165041, "o", "        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[194.165067, "o", "\u001b[?2004h"]
[194.165092, "o", "> "]
[194.165313, "o", "            domain_suffix_content = f.read().strip()\r\n\u001b[?2004l\r"]
[194.165342, "o", "\u001b[?2004h"]
[194.165368, "o", "> "]
[194.165533, "o", "            if domain_suffix_content:\r\n\u001b[?2004l\r"]
[194.165566, "o", "\u001b[?2004h"]
[194.165591, "o", "> "]
[194.16582, "o", "                DOMAIN_SUFFIX = domain_suffix_content\r\n\u001b[?2004l\r"]
[194.165849, "o", "\u001b[?2004h"]
[194.165874, "o", "> "]
[194.165949, "o", "            else:\r\n\u001b[?2004l\r"]
[194.165978, "o", "\u001b[?2004h"]
[194.166002, "o", "> "]
[194.16615, "o", "                DOMAIN_SUFFIX = ''\r\n\u001b[?2004l\r"]
[194.166179, "o", "\u001b[?2004h"]
[194.166204, "o", "> "]
[194.166329, "o", "    except FileNotFoundError:\r\n\u001b[?2004l\r"]
[194.166356, "o", "\u001b[?2004h"]
[194.166382, "o", "> "]
[194.166497, "o", "        DOMAIN_SUFFIX = ''\r\n\u001b[?2004l\r"]
[194.166527, "o", "\u001b[?2004h"]
[194.166554, "o", "> \r\n\u001b[?2004l\r"]
[194.166579, "o", "\u001b[?2004h> "]
[194.166712, "o", "    if not SOURCE_INVENTORY:\r\n\u001b[?2004l\r"]
[194.166742, "o", "\u001b[?2004h"]
[194.166766, "o", "> "]
[194.166849, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[194.166879, "o", "\u001b[?2004h"]
[194.166903, "o", "> "]
[194.167094, "o", "    if not os.path.exists(SOURCE_INVENTORY):\r\n\u001b[?2004l\r"]
[194.167123, "o", "\u001b[?2004h"]
[194.167147, "o", "> "]
[194.167231, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[194.167258, "o", "\u001b[?2004h"]
[194.167284, "o", "> \r\n\u001b[?2004l\r"]
[194.167309, "o", "\u001b[?2004h> "]
[194.167421, "o", "    processed_rows = []\r\n"]
[194.167459, "o", "\u001b[?2004l\r\u001b[?2004h"]
[194.167485, "o", "> "]
[194.167707, "o", "    output_header = [\"ServiceName\", \"TargetFQDN\"]\r\n\u001b[?2004l\r"]
[194.167736, "o", "\u001b[?2004h"]
[194.167761, "o", "> \r\n\u001b[?2004l\r"]
[194.167784, "o", "\u001b[?2004h"]
[194.167809, "o", "> "]
[194.167839, "o", "    try:\r\n\u001b[?2004l\r"]
[194.167864, "o", "\u001b[?2004h"]
[194.167889, "o", "> "]
[194.168101, "o", "        with open(SOURCE_INVENTORY, 'rb') as f_in:\r\n\u001b[?2004l\r"]
[194.168126, "o", "\u001b[?2004h"]
[194.16815, "o", "> "]
[194.168276, "o", "            decoded_lines = []\r\n\u001b[?2004l\r"]
[194.168306, "o", "\u001b[?2004h"]
[194.16833, "o", "> "]
[194.168487, "o", "            for line_bytes in f_in:\r\n\u001b[?2004l\r"]
[194.168516, "o", "\u001b[?2004h"]
[194.168539, "o", "> "]
[194.168822, "o", "                line = line_bytes.decode('utf-16le').strip('\\r\\n')\r\n\u001b[?2004l\r"]
[194.168848, "o", "\u001b[?2004h"]
[194.168874, "o", "> "]
[194.169051, "o", "                decoded_lines.append(line)\r\n\u001b[?2004l\r"]
[194.169078, "o", "\u001b[?2004h"]
[194.169104, "o", "> "]
[194.16915, "o", "            \r\n\u001b[?2004l\r"]
[194.169184, "o", "\u001b[?2004h"]
[194.16921, "o", "> "]
[194.16948, "o", "            reader = csv.reader(decoded_lines, delimiter=',')\r\n\u001b[?2004l\r"]
[194.169506, "o", "\u001b[?2004h"]
[194.169529, "o", "> "]
[194.169764, "o", "            for row_idx, row_data in enumerate(reader):\r\n\u001b[?2004l\r"]
[194.169797, "o", "\u001b[?2004h"]
[194.169821, "o", "> "]
[194.169961, "o", "                if not row_data:\r\n\u001b[?2004l\r"]
[194.16999, "o", "\u001b[?2004h"]
[194.170015, "o", "> "]
[194.170132, "o", "                    continue\r\n\u001b[?2004l\r"]
[194.170159, "o", "\u001b[?2004h"]
[194.170183, "o", "> "]
[194.170437, "o", "                if row_data and row_data[0].startswith('#'):\r\n\u001b[?2004l\r"]
[194.170475, "o", "\u001b[?2004h"]
[194.170499, "o", "> "]
[194.170623, "o", "                    continue\r\n\u001b[?2004l\r"]
[194.170662, "o", "\u001b[?2004h> "]
[194.170765, "o", "                try:\r\n\u001b[?2004l\r"]
[194.170804, "o", "\u001b[?2004h> "]
[194.171035, "o", "                    service_name = row_data[0] \r\n\u001b[?2004l\r\u001b[?2004h"]
[194.171073, "o", "> "]
[194.171241, "o", "                    status = row_data[1] \r\n\u001b[?2004l\r"]
[194.171267, "o", "\u001b[?2004h"]
[194.171292, "o", "> "]
[194.171504, "o", "                    new_server_val = row_data[2]\r\n\u001b[?2004l\r"]
[194.171532, "o", "\u001b[?2004h"]
[194.171557, "o", "> "]
[194.171703, "o", "                except IndexError:\r\n\u001b[?2004l\r"]
[194.171729, "o", "\u001b[?2004h"]
[194.171742, "o", "> "]
[194.171871, "o", "                    continue\r\n\u001b[?2004l\r"]
[194.1719, "o", "\u001b[?2004h"]
[194.171928, "o", "> "]
[194.17214, "o", "                if status in ['Decom', 'Pending']:\r\n\u001b[?2004l\r"]
[194.172167, "o", "\u001b[?2004h"]
[194.172192, "o", "> "]
[194.172311, "o", "                    continue\r\n\u001b[?2004l\r"]
[194.172339, "o", "\u001b[?2004h"]
[194.172363, "o", "> "]
[194.172657, "o", "                if not new_server_val or new_server_val.isspace():\r\n\u001b[?2004l\r"]
[194.172685, "o", "\u001b[?2004h"]
[194.17271, "o", "> "]
[194.172827, "o", "                    continue\r\n\u001b[?2004l\r"]
[194.172858, "o", "\u001b[?2004h"]
[194.172882, "o", "> "]
[194.173142, "o", "                processed_service_name = service_name.lower()\r\n\u001b[?2004l\r"]
[194.173168, "o", "\u001b[?2004h"]
[194.173191, "o", "> "]
[194.17352, "o", "                target_fqdn = remove_zwsp(new_server_val) + DOMAIN_SUFFIX\r\n\u001b[?2004l\r"]
[194.173547, "o", "\u001b[?2004h> "]
[194.173889, "o", "                processed_rows.append([processed_service_name, target_fqdn])\r\n\u001b[?2004l\r"]
[194.173917, "o", "\u001b[?2004h"]
[194.173941, "o", "> "]
[194.17397, "o", "        \r\n\u001b[?2004l\r"]
[194.173995, "o", "\u001b[?2004h"]
[194.174019, "o", "> "]
[194.174219, "o", "        processed_rows.sort(key=lambda x: x[0])\r\n\u001b[?2004l\r"]
[194.174245, "o", "\u001b[?2004h"]
[194.174271, "o", "> \r\n\u001b[?2004l\r"]
[194.174297, "o", "\u001b[?2004h> "]
[194.174361, "o", "        try:\r\n\u001b[?2004l\r"]
[194.174392, "o", "\u001b[?2004h"]
[194.174416, "o", "> "]
[194.174849, "o", "            with open(MIGRATION_PLAN_FILE, 'w', encoding='iso-8859-1', errors='replace') as f_out:\r\n\u001b[?2004l\r"]
[194.174877, "o", "\u001b[?2004h"]
[194.174901, "o", "> "]
[194.175096, "o", "                tsv_writer = CRLFWriter(f_out)\r\n\u001b[?2004l\r"]
[194.175127, "o", "\u001b[?2004h"]
[194.175151, "o", "> "]
[194.175363, "o", "                tsv_writer.writerow(output_header)\r\n\u001b[?2004l\r"]
[194.175392, "o", "\u001b[?2004h"]
[194.175416, "o", "> "]
[194.175655, "o", "                tsv_writer.writerows(processed_rows)\r\n\u001b[?2004l\r"]
[194.175688, "o", "\u001b[?2004h> "]
[194.175849, "o", "        except UnicodeEncodeError:\r\n\u001b[?2004l\r"]
[194.175883, "o", "\u001b[?2004h> "]
[194.175995, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[194.176024, "o", "\u001b[?2004h"]
[194.176049, "o", "> "]
[194.176157, "o", "        except Exception:\r\n\u001b[?2004l\r"]
[194.176185, "o", "\u001b[?2004h"]
[194.17621, "o", "> "]
[194.176304, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[194.17633, "o", "\u001b[?2004h"]
[194.176356, "o", "> \r\n\u001b[?2004l\r"]
[194.176381, "o", "\u001b[?2004h> "]
[194.176588, "o", "        record_count = len(processed_rows)\r\n\u001b[?2004l\r"]
[194.176631, "o", "\u001b[?2004h> "]
[194.176673, "o", "        \r\n\u001b[?2004l\r"]
[194.176713, "o", "\u001b[?2004h"]
[194.176727, "o", "> "]
[194.176817, "o", "        md5_hash = \"\"\r\n\u001b[?2004l\r"]
[194.176859, "o", "\u001b[?2004h"]
[194.176898, "o", "> "]
[194.176946, "o", "        try:\r\n\u001b[?2004l\r"]
[194.176959, "o", "\u001b[?2004h> "]
[194.177209, "o", "            with open(MIGRATION_PLAN_FILE, 'rb') as f:\r\n\u001b[?2004l\r"]
[194.17726, "o", "\u001b[?2004h> "]
[194.177536, "o", "                md5_hash = hashlib.md5(f.read()).hexdigest()\r\n\u001b[?2004l\r"]
[194.17759, "o", "\u001b[?2004h> "]
[194.177715, "o", "        except FileNotFoundError:\r\n\u001b[?2004l\r"]
[194.177771, "o", "\u001b[?2004h> "]
[194.177856, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[194.177912, "o", "\u001b[?2004h> "]
[194.178006, "o", "        except Exception:\r\n\u001b[?2004l\r"]
[194.178061, "o", "\u001b[?2004h> "]
[194.178143, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[194.178196, "o", "\u001b[?2004h> "]
[194.178238, "o", "            \r\n\u001b[?2004l\r"]
[194.178291, "o", "\u001b[?2004h> "]
[194.178382, "o", "        manifest_data = {\r\n\u001b[?2004l\r"]
[194.178436, "o", "\u001b[?2004h> "]
[194.178613, "o", "            \"source_file\": SOURCE_INVENTORY,\r\n\u001b[?2004l\r"]
[194.178669, "o", "\u001b[?2004h> "]
[194.178828, "o", "            \"record_count\": record_count,\r\n\u001b[?2004l\r"]
[194.178883, "o", "\u001b[?2004h> "]
[194.179018, "o", "            \"output_md5\": md5_hash\r\n\u001b[?2004l\r"]
[194.179074, "o", "\u001b[?2004h> "]
[194.179133, "o", "        }\r\n\u001b[?2004l\r\u001b[?2004h"]
[194.179147, "o", "> "]
[194.179217, "o", "        try:\r\n\u001b[?2004l\r\u001b[?2004h"]
[194.179231, "o", "> "]
[194.179507, "o", "            with open(MANIFEST_FILE, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[194.179565, "o", "\u001b[?2004h> "]
[194.179768, "o", "                json.dump(manifest_data, f, indent=2)\r\n\u001b[?2004l\r"]
[194.179823, "o", "\u001b[?2004h> "]
[194.179912, "o", "        except Exception:\r\n\u001b[?2004l\r"]
[194.179966, "o", "\u001b[?2004h> "]
[194.180053, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[194.180102, "o", "\u001b[?2004h> "]
[194.180154, "o", "            \r\n\u001b[?2004l\r"]
[194.180209, "o", "\u001b[?2004h> "]
[194.180312, "o", "        file_size_bytes = 0\r\n\u001b[?2004l\r"]
[194.180367, "o", "\u001b[?2004h> "]
[194.180557, "o", "        if os.path.exists(MIGRATION_PLAN_FILE):\r\n\u001b[?2004l\r"]
[194.180618, "o", "\u001b[?2004h> "]
[194.180871, "o", "            file_size_bytes = os.path.getsize(MIGRATION_PLAN_FILE)\r\n\u001b[?2004l\r"]
[194.180935, "o", "\u001b[?2004h> \r\n\u001b[?2004l\r"]
[194.180992, "o", "\u001b[?2004h> "]
[194.181245, "o", "        audit_log_content = f\"{file_size_bytes} bytes generated\"\r\n\u001b[?2004l\r"]
[194.181306, "o", "\u001b[?2004h> "]
[194.181348, "o", "        try:\r\n\u001b[?2004l\r"]
[194.181402, "o", "\u001b[?2004h> "]
[194.18167, "o", "            with open(AUDIT_LOG_FILE, 'w', encoding='utf-8') as f:\r\n\u001b[?2004l\r"]
[194.181723, "o", "\u001b[?2004h> "]
[194.181913, "o", "                f.write(audit_log_content + \"\\n\")\r\n\u001b[?2004l\r"]
[194.181968, "o", "\u001b[?2004h> "]
[194.182066, "o", "        except Exception:\r\n\u001b[?2004l\r"]
[194.182113, "o", "\u001b[?2004h> "]
[194.182208, "o", "            sys.exit(1)\r\n\u001b[?2004l\r"]
[194.182256, "o", "\u001b[?2004h> "]
[194.182309, "o", "            \r\n\u001b[?2004l\r"]
[194.182356, "o", "\u001b[?2004h> "]
[194.182493, "o", "    except FileNotFoundError:\r\n\u001b[?2004l\r"]
[194.182548, "o", "\u001b[?2004h> "]
[194.182623, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[194.182669, "o", "\u001b[?2004h> "]
[194.182764, "o", "    except Exception:\r\n\u001b[?2004l\r"]
[194.182811, "o", "\u001b[?2004h> "]
[194.182897, "o", "        sys.exit(1)\r\n\u001b[?2004l\r"]
[194.182932, "o", "\u001b[?2004h"]
[194.182945, "o", "> \r\n\u001b[?2004l\r"]
[194.183006, "o", "\u001b[?2004h> "]
[194.183113, "o", "if __name__ == '__main__':\r\n\u001b[?2004l\r"]
[194.183167, "o", "\u001b[?2004h> "]
[194.183223, "o", "    main()\r\n\u001b[?2004l\r\u001b[?2004h"]
[194.183235, "o", "> "]
[194.183292, "o", "EOF\r\n\u001b[?2004l\r"]
[194.188485, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[194.30427, "i", "python3 /tmp/migration_script.py\n"]
[194.304399, "o", "python3 /tmp/migration_script.py\r\n\u001b[?2004l\r"]
[194.324631, "o", "\u001b[?2004hroot@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
[199.309505, "i", "ls -la /home/user/cloud/output/\n"]
[199.309647, "o", "ls -la /home/user/cloud/output/\r\n\u001b[?2004l\r"]
[199.310992, "o", "total 0\r\ndrwxrwxrwx 1 root root  6 Feb 13 00:00 \u001b[0m\u001b[34;42m.\u001b[0m\r\n"]
[199.311073, "o", "drwxrwxrwx 1 root root 90 Feb 13 00:00 \u001b[34;42m..\u001b[0m\r\n"]
[199.31125, "o", "\u001b[?2004h"]
[199.311278, "o", "root@7580c8f0-e7c8-4065-8c8c-491704ad27c6:/home/user/cloud# "]
