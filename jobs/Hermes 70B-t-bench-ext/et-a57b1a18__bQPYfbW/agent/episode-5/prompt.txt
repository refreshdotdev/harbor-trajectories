New Terminal Output:

root@86753427-7808-482c-8ff7-8f392a0d20f2:/home/user/projects/api-test# echo 'import os
      return

    for filename in sorted(os.listdir(input_dir)):
        if filename.endswith(".json") and not os.path.islink(os.path.join(input_dir, filename)):
            processed_files += 1
            file_path = os.path.join(input_dir, filename)

            try:
                # Read file and handle BOM
                with open(file_path, "r", encoding="utf-8-sig") as f:
                    content = f.read()
                    bom_present = (content[:1] == "\\ufeff")
                    if bom_present:
                        content = content[1:]

                # Validate JSON syntax
                try:
                    user_data = json.loads(content)
                except json.JSONDecodeError as e:
                    log_entries.append(f"{log_prefix}{filename} - INVALID: JSON syntax error")
                    continue

                # Validate against schema (simplified validation)
                required_fields = ["id", "name", "email", "isActive", "role"]
                if not all(field in user_data for field in required_fields):
                    log_entries.append(f"{log_prefix}{filename} - INVALID: Missing required fields")
                    continue

                # Role-specific validation (simplified)
                role = user_data.get("role")
                if role == "admin" and "department" not in user_data:
                    log_entries.append(f"{log_prefix}{filename} - INVALID: Admin must have department")
                    continue
                elif role == "user" and "department" in user_data:
                    log_entries.append(f"{log_prefix}{filename} - INVALID: User must not have department")
                    continue
                elif role == "manager":
                    is_active = user_data.get("isActive", False)
                    if is_active and "department" not in user_data:
                        log_entries.append(f"{log_prefix}{filename} - INVALID: Active manager must have department")
                        continue
                    elif not is_active and "department" in user_data:
                        log_entries.append(f"{log_prefix}{filename} - INVALID: Inactive manager must not have department")
                        continue

                # If all validations pass, add to valid users
                # Apply transformations: lowercase email, uppercase role
                valid_user = OrderedDict()
                valid_user["email"] = user_data["email"].lower()
                valid_user["id"] = user_data["id"]
                valid_user["role"] = user_data["role"].upper()
                valid_users.append(valid_user)

                # Log with BOM status
                status = "VALID (BOM stripped)" if bom_present else "VALID"
                log_entries.append(f"{log_prefix}{filename} - {status}")

            except Exception as e:
                log_entries.append(f"{log_prefix}{filename} - INVALID: {> import json
> import sys
> import re
ile in> from collections import OrderedDict
>
> # Create output directory
> os.makedirs("output", exist_ok=True)
>
.> # Get log prefix from environment variable TEST_CONF
> log_prefix = ""
> try:
f>     conf_file = os.environ.get("TEST_CONF")
>     if conf_file and os.path.exists(conf_file):
>         with open(conf_file, "r") as f:
>             content = f.read()
>             match = re.search(r"LOG_PREFIX\\s*=\\s*\"([^\"]*)\"", content)
>             if match:
>                 log_prefix = match.group(1)
>     else:
>         print("Warning: TEST_CONF environment variable not set or file not found", file=sys.stderr)
> except Exception as e:
>     print(f"Error reading config: {e}", file=sys.stderr)
>
> # Initialize counters
> processed_files = 0
> valid_users = []
> log_entries = []
>
> # Process input files
> def process_input_files():
>     global processed_files, valid_users, log_entries
>
>     input_dir = "input"
>     if not os.path.exists(input_dir):
>         print(f"Input directory not found: {input_dir}")
>         return
>
>     for filename in sorted(os.listdir(input_dir)):
>         if filename.endswith(".json") and not os.path.islink(os.path.join(input_dir, filename)):
>             processed_files += 1
>             file_path = os.path.join(input_dir, filename)
>
>             try:
>                 # Read file and handle BOM
>                 with open(file_path, "r", encoding="utf-8-sig") as f:
>                     content = f.read()
>                     bom_present = (content[:1] == "\\ufeff")
>                     if bom_present:
>                         content = content[1:]
>
>                 # Validate JSON syntax
>                 try:
>                     user_data = json.loads(content)
>                 except json.JSONDecodeError as e:
>                     log_entries.append(f"{log_prefix}{filename} - INVALID: JSON syntax error")
>                     continue
>
>                 # Validate against schema (simplified validation)
>                 required_fields = ["id", "name", "email", "isActive", "role"]
>                 if not all(field in user_data for field in required_fields):
>                     log_entries.append(f"{log_prefix}{filename} - INVALID: Missing required fields")
>                     continue
>
>                 # Role-specific validation (simplified)
>                 role = user_data.get("role")
>                 if role == "admin" and "department" not in user_data:
>                     log_entries.append(f"{log_prefix}{filename} - INVALID: Admin must have department")
>                     continue
>                 elif role == "user" and "department" in user_data:
>                     log_entries.append(f"{log_prefix}{filename} - INVALID: User must not have department")
>                     continue
>                 elif role == "manager":
>                     is_active = user_data.get("isActive", False)
>                     if is_active and "department" not in user_data:
>                         log_entries.append(f"{log_prefix}{filename} - INVALID: Active manager must have department")
>                         continue
>                     elif not is_active and "department" in user_data:
>                         log_entries.append(f"{log_prefix}{filename} - INVALID: Inactive manager must not have department")
>                         continue
>
>                 # If all validations pass, add to valid users
>                 # Apply transformations: lowercase email, uppercase role
>                 valid_user = OrderedDict()
>                 valid_user["email"] = user_data["email"].lower()
>                 valid_user["id"] = user_data["id"]
>                 valid_user["role"] = user_data["role"].upper()
>                 valid_users.append(valid_user)
>
>                 # Log with BOM status
>                 status = "VALID (BOM stripped)" if bom_present else "VALID"
>                 log_entries.append(f"{log_prefix}{filename} - {status}")
>
>             except Exception as e:
>                 log_entries.append(f"{log_prefix}{filename} - INVALID: {str(e)}")
>
> # Generate log file in ISO-8859-1 encoding
> def generate_log_file():
>     try:
>         log_content = "\\n".join(log_entries).encode("iso-8859-1")
>         with open("output/validation.log", "wb") as f:
>             f.write(log_content)
>     except Exception as e:
>         print(f"Error writing log file: {e}", file=sys.stderr)
>
> # Generate valid users JSON without trailing newline
> def generate_valid_users_file():
>     try:
>         # Create a string with no trailing newline
>         json_str = json.dumps(valid_users, indent=2)
>         # Remove the trailing newline that json.dumps adds
>         json_str = json_str.rstrip() + "\\n"
>         with open("output/valid_users.json", "w") as f:
>             f.write(json_str)
>     except Exception as e:
>         print(f"Error writing valid users file: {e}", file=sys.stderr)
>
> # Generate summary file with CRLF line endings
> def generate_summary_file():
>     try:
>         # Calculate MD5 hash of the log file
>         import hashlib
>         with open("output/validation.log", "rb") as f:
>             log_content = f.read()
>         log_md5 = hashlib.md5(log_content).hexdigest()
>
>         summary_content = f"Files: {processed_files}\\r\\nPassed: {len(valid_users)}\\r\\nLog Checksum: {log_md5}\\r\\n"
>         with open("output/summary.txt", "w") as f:
>             f.write(summary_content)
>     except Exception as e:
>         print(f"Error writing summary file: {e}", file=sys.stderr)
>
> # Main execution
> if __name__ == "__main__":
>     process_input_files()
>     generate_log_file()
>     generate_valid_users_file()
>     generate_summary_file()
>     print("Processing complete. Deliverables generated.")' > process_inputs.py
root@86753427-7808-482c-8ff7-8f392a0d20f2:/home/user/projects/api-test# python3 process_inputs.py
Processing complete. Deliverables generated.
root@86753427-7808-482c-8ff7-8f392a0d20f2:/home/user/projects/api-test#
